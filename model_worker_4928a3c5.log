2024-03-19 11:32:12 | INFO | model_worker | args: Namespace(host='localhost', port=21002, worker_address='http://localhost:21002', controller_address='http://localhost:21001', model_path='/home/w1nd/darkword/1darkword/model_train/Baichuan2-7B-Chat/darkword-Baichuan2-7B-Chat', revision='main', device='cuda', gpus=None, num_gpus=1, max_gpu_memory=None, dtype=None, load_8bit=False, cpu_offloading=False, gptq_ckpt=None, gptq_wbits=16, gptq_groupsize=-1, gptq_act_order=False, awq_ckpt=None, awq_wbits=16, awq_groupsize=-1, enable_exllama=False, exllama_max_seq_len=4096, exllama_gpu_split=None, exllama_cache_8bit=False, enable_xft=False, xft_max_seq_len=4096, xft_dtype=None, model_names=None, conv_template=None, embed_in_truncate=False, limit_worker_concurrency=5, stream_interval=2, no_register=False, seed=None, debug=False, ssl=False)
2024-03-19 11:32:12 | INFO | model_worker | Loading the model ['darkword-Baichuan2-7B-Chat'] on worker 4928a3c5 ...
2024-03-19 11:32:18 | ERROR | stderr | Traceback (most recent call last):
2024-03-19 11:32:18 | ERROR | stderr |   File "/home/w1nd/.conda/envs/llm/lib/python3.10/runpy.py", line 196, in _run_module_as_main
2024-03-19 11:32:18 | ERROR | stderr |     return _run_code(code, main_globals, None,
2024-03-19 11:32:18 | ERROR | stderr |   File "/home/w1nd/.conda/envs/llm/lib/python3.10/runpy.py", line 86, in _run_code
2024-03-19 11:32:18 | ERROR | stderr |     exec(code, run_globals)
2024-03-19 11:32:18 | ERROR | stderr |   File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/fastchat/serve/model_worker.py", line 414, in <module>
2024-03-19 11:32:18 | ERROR | stderr |     args, worker = create_model_worker()
2024-03-19 11:32:18 | ERROR | stderr |   File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/fastchat/serve/model_worker.py", line 385, in create_model_worker
2024-03-19 11:32:18 | ERROR | stderr |     worker = ModelWorker(
2024-03-19 11:32:18 | ERROR | stderr |   File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/fastchat/serve/model_worker.py", line 77, in __init__
2024-03-19 11:32:18 | ERROR | stderr |     self.model, self.tokenizer = load_model(
2024-03-19 11:32:18 | ERROR | stderr |   File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/fastchat/model/model_adapter.py", line 353, in load_model
2024-03-19 11:32:18 | ERROR | stderr |     model, tokenizer = adapter.load_model(model_path, kwargs)
2024-03-19 11:32:18 | ERROR | stderr |   File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/fastchat/model/model_adapter.py", line 1404, in load_model
2024-03-19 11:32:18 | ERROR | stderr |     model = AutoModelForCausalLM.from_pretrained(
2024-03-19 11:32:18 | ERROR | stderr |   File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py", line 561, in from_pretrained
2024-03-19 11:32:18 | ERROR | stderr |     return model_class.from_pretrained(
2024-03-19 11:32:18 | ERROR | stderr |   File "/home/w1nd/.cache/huggingface/modules/transformers_modules/baichuan-inc/Baichuan2-7B-Chat/ea66ced17780ca3db39bc9f8aa601d8463db3da5/modeling_baichuan.py", line 660, in from_pretrained
2024-03-19 11:32:18 | ERROR | stderr |     return super(BaichuanForCausalLM, cls).from_pretrained(pretrained_model_name_or_path, *model_args,
2024-03-19 11:32:18 | ERROR | stderr |   File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3931, in from_pretrained
2024-03-19 11:32:18 | ERROR | stderr |     model.load_adapter(
2024-03-19 11:32:18 | ERROR | stderr |   File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/transformers/integrations/peft.py", line 180, in load_adapter
2024-03-19 11:32:18 | ERROR | stderr |     peft_config = PeftConfig.from_pretrained(
2024-03-19 11:32:18 | ERROR | stderr |   File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/peft/config.py", line 137, in from_pretrained
2024-03-19 11:32:18 | ERROR | stderr |     config = config_cls(**kwargs)
2024-03-19 11:32:18 | ERROR | stderr | TypeError: LoraConfig.__init__() got an unexpected keyword argument 'use_rslora'

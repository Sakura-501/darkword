
  0%|▏                                                                                                         | 7/3000 [00:01<07:31,  6.62it/s]
{'loss': 4.765, 'learning_rate': 4.9833333333333336e-05, 'epoch': 0.02}

  1%|▉                                                                                                        | 25/3000 [00:03<05:20,  9.29it/s]
{'loss': 4.6232, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.06}

  1%|█▌                                                                                                       | 43/3000 [00:05<05:09,  9.55it/s]
{'loss': 4.2217, 'learning_rate': 4.9166666666666665e-05, 'epoch': 0.1}

  2%|██▏                                                                                                      | 62/3000 [00:07<04:57,  9.88it/s]
{'loss': 3.8334, 'learning_rate': 4.883333333333334e-05, 'epoch': 0.14}

  3%|██▊                                                                                                      | 82/3000 [00:09<05:07,  9.50it/s]
{'loss': 3.359, 'learning_rate': 4.85e-05, 'epoch': 0.18}

  3%|███▌                                                                                                    | 101/3000 [00:12<05:05,  9.49it/s]
{'loss': 3.3816, 'learning_rate': 4.8166666666666674e-05, 'epoch': 0.22}

  4%|████▏                                                                                                   | 120/3000 [00:14<05:09,  9.30it/s]
{'loss': 3.5682, 'learning_rate': 4.7833333333333335e-05, 'epoch': 0.26}

  5%|████▊                                                                                                   | 139/3000 [00:16<05:08,  9.28it/s]

  5%|█████▍                                                                                                  | 156/3000 [00:18<05:22,  8.81it/s]
{'loss': 3.7242, 'learning_rate': 4.7333333333333336e-05, 'epoch': 0.32}

  6%|██████                                                                                                  | 174/3000 [00:20<05:08,  9.17it/s]
{'loss': 3.7107, 'learning_rate': 4.7e-05, 'epoch': 0.36}

  6%|██████▋                                                                                                 | 193/3000 [00:22<05:04,  9.20it/s]
{'loss': 3.3514, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.4}

  7%|███████▎                                                                                                | 211/3000 [00:24<05:15,  8.84it/s]
{'loss': 3.3141, 'learning_rate': 4.633333333333333e-05, 'epoch': 0.44}

  8%|███████▉                                                                                                | 228/3000 [00:26<05:03,  9.13it/s]

  8%|████████▌                                                                                               | 246/3000 [00:28<05:05,  9.01it/s]
{'loss': 3.3361, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.5}

  9%|█████████                                                                                               | 262/3000 [00:29<05:06,  8.92it/s]
{'loss': 3.164, 'learning_rate': 4.55e-05, 'epoch': 0.54}

  9%|█████████▋                                                                                              | 281/3000 [00:32<05:04,  8.94it/s]
{'loss': 2.8479, 'learning_rate': 4.516666666666667e-05, 'epoch': 0.58}

 10%|██████████▎                                                                                             | 299/3000 [00:34<05:06,  8.82it/s]
{'loss': 2.8934, 'learning_rate': 4.483333333333333e-05, 'epoch': 0.62}

 11%|███████████                                                                                             | 318/3000 [00:36<04:38,  9.61it/s]

 11%|███████████▋                                                                                            | 337/3000 [00:38<04:46,  9.31it/s]
{'loss': 2.9376, 'learning_rate': 4.433333333333334e-05, 'epoch': 0.68}

 12%|████████████▎                                                                                           | 355/3000 [00:40<04:48,  9.16it/s]
{'loss': 3.0229, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.72}

 12%|████████████▉                                                                                           | 374/3000 [00:42<04:46,  9.16it/s]
{'loss': 3.106, 'learning_rate': 4.3666666666666666e-05, 'epoch': 0.76}

 13%|█████████████▌                                                                                          | 392/3000 [00:44<04:47,  9.08it/s]
{'loss': 3.1017, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.8}

 14%|██████████████▏                                                                                         | 411/3000 [00:46<04:45,  9.07it/s]
{'loss': 3.152, 'learning_rate': 4.3e-05, 'epoch': 0.84}

 14%|██████████████▊                                                                                         | 429/3000 [00:48<04:36,  9.30it/s]

 15%|███████████████▌                                                                                        | 448/3000 [00:50<04:33,  9.33it/s]
{'loss': 2.6246, 'learning_rate': 4.25e-05, 'epoch': 0.9}

 16%|████████████████▏                                                                                       | 467/3000 [00:52<04:45,  8.86it/s]
{'loss': 3.0547, 'learning_rate': 4.216666666666667e-05, 'epoch': 0.94}

 16%|████████████████▊                                                                                       | 485/3000 [00:54<04:41,  8.92it/s]
{'loss': 3.2262, 'learning_rate': 4.183333333333334e-05, 'epoch': 0.98}
 17%|█████████████████▎                                                                                      | 500/3000 [00:55<04:27,  9.36it/s]***** Running Evaluation *****
  Num examples = 50
  Batch size = 16


 75%|█████████████████████████████████████████████████████████████████████████████████▊                           | 3/4 [00:16<00:05,  5.96s/it]
Loading model from cache /tmp/jieba.cache█████████████████████████████████████████████████████████████████████████| 4/4 [00:18<00:00,  4.37s/it]
Loading model cost 0.590 seconds.
Prefix dict has been built successfully.
 17%|█████████████████▎                                                                                      | 500/3000 [01:24<04:27,  9.36it/s]Checkpoint destination directory ../darkword-ChatGLM3-6B/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.
Saving model checkpoint to ../darkword-ChatGLM3-6B/checkpoint-500
[2024-03-17 11:01:36,471] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step500 is about to be saved!
[2024-03-17 11:01:36,480] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ../darkword-ChatGLM3-6B/checkpoint-500/global_step500/mp_rank_00_model_states.pt
[2024-03-17 11:01:36,480] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/checkpoint-500/global_step500/mp_rank_00_model_states.pt...
[2024-03-17 11:01:36,501] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/checkpoint-500/global_step500/mp_rank_00_model_states.pt.
[2024-03-17 11:01:36,502] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-17 11:01:36,563] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-17 11:01:36,563] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved ../darkword-ChatGLM3-6B/checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-17 11:01:36,564] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!
{'loss': 2.9695, 'learning_rate': 4.15e-05, 'epoch': 1.02}
loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/9addbe01105ca1939dd60a0e5866a1812be9daea/config.json
Model config ChatGLMConfig {
  "_name_or_path": "THUDM/chatglm3-6b",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig",
    "AutoModel": "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1e-05,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_layers": 28,
  "original_rope": true,
  "pad_token_id": 0,
  "padded_vocab_size": 65024,
  "post_layer_norm": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "rmsnorm": true,
  "seq_length": 8192,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65024
}
tokenizer config file saved in ../darkword-ChatGLM3-6B/checkpoint-500/tokenizer_config.json
Special tokens file saved in ../darkword-ChatGLM3-6B/checkpoint-500/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
 17%|█████████████████▋                                                                                      | 509/3000 [01:32<30:21,  1.37it/s]

 18%|██████████████████▎                                                                                     | 528/3000 [01:34<04:17,  9.61it/s]
{'loss': 2.5064, 'learning_rate': 4.116666666666667e-05, 'epoch': 1.06}

 18%|██████████████████▉                                                                                     | 547/3000 [01:36<04:09,  9.83it/s]
{'loss': 2.7156, 'learning_rate': 4.0833333333333334e-05, 'epoch': 1.1}

 19%|███████████████████▋                                                                                    | 567/3000 [01:38<04:18,  9.41it/s]
{'loss': 2.8866, 'learning_rate': 4.05e-05, 'epoch': 1.14}

 20%|████████████████████▎                                                                                   | 585/3000 [01:40<04:18,  9.33it/s]
{'loss': 3.1355, 'learning_rate': 4.016666666666667e-05, 'epoch': 1.18}

 20%|████████████████████▉                                                                                   | 604/3000 [01:42<04:21,  9.15it/s]
{'loss': 2.8797, 'learning_rate': 3.983333333333333e-05, 'epoch': 1.22}

 21%|█████████████████████▌                                                                                  | 623/3000 [01:44<04:14,  9.35it/s]
{'loss': 2.9115, 'learning_rate': 3.9500000000000005e-05, 'epoch': 1.26}

 21%|██████████████████████▎                                                                                 | 642/3000 [01:46<04:10,  9.40it/s]
{'loss': 2.5349, 'learning_rate': 3.9166666666666665e-05, 'epoch': 1.3}

 22%|██████████████████████▉                                                                                 | 661/3000 [01:48<04:02,  9.63it/s]
{'loss': 2.4814, 'learning_rate': 3.883333333333333e-05, 'epoch': 1.34}

 23%|███████████████████████▌                                                                                | 679/3000 [01:50<04:14,  9.14it/s]

 23%|████████████████████████▏                                                                               | 697/3000 [01:52<04:14,  9.04it/s]
{'loss': 2.7876, 'learning_rate': 3.8333333333333334e-05, 'epoch': 1.4}

 24%|████████████████████████▊                                                                               | 715/3000 [01:54<04:14,  8.99it/s]
{'loss': 2.3558, 'learning_rate': 3.8e-05, 'epoch': 1.44}

 24%|█████████████████████████▍                                                                              | 734/3000 [01:56<04:05,  9.22it/s]
{'loss': 2.2206, 'learning_rate': 3.766666666666667e-05, 'epoch': 1.48}

 25%|██████████████████████████                                                                              | 752/3000 [01:58<04:08,  9.04it/s]
{'loss': 3.3695, 'learning_rate': 3.733333333333334e-05, 'epoch': 1.52}

 26%|██████████████████████████▋                                                                             | 771/3000 [02:00<04:06,  9.03it/s]
{'loss': 2.6291, 'learning_rate': 3.7e-05, 'epoch': 1.56}

 26%|███████████████████████████▎                                                                            | 789/3000 [02:02<03:57,  9.31it/s]

 27%|████████████████████████████                                                                            | 808/3000 [02:04<03:55,  9.29it/s]
{'loss': 2.8797, 'learning_rate': 3.65e-05, 'epoch': 1.62}

 28%|████████████████████████████▋                                                                           | 826/3000 [02:06<03:52,  9.35it/s]
{'loss': 2.9543, 'learning_rate': 3.6166666666666674e-05, 'epoch': 1.66}

 28%|█████████████████████████████▎                                                                          | 844/3000 [02:08<04:01,  8.92it/s]

 29%|█████████████████████████████▉                                                                          | 862/3000 [02:10<04:02,  8.83it/s]
{'loss': 3.056, 'learning_rate': 3.566666666666667e-05, 'epoch': 1.72}

 29%|██████████████████████████████▌                                                                         | 880/3000 [02:12<03:48,  9.26it/s]
{'loss': 2.9628, 'learning_rate': 3.5333333333333336e-05, 'epoch': 1.76}

 30%|███████████████████████████████▏                                                                        | 899/3000 [02:14<03:44,  9.35it/s]
{'loss': 3.1531, 'learning_rate': 3.5e-05, 'epoch': 1.8}

 31%|███████████████████████████████▊                                                                        | 918/3000 [02:16<03:45,  9.25it/s]
{'loss': 2.2839, 'learning_rate': 3.466666666666667e-05, 'epoch': 1.84}

 31%|████████████████████████████████▍                                                                       | 936/3000 [02:18<03:41,  9.30it/s]
{'loss': 2.9323, 'learning_rate': 3.433333333333333e-05, 'epoch': 1.88}

 32%|█████████████████████████████████                                                                       | 955/3000 [02:20<03:40,  9.29it/s]

 32%|█████████████████████████████████▋                                                                      | 973/3000 [02:22<03:38,  9.28it/s]
{'loss': 3.0039, 'learning_rate': 3.3833333333333334e-05, 'epoch': 1.94}

 33%|██████████████████████████████████▍                                                                     | 992/3000 [02:24<03:33,  9.39it/s]
{'loss': 2.7935, 'learning_rate': 3.35e-05, 'epoch': 1.98}
 33%|██████████████████████████████████▎                                                                    | 1000/3000 [02:25<03:29,  9.54it/s]***** Running Evaluation *****
  Num examples = 50
  Batch size = 16


 75%|█████████████████████████████████████████████████████████████████████████████████▊                           | 3/4 [00:16<00:06,  6.00s/it]

{'eval_rouge-1': 25.079726, 'eval_rouge-2': 7.779688000000001, 'eval_rouge-l': 20.171038, 'eval_bleu-4': 0.04119608185130343, 'eval_runtime': 28.2312, 'eval_samples_per_second': 1.771, 'eval_steps_per_second': 0.142, 'epoch': 2.0}
[2024-03-17 11:03:05,734] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
[2024-03-17 11:03:05,743] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ../darkword-ChatGLM3-6B/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt
[2024-03-17 11:03:05,743] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt...
[2024-03-17 11:03:05,762] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt.
[2024-03-17 11:03:05,763] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-17 11:03:05,814] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-17 11:03:05,814] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved ../darkword-ChatGLM3-6B/checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-17 11:03:05,814] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
Saving model checkpoint to ../darkword-ChatGLM3-6B/checkpoint-1000
loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/9addbe01105ca1939dd60a0e5866a1812be9daea/config.json
Model config ChatGLMConfig {
  "_name_or_path": "THUDM/chatglm3-6b",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig",
    "AutoModel": "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1e-05,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_layers": 28,
  "original_rope": true,
  "pad_token_id": 0,
  "padded_vocab_size": 65024,
  "post_layer_norm": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "rmsnorm": true,
  "seq_length": 8192,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65024
}
tokenizer config file saved in ../darkword-ChatGLM3-6B/checkpoint-1000/tokenizer_config.json
Special tokens file saved in ../darkword-ChatGLM3-6B/checkpoint-1000/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
 33%|█████████████████████████████████▋                                                                   | 1002/3000 [03:00<4:07:08,  7.42s/it]
 34%|██████████████████████████████████▉                                                                    | 1016/3000 [03:02<05:29,  6.03it/s]
 34%|██████████████████████████████████▉                                                                    | 1016/3000 [03:02<05:29,  6.03it/s]
{'loss': 2.1623, 'learning_rate': 3.3e-05, 'epoch': 2.04}
 34%|███████████████████████████████████▌                                                                   | 1034/3000 [03:04<03:46,  8.67it/s]
 34%|███████████████████████████████████▌                                                                   | 1034/3000 [03:04<03:46,  8.67it/s]
{'loss': 2.781, 'learning_rate': 3.266666666666667e-05, 'epoch': 2.08}
 35%|████████████████████████████████████                                                                   | 1051/3000 [03:06<03:41,  8.80it/s]
 35%|████████████████████████████████████                                                                   | 1051/3000 [03:06<03:41,  8.80it/s]
 36%|████████████████████████████████████▋                                                                  | 1070/3000 [03:08<03:29,  9.23it/s]
 36%|████████████████████████████████████▋                                                                  | 1070/3000 [03:08<03:29,  9.23it/s]
{'loss': 3.268, 'learning_rate': 3.2166666666666665e-05, 'epoch': 2.14}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 3.3125, 'learning_rate': 3.183333333333334e-05, 'epoch': 2.18}
{'loss': 2.3417, 'learning_rate': 3.1666666666666666e-05, 'epoch': 2.2}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.3685, 'learning_rate': 3.15e-05, 'epoch': 2.22}
{'loss': 2.0545, 'learning_rate': 3.1333333333333334e-05, 'epoch': 2.24}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.8747, 'learning_rate': 3.116666666666667e-05, 'epoch': 2.26}
{'loss': 2.3267, 'learning_rate': 3.1e-05, 'epoch': 2.28}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.4438, 'learning_rate': 3.0833333333333335e-05, 'epoch': 2.3}
{'loss': 2.6102, 'learning_rate': 3.066666666666667e-05, 'epoch': 2.32}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.7308, 'learning_rate': 3.05e-05, 'epoch': 2.34}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.9311, 'learning_rate': 3.0333333333333337e-05, 'epoch': 2.36}
{'loss': 2.856, 'learning_rate': 3.016666666666667e-05, 'epoch': 2.38}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.8397, 'learning_rate': 3e-05, 'epoch': 2.4}
{'loss': 2.6132, 'learning_rate': 2.9833333333333335e-05, 'epoch': 2.42}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.6774, 'learning_rate': 2.9666666666666672e-05, 'epoch': 2.44}
{'loss': 2.6416, 'learning_rate': 2.95e-05, 'epoch': 2.46}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.3659, 'learning_rate': 2.9333333333333336e-05, 'epoch': 2.48}
{'loss': 2.2417, 'learning_rate': 2.916666666666667e-05, 'epoch': 2.5}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.4414, 'learning_rate': 2.9e-05, 'epoch': 2.51}
{'loss': 2.2347, 'learning_rate': 2.8833333333333334e-05, 'epoch': 2.53}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.9385, 'learning_rate': 2.8666666666666668e-05, 'epoch': 2.55}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.5283, 'learning_rate': 2.8499999999999998e-05, 'epoch': 2.57}
{'loss': 2.3107, 'learning_rate': 2.8333333333333335e-05, 'epoch': 2.59}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.5293, 'learning_rate': 2.816666666666667e-05, 'epoch': 2.61}
{'loss': 2.831, 'learning_rate': 2.8000000000000003e-05, 'epoch': 2.63}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.1392, 'learning_rate': 2.7833333333333333e-05, 'epoch': 2.65}
{'loss': 2.3966, 'learning_rate': 2.7666666666666667e-05, 'epoch': 2.67}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.5334, 'learning_rate': 2.7500000000000004e-05, 'epoch': 2.69}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.4301, 'learning_rate': 2.733333333333333e-05, 'epoch': 2.71}
{'loss': 2.7629, 'learning_rate': 2.716666666666667e-05, 'epoch': 2.73}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.3959, 'learning_rate': 2.7000000000000002e-05, 'epoch': 2.75}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.5993, 'learning_rate': 2.6833333333333333e-05, 'epoch': 2.77}
{'loss': 2.9988, 'learning_rate': 2.6666666666666667e-05, 'epoch': 2.79}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.6574, 'learning_rate': 2.6500000000000004e-05, 'epoch': 2.81}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.7569, 'learning_rate': 2.633333333333333e-05, 'epoch': 2.83}
{'loss': 2.393, 'learning_rate': 2.6166666666666668e-05, 'epoch': 2.85}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.4745, 'learning_rate': 2.6000000000000002e-05, 'epoch': 2.87}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.623, 'learning_rate': 2.5833333333333336e-05, 'epoch': 2.89}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.767, 'learning_rate': 2.5666666666666666e-05, 'epoch': 2.91}
{'loss': 2.4603, 'learning_rate': 2.5500000000000003e-05, 'epoch': 2.93}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.3707, 'learning_rate': 2.5333333333333337e-05, 'epoch': 2.95}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.8218, 'learning_rate': 2.5166666666666667e-05, 'epoch': 2.97}
{'loss': 2.8828, 'learning_rate': 2.5e-05, 'epoch': 2.99}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
{'eval_rouge-1': 27.558680000000003, 'eval_rouge-2': 8.201974, 'eval_rouge-l': 22.470180000000006, 'eval_bleu-4': 0.044897447566115085, 'eval_runtime': 35.6447, 'eval_samples_per_second': 1.403, 'eval_steps_per_second': 0.112, 'epoch': 2.99}
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
 36%|█████████████████████████████████████▎                                                                 | 1088/3000 [03:10<03:33,  8.95it/s]
[2024-03-17 11:04:48,943] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1500 is about to be saved!
[2024-03-17 11:04:48,956] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ../darkword-ChatGLM3-6B/checkpoint-1500/global_step1500/mp_rank_00_model_states.pt
[2024-03-17 11:04:48,956] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/checkpoint-1500/global_step1500/mp_rank_00_model_states.pt...
[2024-03-17 11:04:48,979] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/checkpoint-1500/global_step1500/mp_rank_00_model_states.pt.
[2024-03-17 11:04:48,980] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-17 11:04:49,027] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-17 11:04:49,027] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved ../darkword-ChatGLM3-6B/checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_optim_states.pt
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.9406, 'learning_rate': 2.4833333333333335e-05, 'epoch': 3.01}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.747, 'learning_rate': 2.466666666666667e-05, 'epoch': 3.03}
{'loss': 2.8197, 'learning_rate': 2.45e-05, 'epoch': 3.05}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.5036, 'learning_rate': 2.4333333333333336e-05, 'epoch': 3.07}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.2102, 'learning_rate': 2.4166666666666667e-05, 'epoch': 3.09}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.2758, 'learning_rate': 2.4e-05, 'epoch': 3.11}
{'loss': 2.3265, 'learning_rate': 2.3833333333333334e-05, 'epoch': 3.13}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.3876, 'learning_rate': 2.3666666666666668e-05, 'epoch': 3.15}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.6335, 'learning_rate': 2.35e-05, 'epoch': 3.17}
{'loss': 2.8083, 'learning_rate': 2.3333333333333336e-05, 'epoch': 3.19}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.2895, 'learning_rate': 2.3166666666666666e-05, 'epoch': 3.21}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 1.9067, 'learning_rate': 2.3000000000000003e-05, 'epoch': 3.23}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 1.7309, 'learning_rate': 2.2833333333333334e-05, 'epoch': 3.25}
{'loss': 2.5452, 'learning_rate': 2.2666666666666668e-05, 'epoch': 3.27}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.6877, 'learning_rate': 2.25e-05, 'epoch': 3.29}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.6623, 'learning_rate': 2.2333333333333335e-05, 'epoch': 3.31}
{'loss': 2.759, 'learning_rate': 2.216666666666667e-05, 'epoch': 3.33}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.6403, 'learning_rate': 2.2000000000000003e-05, 'epoch': 3.35}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.203, 'learning_rate': 2.1833333333333333e-05, 'epoch': 3.37}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.2752, 'learning_rate': 2.1666666666666667e-05, 'epoch': 3.39}
{'loss': 2.5169, 'learning_rate': 2.15e-05, 'epoch': 3.41}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.3873, 'learning_rate': 2.1333333333333335e-05, 'epoch': 3.43}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.4545, 'learning_rate': 2.116666666666667e-05, 'epoch': 3.45}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.701, 'learning_rate': 2.1e-05, 'epoch': 3.47}
{'loss': 2.4128, 'learning_rate': 2.0833333333333336e-05, 'epoch': 3.49}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.142, 'learning_rate': 2.0666666666666666e-05, 'epoch': 3.51}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 1.9715, 'learning_rate': 2.05e-05, 'epoch': 3.53}
{'loss': 2.2995, 'learning_rate': 2.0333333333333334e-05, 'epoch': 3.55}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.2397, 'learning_rate': 2.0166666666666668e-05, 'epoch': 3.57}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.6063, 'learning_rate': 2e-05, 'epoch': 3.59}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.0732, 'learning_rate': 1.9833333333333335e-05, 'epoch': 3.61}
{'loss': 2.4652, 'learning_rate': 1.9666666666666666e-05, 'epoch': 3.63}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.8835, 'learning_rate': 1.9500000000000003e-05, 'epoch': 3.65}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 1.9295, 'learning_rate': 1.9333333333333333e-05, 'epoch': 3.67}
{'loss': 2.6772, 'learning_rate': 1.9166666666666667e-05, 'epoch': 3.69}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 3.0791, 'learning_rate': 1.9e-05, 'epoch': 3.71}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.2283, 'learning_rate': 1.8833333333333335e-05, 'epoch': 3.73}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.2839, 'learning_rate': 1.866666666666667e-05, 'epoch': 3.75}
{'loss': 2.4216, 'learning_rate': 1.85e-05, 'epoch': 3.77}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.2666, 'learning_rate': 1.8333333333333333e-05, 'epoch': 3.79}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.4667, 'learning_rate': 1.8166666666666667e-05, 'epoch': 3.81}
{'loss': 2.4279, 'learning_rate': 1.8e-05, 'epoch': 3.83}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.9099, 'learning_rate': 1.7833333333333334e-05, 'epoch': 3.85}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.312, 'learning_rate': 1.7666666666666668e-05, 'epoch': 3.87}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.2985, 'learning_rate': 1.75e-05, 'epoch': 3.89}
{'loss': 2.3513, 'learning_rate': 1.7333333333333336e-05, 'epoch': 3.91}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.0619, 'learning_rate': 1.7166666666666666e-05, 'epoch': 3.93}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.7495, 'learning_rate': 1.7000000000000003e-05, 'epoch': 3.95}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.148, 'learning_rate': 1.6833333333333334e-05, 'epoch': 3.97}
{'loss': 2.6381, 'learning_rate': 1.6666666666666667e-05, 'epoch': 3.99}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'eval_rouge-1': 26.301912, 'eval_rouge-2': 7.555377999999999, 'eval_rouge-l': 21.174368, 'eval_bleu-4': 0.04129527158180577, 'eval_runtime': 36.9163, 'eval_samples_per_second': 1.354, 'eval_steps_per_second': 0.108, 'epoch': 3.99}
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
[2024-03-17 11:06:46,054] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2000 is about to be saved!
[2024-03-17 11:06:46,068] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ../darkword-ChatGLM3-6B/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt
[2024-03-17 11:06:46,068] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt...
[2024-03-17 11:06:46,126] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt.
[2024-03-17 11:06:46,127] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-17 11:06:46,180] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-17 11:06:46,181] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved ../darkword-ChatGLM3-6B/checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_optim_states.pt
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.0537, 'learning_rate': 1.65e-05, 'epoch': 4.01}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.7864, 'learning_rate': 1.6333333333333335e-05, 'epoch': 4.03}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.3681, 'learning_rate': 1.6166666666666665e-05, 'epoch': 4.05}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.2153, 'learning_rate': 1.6000000000000003e-05, 'epoch': 4.07}
{'loss': 2.6299, 'learning_rate': 1.5833333333333333e-05, 'epoch': 4.09}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.3597, 'learning_rate': 1.5666666666666667e-05, 'epoch': 4.11}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 1.9488, 'learning_rate': 1.55e-05, 'epoch': 4.13}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.3506, 'learning_rate': 1.5333333333333334e-05, 'epoch': 4.15}
{'loss': 2.1728, 'learning_rate': 1.5166666666666668e-05, 'epoch': 4.17}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 1.4559, 'learning_rate': 1.5e-05, 'epoch': 4.19}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.2302, 'learning_rate': 1.4833333333333336e-05, 'epoch': 4.21}
{'loss': 2.3513, 'learning_rate': 1.4666666666666668e-05, 'epoch': 4.23}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 1.6286, 'learning_rate': 1.45e-05, 'epoch': 4.25}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.2791, 'learning_rate': 1.4333333333333334e-05, 'epoch': 4.27}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.5127, 'learning_rate': 1.4166666666666668e-05, 'epoch': 4.29}
{'loss': 2.7986, 'learning_rate': 1.4000000000000001e-05, 'epoch': 4.31}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.9118, 'learning_rate': 1.3833333333333334e-05, 'epoch': 4.33}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.4217, 'learning_rate': 1.3666666666666666e-05, 'epoch': 4.35}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.6652, 'learning_rate': 1.3500000000000001e-05, 'epoch': 4.37}
{'loss': 2.7694, 'learning_rate': 1.3333333333333333e-05, 'epoch': 4.39}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.1839, 'learning_rate': 1.3166666666666665e-05, 'epoch': 4.41}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.4812, 'learning_rate': 1.3000000000000001e-05, 'epoch': 4.43}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.3517, 'learning_rate': 1.2833333333333333e-05, 'epoch': 4.45}
{'loss': 2.0298, 'learning_rate': 1.2666666666666668e-05, 'epoch': 4.47}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.3985, 'learning_rate': 1.25e-05, 'epoch': 4.49}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.1163, 'learning_rate': 1.2333333333333334e-05, 'epoch': 4.51}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.1755, 'learning_rate': 1.2166666666666668e-05, 'epoch': 4.53}
{'loss': 2.1538, 'learning_rate': 1.2e-05, 'epoch': 4.55}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.267, 'learning_rate': 1.1833333333333334e-05, 'epoch': 4.57}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 1.7553, 'learning_rate': 1.1666666666666668e-05, 'epoch': 4.59}
{'loss': 2.584, 'learning_rate': 1.1500000000000002e-05, 'epoch': 4.61}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 1.7451, 'learning_rate': 1.1333333333333334e-05, 'epoch': 4.63}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.1053, 'learning_rate': 1.1166666666666668e-05, 'epoch': 4.65}
{'loss': 2.4601, 'learning_rate': 1.1000000000000001e-05, 'epoch': 4.67}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.4337, 'learning_rate': 1.0833333333333334e-05, 'epoch': 4.69}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.1674, 'learning_rate': 1.0666666666666667e-05, 'epoch': 4.71}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.5106, 'learning_rate': 1.05e-05, 'epoch': 4.73}
{'loss': 2.3743, 'learning_rate': 1.0333333333333333e-05, 'epoch': 4.75}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.5764, 'learning_rate': 1.0166666666666667e-05, 'epoch': 4.77}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.5756, 'learning_rate': 1e-05, 'epoch': 4.79}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.0325, 'learning_rate': 9.833333333333333e-06, 'epoch': 4.81}
{'loss': 2.1373, 'learning_rate': 9.666666666666667e-06, 'epoch': 4.83}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 1.6547, 'learning_rate': 9.5e-06, 'epoch': 4.85}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.6677, 'learning_rate': 9.333333333333334e-06, 'epoch': 4.87}
{'loss': 2.3226, 'learning_rate': 9.166666666666666e-06, 'epoch': 4.89}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.5266, 'learning_rate': 9e-06, 'epoch': 4.91}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 1.8804, 'learning_rate': 8.833333333333334e-06, 'epoch': 4.93}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.2621, 'learning_rate': 8.666666666666668e-06, 'epoch': 4.95}
{'loss': 2.1317, 'learning_rate': 8.500000000000002e-06, 'epoch': 4.97}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 1.7452, 'learning_rate': 8.333333333333334e-06, 'epoch': 4.99}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'eval_rouge-1': 26.98602, 'eval_rouge-2': 7.899970000000001, 'eval_rouge-l': 21.393393999999997, 'eval_bleu-4': 0.043279393290632, 'eval_runtime': 37.1022, 'eval_samples_per_second': 1.348, 'eval_steps_per_second': 0.108, 'epoch': 4.99}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
[2024-03-17 11:08:43,693] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2500 is about to be saved!
[2024-03-17 11:08:43,702] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ../darkword-ChatGLM3-6B/checkpoint-2500/global_step2500/mp_rank_00_model_states.pt
[2024-03-17 11:08:43,702] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/checkpoint-2500/global_step2500/mp_rank_00_model_states.pt...
[2024-03-17 11:08:43,725] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/checkpoint-2500/global_step2500/mp_rank_00_model_states.pt.
[2024-03-17 11:08:43,727] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/checkpoint-2500/global_step2500/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-17 11:08:43,776] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/checkpoint-2500/global_step2500/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-17 11:08:43,777] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved ../darkword-ChatGLM3-6B/checkpoint-2500/global_step2500/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-17 11:08:43,777] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2500 is ready now!
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.5901, 'learning_rate': 8.166666666666668e-06, 'epoch': 5.01}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.1177, 'learning_rate': 8.000000000000001e-06, 'epoch': 5.03}
{'loss': 2.2037, 'learning_rate': 7.833333333333333e-06, 'epoch': 5.05}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.1714, 'learning_rate': 7.666666666666667e-06, 'epoch': 5.07}
{'loss': 2.6301, 'learning_rate': 7.5e-06, 'epoch': 5.09}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 1.8134, 'learning_rate': 7.333333333333334e-06, 'epoch': 5.11}
{'loss': 2.2289, 'learning_rate': 7.166666666666667e-06, 'epoch': 5.13}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.1927, 'learning_rate': 7.000000000000001e-06, 'epoch': 5.15}
{'loss': 2.1429, 'learning_rate': 6.833333333333333e-06, 'epoch': 5.17}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.5673, 'learning_rate': 6.666666666666667e-06, 'epoch': 5.19}
{'loss': 2.0715, 'learning_rate': 6.5000000000000004e-06, 'epoch': 5.21}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 1.888, 'learning_rate': 6.333333333333334e-06, 'epoch': 5.23}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.1806, 'learning_rate': 6.166666666666667e-06, 'epoch': 5.25}
{'loss': 1.8948, 'learning_rate': 6e-06, 'epoch': 5.27}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 1.9674, 'learning_rate': 5.833333333333334e-06, 'epoch': 5.29}
{'loss': 1.5644, 'learning_rate': 5.666666666666667e-06, 'epoch': 5.31}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.2261, 'learning_rate': 5.500000000000001e-06, 'epoch': 5.33}
{'loss': 2.3492, 'learning_rate': 5.333333333333334e-06, 'epoch': 5.35}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 1.482, 'learning_rate': 5.166666666666667e-06, 'epoch': 5.37}
{'loss': 2.4333, 'learning_rate': 5e-06, 'epoch': 5.39}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.2799, 'learning_rate': 4.833333333333333e-06, 'epoch': 5.41}
{'loss': 2.4376, 'learning_rate': 4.666666666666667e-06, 'epoch': 5.43}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.0603, 'learning_rate': 4.5e-06, 'epoch': 5.45}
{'loss': 2.0745, 'learning_rate': 4.333333333333334e-06, 'epoch': 5.47}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.3378, 'learning_rate': 4.166666666666667e-06, 'epoch': 5.49}
{'loss': 2.4396, 'learning_rate': 4.000000000000001e-06, 'epoch': 5.51}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.0362, 'learning_rate': 3.833333333333334e-06, 'epoch': 5.53}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 1.7476, 'learning_rate': 3.666666666666667e-06, 'epoch': 5.55}
{'loss': 2.0194, 'learning_rate': 3.5000000000000004e-06, 'epoch': 5.57}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.4177, 'learning_rate': 3.3333333333333333e-06, 'epoch': 5.59}
{'loss': 1.7632, 'learning_rate': 3.166666666666667e-06, 'epoch': 5.61}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.3661, 'learning_rate': 3e-06, 'epoch': 5.63}
{'loss': 2.5686, 'learning_rate': 2.8333333333333335e-06, 'epoch': 5.65}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.222, 'learning_rate': 2.666666666666667e-06, 'epoch': 5.67}
{'loss': 1.8868, 'learning_rate': 2.5e-06, 'epoch': 5.69}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 1.7297, 'learning_rate': 2.3333333333333336e-06, 'epoch': 5.71}
{'loss': 1.9508, 'learning_rate': 2.166666666666667e-06, 'epoch': 5.73}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.4118, 'learning_rate': 2.0000000000000003e-06, 'epoch': 5.75}
{'loss': 2.3659, 'learning_rate': 1.8333333333333335e-06, 'epoch': 5.77}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.1709, 'learning_rate': 1.6666666666666667e-06, 'epoch': 5.79}
{'loss': 2.2115, 'learning_rate': 1.5e-06, 'epoch': 5.81}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.0022, 'learning_rate': 1.3333333333333334e-06, 'epoch': 5.83}
{'loss': 1.984, 'learning_rate': 1.1666666666666668e-06, 'epoch': 5.85}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.4804, 'learning_rate': 1.0000000000000002e-06, 'epoch': 5.87}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.0158, 'learning_rate': 8.333333333333333e-07, 'epoch': 5.89}
{'loss': 2.4393, 'learning_rate': 6.666666666666667e-07, 'epoch': 5.91}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.3043, 'learning_rate': 5.000000000000001e-07, 'epoch': 5.93}
{'loss': 2.3972, 'learning_rate': 3.3333333333333335e-07, 'epoch': 5.95}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'loss': 2.0437, 'learning_rate': 1.6666666666666668e-07, 'epoch': 5.97}
{'loss': 2.3389, 'learning_rate': 0.0, 'epoch': 5.99}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
{'eval_rouge-1': 26.580252, 'eval_rouge-2': 8.097376, 'eval_rouge-l': 20.917064, 'eval_bleu-4': 0.04313594080076689, 'eval_runtime': 27.2364, 'eval_samples_per_second': 1.836, 'eval_steps_per_second': 0.147, 'epoch': 5.99}
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
  "padded_vocab_size": 65024,lm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",eneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
[2024-03-17 11:10:11,886] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3000 is about to be saved!
[2024-03-17 11:10:11,894] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ../darkword-ChatGLM3-6B/checkpoint-3000/global_step3000/mp_rank_00_model_states.pt
[2024-03-17 11:10:11,894] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/checkpoint-3000/global_step3000/mp_rank_00_model_states.pt...
[2024-03-17 11:10:11,917] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/checkpoint-3000/global_step3000/mp_rank_00_model_states.pt.
[2024-03-17 11:10:11,918] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/checkpoint-3000/global_step3000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-17 11:10:11,973] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/checkpoint-3000/global_step3000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-17 11:10:11,973] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved ../darkword-ChatGLM3-6B/checkpoint-3000/global_step3000/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-17 11:10:11,973] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
  "post_layer_norm": true,: "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",          | 1088/3000 [03:10<03:33,  8.95it/s]
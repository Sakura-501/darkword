
  0%|          | 5/3000 [00:03<22:45,  2.19it/s]Traceback (most recent call last):
  File "/home/w1nd/darkword/1darkword/model_train/ChatGLM3-6B/finetune_demo/finetune_hf.py", line 558, in <module>
    app()
  File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/typer/main.py", line 328, in __call__
    raise e
  File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/typer/main.py", line 311, in __call__
    return get_command(self)(*args, **kwargs)
  File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/click/core.py", line 1157, in __call__
    return self.main(*args, **kwargs)
  File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/typer/core.py", line 716, in main
    return _main(
  File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/typer/core.py", line 216, in _main
    rv = self.invoke(ctx)
  File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/click/core.py", line 1434, in invoke
    return ctx.invoke(self.callback, **ctx.params)
  File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/click/core.py", line 783, in invoke
    return __callback(*args, **kwargs)
  File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/typer/main.py", line 683, in wrapper
    return callback(**use_params)  # type: ignore
  File "/home/w1nd/darkword/1darkword/model_train/ChatGLM3-6B/finetune_demo/finetune_hf.py", line 520, in main
    trainer.train()
  File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/transformers/trainer.py", line 1869, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/transformers/trainer.py", line 2781, in training_step
    self.accelerator.backward(loss)
  File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/accelerate/accelerator.py", line 1960, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/accelerate/utils/deepspeed.py", line 167, in backward
    self.engine.backward(loss, **kwargs)
  File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/deepspeed/runtime/engine.py", line 1974, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py", line 2052, in backward
    buf_1 = torch.empty(int(self.reduce_bucket_size),
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.15 GiB of which 1.63 GiB is free. Process 2673556 has 34.17 GiB memory in use. Process 4029770 has 26.77 GiB memory in use. Including non-PyTorch memory, this process has 16.55 GiB memory in use. Of the allocated memory 14.39 GiB is allocated by PyTorch, and 1.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
│ /home/w1nd/darkword/1darkword/model_train/ChatGLM3-6B/finetune_demo/finetune_hf.py:520 in main   │
│                                                                                                  │
│   517 │                                                                                          │
│   518 │   # Determine whether to continue training without breakpoints or if it is empty, then   │
│   519 │   if auto_resume_from_checkpoint.upper() == "" or auto_resume_from_checkpoint is None:   │
│ ❱ 520 │   │   trainer.train()                                                                    │
│   521 │   else:                                                                                  │
│   522 │   │   output_dir = ft_config.training_args.output_dir                                    │
│   523 │   │   dirlist = os.listdir(output_dir)                                                   │
│                                                                                                  │
│ /home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/transformers/trainer.py:1539 in train    │
│                                                                                                  │
│   1536 │   │   │   finally:                                                                      │
│   1537 │   │   │   │   hf_hub_utils.enable_progress_bars()                                       │
│   1538 │   │   else:                                                                             │
│ ❱ 1539 │   │   │   return inner_training_loop(                                                   │
│   1540 │   │   │   │   args=args,                                                                │
│   1541 │   │   │   │   resume_from_checkpoint=resume_from_checkpoint,                            │
│   1542 │   │   │   │   trial=trial,                                                              │
│                                                                                                  │
│ /home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/transformers/trainer.py:1869 in          │
│ _inner_training_loop                                                                             │
│                                                                                                  │
│   1866 │   │   │   │   │   self.control = self.callback_handler.on_step_begin(args, self.state,  │
│   1867 │   │   │   │                                                                             │
│   1868 │   │   │   │   with self.accelerator.accumulate(model):                                  │
│ ❱ 1869 │   │   │   │   │   tr_loss_step = self.training_step(model, inputs)                      │
│   1870 │   │   │   │                                                                             │
│   1871 │   │   │   │   if (                                                                      │
│   1872 │   │   │   │   │   args.logging_nan_inf_filter                                           │
│                                                                                                  │
│ /home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/transformers/trainer.py:2781 in          │
│ training_step                                                                                    │
│                                                                                                  │
│   2778 │   │   │   with amp.scale_loss(loss, self.optimizer) as scaled_loss:                     │
│   2779 │   │   │   │   scaled_loss.backward()                                                    │
│   2780 │   │   else:                                                                             │
│ ❱ 2781 │   │   │   self.accelerator.backward(loss)                                               │
│   2782 │   │                                                                                     │
│   2783 │   │   return loss.detach() / self.args.gradient_accumulation_steps                      │
│   2784                                                                                           │
│                                                                                                  │
│ /home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/accelerate/accelerator.py:1960 in        │
│ backward                                                                                         │
│                                                                                                  │
│   1957 │   │   │   # deepspeed handles loss scaling by gradient_accumulation_steps in its `back  │
│   1958 │   │   │   loss = loss / self.gradient_accumulation_steps                                │
│   1959 │   │   if self.distributed_type == DistributedType.DEEPSPEED:                            │
│ ❱ 1960 │   │   │   self.deepspeed_engine_wrapped.backward(loss, **kwargs)                        │
│   1961 │   │   elif self.distributed_type == DistributedType.MEGATRON_LM:                        │
│   1962 │   │   │   return                                                                        │
│   1963 │   │   elif self.scaler is not None:                                                     │
│                                                                                                  │
│ /home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/accelerate/utils/deepspeed.py:167 in     │
│ backward                                                                                         │
│                                                                                                  │
│   164 │                                                                                          │
│   165 │   def backward(self, loss, **kwargs):                                                    │
│   166 │   │   # runs backpropagation and handles mixed precision                                 │
│ ❱ 167 │   │   self.engine.backward(loss, **kwargs)                                               │
│   168 │   │                                                                                      │
│   169 │   │   # Deepspeed's `engine.step` performs the following operations:                     │
│   170 │   │   # - gradient accumulation check                                                    │
│                                                                                                  │
│ /home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/deepspeed/utils/nvtx.py:15 in wrapped_fn │
│                                                                                                  │
│   12 │                                                                                           │
│   13 │   def wrapped_fn(*args, **kwargs):                                                        │
│   14 │   │   get_accelerator().range_push(func.__qualname__)                                     │
│ ❱ 15 │   │   ret_val = func(*args, **kwargs)                                                     │
│   16 │   │   get_accelerator().range_pop()                                                       │
│   17 │   │   return ret_val                                                                      │
│   18                                                                                             │
│                                                                                                  │
│ /home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/deepspeed/runtime/engine.py:1974 in      │
│ backward                                                                                         │
│                                                                                                  │
│   1971 │   │                                                                                     │
│   1972 │   │   if self.zero_optimization():                                                      │
│   1973 │   │   │   self.optimizer.is_gradient_accumulation_boundary = self.is_gradient_accumula  │
│ ❱ 1974 │   │   │   self.optimizer.backward(loss, retain_graph=retain_graph)                      │
│   1975 │   │   elif self.amp_enabled():                                                          │
│   1976 │   │   │   # AMP requires delaying unscale when inside gradient accumulation boundaries  │
│   1977 │   │   │   # https://nvidia.github.io/apex/advanced.html#gradient-accumulation-across-i  │
│                                                                                                  │
│ /home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py: │
│ 2052 in backward                                                                                 │
│                                                                                                  │
│   2049 │   │   │                                                                                 │
│   2050 │   │   │   # Use double buffers to avoid data access conflict when overlap_comm is enab  │
│   2051 │   │   │   if self.overlap_comm:                                                         │
│ ❱ 2052 │   │   │   │   buf_1 = torch.empty(int(self.reduce_bucket_size),                         │
│   2053 │   │   │   │   │   │   │   │   │   dtype=self.dtype,                                     │
│   2054 │   │   │   │   │   │   │   │   │   device=get_accelerator().current_device_name())       │
│   2055 │   │   │   │   self.ipg_buffer.append(buf_1)                                             │
╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
OutOfMemoryError: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.15 GiB of which 1.63 GiB is free. Process
2673556 has 34.17 GiB memory in use. Process 4029770 has 26.77 GiB memory in use. Including non-PyTorch memory, this process has 16.55 GiB
memory in use. Of the allocated memory 14.39 GiB is allocated by PyTorch, and 1.35 GiB is reserved by PyTorch but unallocated. If reserved but
unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory
Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
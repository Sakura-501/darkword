
  0%|▎                                                                                                         | 8/3000 [00:02<06:57,  7.18it/s]
{'loss': 4.2894, 'learning_rate': 4.9833333333333336e-05, 'epoch': 0.04}

  1%|▉                                                                                                        | 26/3000 [00:04<05:05,  9.75it/s]
{'loss': 4.5536, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.12}

  2%|█▌                                                                                                       | 45/3000 [00:06<04:58,  9.89it/s]
{'loss': 4.485, 'learning_rate': 4.9166666666666665e-05, 'epoch': 0.2}

  2%|██▏                                                                                                      | 64/3000 [00:08<05:03,  9.66it/s]
{'loss': 3.8615, 'learning_rate': 4.883333333333334e-05, 'epoch': 0.28}

  3%|██▉                                                                                                      | 84/3000 [00:10<05:00,  9.71it/s]
{'loss': 3.46, 'learning_rate': 4.85e-05, 'epoch': 0.36}

  3%|███▌                                                                                                    | 104/3000 [00:12<04:56,  9.78it/s]
{'loss': 3.6524, 'learning_rate': 4.8166666666666674e-05, 'epoch': 0.44}

  4%|████▎                                                                                                   | 123/3000 [00:14<04:56,  9.70it/s]
{'loss': 3.4905, 'learning_rate': 4.7833333333333335e-05, 'epoch': 0.52}

  5%|████▉                                                                                                   | 142/3000 [00:16<05:38,  8.44it/s]
{'loss': 3.3397, 'learning_rate': 4.75e-05, 'epoch': 0.6}

  5%|█████▌                                                                                                  | 161/3000 [00:18<04:56,  9.57it/s]
{'loss': 3.5389, 'learning_rate': 4.716666666666667e-05, 'epoch': 0.68}

  6%|██████▏                                                                                                 | 180/3000 [00:20<05:23,  8.73it/s]
{'loss': 3.3772, 'learning_rate': 4.683333333333334e-05, 'epoch': 0.76}

  7%|██████▉                                                                                                 | 200/3000 [00:22<04:43,  9.87it/s]
{'loss': 3.3162, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.84}

  7%|███████▋                                                                                                | 220/3000 [00:24<04:37, 10.03it/s]
{'loss': 3.3444, 'learning_rate': 4.6166666666666666e-05, 'epoch': 0.92}

  8%|████████▎                                                                                               | 240/3000 [00:26<04:45,  9.68it/s]

  8%|████████▊                                                                                               | 253/3000 [00:28<08:50,  5.18it/s]
{'loss': 3.0949, 'learning_rate': 4.566666666666667e-05, 'epoch': 1.04}

  9%|█████████▍                                                                                              | 271/3000 [00:29<04:42,  9.66it/s]
{'loss': 3.0783, 'learning_rate': 4.5333333333333335e-05, 'epoch': 1.12}

 10%|██████████                                                                                              | 291/3000 [00:32<05:09,  8.76it/s]
{'loss': 2.9073, 'learning_rate': 4.5e-05, 'epoch': 1.2}

 10%|██████████▋                                                                                             | 309/3000 [00:34<05:01,  8.92it/s]
{'loss': 3.2567, 'learning_rate': 4.466666666666667e-05, 'epoch': 1.27}

 11%|███████████▎                                                                                            | 328/3000 [00:36<04:36,  9.65it/s]

 12%|███████████▉                                                                                            | 346/3000 [00:38<04:34,  9.66it/s]
{'loss': 2.7274, 'learning_rate': 4.4166666666666665e-05, 'epoch': 1.39}

 12%|████████████▋                                                                                           | 367/3000 [00:40<04:27,  9.86it/s]
{'loss': 2.7639, 'learning_rate': 4.383333333333334e-05, 'epoch': 1.47}

 13%|█████████████▍                                                                                          | 386/3000 [00:42<04:29,  9.70it/s]
{'loss': 2.9247, 'learning_rate': 4.35e-05, 'epoch': 1.55}

 14%|██████████████                                                                                          | 406/3000 [00:44<04:25,  9.76it/s]
{'loss': 3.0705, 'learning_rate': 4.316666666666667e-05, 'epoch': 1.63}

 14%|██████████████▋                                                                                         | 425/3000 [00:46<04:33,  9.43it/s]
{'loss': 3.0415, 'learning_rate': 4.2833333333333335e-05, 'epoch': 1.71}

 15%|███████████████▍                                                                                        | 445/3000 [00:48<04:30,  9.44it/s]
{'loss': 3.1756, 'learning_rate': 4.25e-05, 'epoch': 1.79}

 16%|████████████████                                                                                        | 465/3000 [00:50<04:17,  9.83it/s]
{'loss': 3.0069, 'learning_rate': 4.216666666666667e-05, 'epoch': 1.87}

 16%|████████████████▊                                                                                       | 484/3000 [00:52<04:10, 10.04it/s]
{'loss': 2.8939, 'learning_rate': 4.183333333333334e-05, 'epoch': 1.95}
 17%|█████████████████▎                                                                                      | 500/3000 [00:53<04:08, 10.06it/s]***** Running Evaluation *****
  Num examples = 50
  Batch size = 16
  0%|                                                                                                                     | 0/2 [00:00<?, ?it/s]
Loading model from cache /tmp/jieba.cache█████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.11s/it]
Loading model cost 0.591 seconds.
Prefix dict has been built successfully.

 17%|█████████████████▎                                                                                      | 500/3000 [01:11<04:08, 10.06it/s]Saving model checkpoint to ../darkword-ChatGLM3-6B/tmp-checkpoint-500
[2024-03-17 06:33:28,929] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step500 is about to be saved!
[2024-03-17 06:33:28,938] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ../darkword-ChatGLM3-6B/tmp-checkpoint-500/global_step500/mp_rank_00_model_states.pt
[2024-03-17 06:33:28,938] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/tmp-checkpoint-500/global_step500/mp_rank_00_model_states.pt...
[2024-03-17 06:33:28,949] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/tmp-checkpoint-500/global_step500/mp_rank_00_model_states.pt.
[2024-03-17 06:33:28,950] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/tmp-checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-17 06:33:28,970] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/tmp-checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-17 06:33:28,971] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved ../darkword-ChatGLM3-6B/tmp-checkpoint-500/global_step500/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-17 06:33:28,971] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!
loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/9addbe01105ca1939dd60a0e5866a1812be9daea/config.json
Model config ChatGLMConfig {
  "_name_or_path": "THUDM/chatglm3-6b",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig",
    "AutoModel": "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1e-05,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_layers": 28,
  "original_rope": true,
  "pad_token_id": 0,
  "padded_vocab_size": 65024,
  "post_layer_norm": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "rmsnorm": true,
  "seq_length": 8192,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65024
}
tokenizer config file saved in ../darkword-ChatGLM3-6B/tmp-checkpoint-500/tokenizer_config.json
Special tokens file saved in ../darkword-ChatGLM3-6B/tmp-checkpoint-500/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
 17%|█████████████████                                                                                     | 503/3000 [01:18<1:58:10,  2.84s/it]
{'loss': 2.9301, 'learning_rate': 4.15e-05, 'epoch': 2.03}

 17%|██████████████████                                                                                      | 521/3000 [01:20<04:46,  8.64it/s]
{'loss': 2.7347, 'learning_rate': 4.116666666666667e-05, 'epoch': 2.11}

 18%|██████████████████▋                                                                                     | 540/3000 [01:22<04:17,  9.57it/s]
{'loss': 2.7656, 'learning_rate': 4.0833333333333334e-05, 'epoch': 2.19}

 19%|███████████████████▍                                                                                    | 559/3000 [01:24<04:12,  9.65it/s]
{'loss': 2.6513, 'learning_rate': 4.05e-05, 'epoch': 2.27}

 19%|████████████████████                                                                                    | 579/3000 [01:26<04:17,  9.39it/s]

 20%|████████████████████▋                                                                                   | 597/3000 [01:28<04:31,  8.86it/s]
{'loss': 3.0905, 'learning_rate': 4e-05, 'epoch': 2.39}

 21%|█████████████████████▎                                                                                  | 616/3000 [01:30<04:17,  9.26it/s]
{'loss': 2.9792, 'learning_rate': 3.966666666666667e-05, 'epoch': 2.47}

 21%|██████████████████████                                                                                  | 635/3000 [01:32<04:07,  9.55it/s]
{'loss': 2.8743, 'learning_rate': 3.933333333333333e-05, 'epoch': 2.55}

 22%|██████████████████████▋                                                                                 | 653/3000 [01:34<04:11,  9.34it/s]
{'loss': 2.5835, 'learning_rate': 3.9000000000000006e-05, 'epoch': 2.63}

 22%|███████████████████████▎                                                                                | 672/3000 [01:36<04:11,  9.26it/s]
{'loss': 2.6688, 'learning_rate': 3.866666666666667e-05, 'epoch': 2.71}

 23%|███████████████████████▉                                                                                | 692/3000 [01:38<03:55,  9.82it/s]
{'loss': 2.3983, 'learning_rate': 3.8333333333333334e-05, 'epoch': 2.79}

 24%|████████████████████████▋                                                                               | 711/3000 [01:40<03:54,  9.77it/s]
{'loss': 2.7167, 'learning_rate': 3.8e-05, 'epoch': 2.87}

 24%|█████████████████████████▎                                                                              | 731/3000 [01:42<03:59,  9.47it/s]
{'loss': 2.8674, 'learning_rate': 3.766666666666667e-05, 'epoch': 2.95}

 25%|██████████████████████████                                                                              | 750/3000 [01:44<03:51,  9.70it/s]

 25%|██████████████████████████▍                                                                             | 762/3000 [01:46<04:18,  8.64it/s]
{'loss': 2.5942, 'learning_rate': 3.7166666666666664e-05, 'epoch': 3.07}

 26%|███████████████████████████                                                                             | 781/3000 [01:48<03:43,  9.93it/s]
{'loss': 2.4072, 'learning_rate': 3.683333333333334e-05, 'epoch': 3.15}

 27%|███████████████████████████▊                                                                            | 801/3000 [01:50<03:49,  9.58it/s]
{'loss': 2.2779, 'learning_rate': 3.65e-05, 'epoch': 3.23}

 27%|████████████████████████████▍                                                                           | 821/3000 [01:52<03:41,  9.84it/s]

 28%|█████████████████████████████                                                                           | 839/3000 [01:54<04:02,  8.92it/s]
{'loss': 2.1276, 'learning_rate': 3.6e-05, 'epoch': 3.35}

 29%|█████████████████████████████▋                                                                          | 858/3000 [01:56<03:42,  9.64it/s]
{'loss': 2.4369, 'learning_rate': 3.566666666666667e-05, 'epoch': 3.43}

 29%|██████████████████████████████▍                                                                         | 877/3000 [01:58<03:41,  9.59it/s]
{'loss': 2.8567, 'learning_rate': 3.5333333333333336e-05, 'epoch': 3.51}

 30%|███████████████████████████████                                                                         | 897/3000 [02:00<03:35,  9.74it/s]
{'loss': 2.4282, 'learning_rate': 3.5e-05, 'epoch': 3.59}

 31%|███████████████████████████████▊                                                                        | 916/3000 [02:02<03:33,  9.76it/s]
{'loss': 2.4698, 'learning_rate': 3.466666666666667e-05, 'epoch': 3.67}

 31%|████████████████████████████████▍                                                                       | 936/3000 [02:04<03:32,  9.71it/s]
{'loss': 2.44, 'learning_rate': 3.433333333333333e-05, 'epoch': 3.75}

 32%|█████████████████████████████████                                                                       | 955/3000 [02:06<03:49,  8.92it/s]
{'loss': 2.8574, 'learning_rate': 3.4000000000000007e-05, 'epoch': 3.82}

 32%|█████████████████████████████████▋                                                                      | 972/3000 [02:08<03:49,  8.84it/s]

 33%|██████████████████████████████████▎                                                                     | 990/3000 [02:10<03:55,  8.55it/s]
{'loss': 2.5276, 'learning_rate': 3.35e-05, 'epoch': 3.94}
 33%|██████████████████████████████████▎                                                                    | 1000/3000 [02:11<03:41,  9.03it/s]***** Running Evaluation *****
  Num examples = 50
  Batch size = 16
  0%|                                                                                                                     | 0/2 [00:00<?, ?it/s]


 33%|██████████████████████████████████▎                                                                    | 1000/3000 [02:29<03:41,  9.03it/s]Saving model checkpoint to ../darkword-ChatGLM3-6B/tmp-checkpoint-1000
[2024-03-17 06:34:48,843] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
[2024-03-17 06:34:48,851] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ../darkword-ChatGLM3-6B/tmp-checkpoint-1000/global_step1000/mp_rank_00_model_states.pt
[2024-03-17 06:34:48,852] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/tmp-checkpoint-1000/global_step1000/mp_rank_00_model_states.pt...
[2024-03-17 06:34:48,871] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/tmp-checkpoint-1000/global_step1000/mp_rank_00_model_states.pt.
[2024-03-17 06:34:48,873] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/tmp-checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-17 06:34:48,892] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/tmp-checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-17 06:34:48,893] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved ../darkword-ChatGLM3-6B/tmp-checkpoint-1000/global_step1000/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-17 06:34:48,893] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--THUDM--chatglm3-6b/snapshots/9addbe01105ca1939dd60a0e5866a1812be9daea/config.json
Model config ChatGLMConfig {
  "_name_or_path": "THUDM/chatglm3-6b",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "THUDM/chatglm3-6b--configuration_chatglm.ChatGLMConfig",
    "AutoModel": "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "THUDM/chatglm3-6b--modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1e-05,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_layers": 28,
  "original_rope": true,
  "pad_token_id": 0,
  "padded_vocab_size": 65024,
  "post_layer_norm": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "rmsnorm": true,
  "seq_length": 8192,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65024
}
tokenizer config file saved in ../darkword-ChatGLM3-6B/tmp-checkpoint-1000/tokenizer_config.json
Special tokens file saved in ../darkword-ChatGLM3-6B/tmp-checkpoint-1000/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
 34%|██████████████████████████████████▌                                                                    | 1007/3000 [02:38<36:52,  1.11s/it]
{'loss': 2.4519, 'learning_rate': 3.316666666666667e-05, 'epoch': 4.02}
 34%|███████████████████████████████████▏                                                                   | 1024/3000 [02:40<03:43,  8.84it/s]
 34%|███████████████████████████████████▏                                                                   | 1024/3000 [02:40<03:43,  8.84it/s]
{'loss': 2.7712, 'learning_rate': 3.283333333333333e-05, 'epoch': 4.1}
 35%|███████████████████████████████████▊                                                                   | 1042/3000 [02:42<03:37,  8.99it/s]
 35%|███████████████████████████████████▊                                                                   | 1042/3000 [02:42<03:37,  8.99it/s]
 35%|████████████████████████████████████▍                                                                  | 1060/3000 [02:44<03:29,  9.26it/s]
 35%|████████████████████████████████████▍                                                                  | 1060/3000 [02:44<03:29,  9.26it/s]
{'loss': 2.4353, 'learning_rate': 3.233333333333333e-05, 'epoch': 4.22}
 36%|█████████████████████████████████████                                                                  | 1079/3000 [02:46<03:32,  9.02it/s]
 36%|█████████████████████████████████████                                                                  | 1079/3000 [02:46<03:32,  9.02it/s]
{'loss': 2.377, 'learning_rate': 3.2000000000000005e-05, 'epoch': 4.3}
 37%|█████████████████████████████████████▋                                                                 | 1097/3000 [02:48<03:29,  9.07it/s]
 37%|█████████████████████████████████████▋                                                                 | 1097/3000 [02:48<03:29,  9.07it/s]
{'loss': 2.225, 'learning_rate': 3.1666666666666666e-05, 'epoch': 4.38}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.2189, 'learning_rate': 3.1333333333333334e-05, 'epoch': 4.46}
{'loss': 2.3474, 'learning_rate': 3.116666666666667e-05, 'epoch': 4.5}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.6222, 'learning_rate': 3.1e-05, 'epoch': 4.54}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.193, 'learning_rate': 3.0833333333333335e-05, 'epoch': 4.58}
{'loss': 2.2877, 'learning_rate': 3.066666666666667e-05, 'epoch': 4.62}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.4557, 'learning_rate': 3.05e-05, 'epoch': 4.66}
{'loss': 2.9045, 'learning_rate': 3.0333333333333337e-05, 'epoch': 4.7}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.9223, 'learning_rate': 3.016666666666667e-05, 'epoch': 4.74}
{'loss': 2.3364, 'learning_rate': 3e-05, 'epoch': 4.78}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.2191, 'learning_rate': 2.9833333333333335e-05, 'epoch': 4.82}
{'loss': 2.2859, 'learning_rate': 2.9666666666666672e-05, 'epoch': 4.86}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.2683, 'learning_rate': 2.95e-05, 'epoch': 4.9}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.0733, 'learning_rate': 2.9333333333333336e-05, 'epoch': 4.94}
{'loss': 2.451, 'learning_rate': 2.916666666666667e-05, 'epoch': 4.98}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.193, 'learning_rate': 2.9e-05, 'epoch': 5.02}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.1391, 'learning_rate': 2.8833333333333334e-05, 'epoch': 5.06}
{'loss': 2.3383, 'learning_rate': 2.8666666666666668e-05, 'epoch': 5.1}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.7197, 'learning_rate': 2.8499999999999998e-05, 'epoch': 5.14}
{'loss': 2.4872, 'learning_rate': 2.8333333333333335e-05, 'epoch': 5.18}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.8874, 'learning_rate': 2.816666666666667e-05, 'epoch': 5.22}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.0205, 'learning_rate': 2.8000000000000003e-05, 'epoch': 5.26}
{'loss': 2.6285, 'learning_rate': 2.7833333333333333e-05, 'epoch': 5.3}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.3684, 'learning_rate': 2.7666666666666667e-05, 'epoch': 5.34}
{'loss': 2.3502, 'learning_rate': 2.7500000000000004e-05, 'epoch': 5.38}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.0848, 'learning_rate': 2.733333333333333e-05, 'epoch': 5.42}
{'loss': 1.8547, 'learning_rate': 2.716666666666667e-05, 'epoch': 5.46}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.2205, 'learning_rate': 2.7000000000000002e-05, 'epoch': 5.5}
{'loss': 2.5387, 'learning_rate': 2.6833333333333333e-05, 'epoch': 5.54}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.0483, 'learning_rate': 2.6666666666666667e-05, 'epoch': 5.58}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.5368, 'learning_rate': 2.6500000000000004e-05, 'epoch': 5.62}
{'loss': 2.4278, 'learning_rate': 2.633333333333333e-05, 'epoch': 5.66}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.5873, 'learning_rate': 2.6166666666666668e-05, 'epoch': 5.7}
{'loss': 1.9906, 'learning_rate': 2.6000000000000002e-05, 'epoch': 5.74}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.235, 'learning_rate': 2.5833333333333336e-05, 'epoch': 5.78}
{'loss': 1.7959, 'learning_rate': 2.5666666666666666e-05, 'epoch': 5.82}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.1259, 'learning_rate': 2.5500000000000003e-05, 'epoch': 5.86}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.7185, 'learning_rate': 2.5333333333333337e-05, 'epoch': 5.9}
{'loss': 1.9732, 'learning_rate': 2.5166666666666667e-05, 'epoch': 5.94}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.4593, 'learning_rate': 2.5e-05, 'epoch': 5.98}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'eval_rouge-1': 32.5495, 'eval_rouge-2': 14.5090015625, 'eval_rouge-l': 28.0764328125, 'eval_bleu-4': 0.10796582947841234, 'eval_runtime': 18.035, 'eval_samples_per_second': 2.772, 'eval_steps_per_second': 0.111, 'epoch': 5.98}
 37%|██████████████████████████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
[2024-03-17 06:36:11,947] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1500 is about to be saved!
[2024-03-17 06:36:11,956] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ../darkword-ChatGLM3-6B/tmp-checkpoint-1500/global_step1500/mp_rank_00_model_states.pt
[2024-03-17 06:36:11,956] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/tmp-checkpoint-1500/global_step1500/mp_rank_00_model_states.pt...
[2024-03-17 06:36:11,979] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/tmp-checkpoint-1500/global_step1500/mp_rank_00_model_states.pt.
[2024-03-17 06:36:11,980] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/tmp-checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-17 06:36:12,005] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/tmp-checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-17 06:36:12,005] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved ../darkword-ChatGLM3-6B/tmp-checkpoint-1500/global_step1500/zero_pp_rank_0_mp_rank_00_optim_states.pt
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.0019, 'learning_rate': 2.4833333333333335e-05, 'epoch': 6.02}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.8918, 'learning_rate': 2.466666666666667e-05, 'epoch': 6.06}
{'loss': 2.2127, 'learning_rate': 2.45e-05, 'epoch': 6.1}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.429, 'learning_rate': 2.4333333333333336e-05, 'epoch': 6.14}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.0425, 'learning_rate': 2.4166666666666667e-05, 'epoch': 6.18}
{'loss': 2.1599, 'learning_rate': 2.4e-05, 'epoch': 6.22}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.0419, 'learning_rate': 2.3833333333333334e-05, 'epoch': 6.25}
{'loss': 2.3535, 'learning_rate': 2.3666666666666668e-05, 'epoch': 6.29}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.1044, 'learning_rate': 2.35e-05, 'epoch': 6.33}
{'loss': 1.556, 'learning_rate': 2.3333333333333336e-05, 'epoch': 6.37}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.6537, 'learning_rate': 2.3166666666666666e-05, 'epoch': 6.41}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.0064, 'learning_rate': 2.3000000000000003e-05, 'epoch': 6.45}
{'loss': 2.2687, 'learning_rate': 2.2833333333333334e-05, 'epoch': 6.49}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.2424, 'learning_rate': 2.2666666666666668e-05, 'epoch': 6.53}
{'loss': 1.7302, 'learning_rate': 2.25e-05, 'epoch': 6.57}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.2265, 'learning_rate': 2.2333333333333335e-05, 'epoch': 6.61}
{'loss': 1.7322, 'learning_rate': 2.216666666666667e-05, 'epoch': 6.65}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.8933, 'learning_rate': 2.2000000000000003e-05, 'epoch': 6.69}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.9386, 'learning_rate': 2.1833333333333333e-05, 'epoch': 6.73}
{'loss': 2.4579, 'learning_rate': 2.1666666666666667e-05, 'epoch': 6.77}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.8202, 'learning_rate': 2.15e-05, 'epoch': 6.81}
{'loss': 1.9137, 'learning_rate': 2.1333333333333335e-05, 'epoch': 6.85}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.9655, 'learning_rate': 2.116666666666667e-05, 'epoch': 6.89}
{'loss': 2.1033, 'learning_rate': 2.1e-05, 'epoch': 6.93}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.8939, 'learning_rate': 2.0833333333333336e-05, 'epoch': 6.97}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.6838, 'learning_rate': 2.0666666666666666e-05, 'epoch': 7.01}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.0114, 'learning_rate': 2.05e-05, 'epoch': 7.05}
{'loss': 2.1838, 'learning_rate': 2.0333333333333334e-05, 'epoch': 7.09}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.2547, 'learning_rate': 2.0166666666666668e-05, 'epoch': 7.13}
{'loss': 2.1686, 'learning_rate': 2e-05, 'epoch': 7.17}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.1811, 'learning_rate': 1.9833333333333335e-05, 'epoch': 7.21}
{'loss': 1.9423, 'learning_rate': 1.9666666666666666e-05, 'epoch': 7.25}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.8192, 'learning_rate': 1.9500000000000003e-05, 'epoch': 7.29}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.6782, 'learning_rate': 1.9333333333333333e-05, 'epoch': 7.33}
{'loss': 2.2538, 'learning_rate': 1.9166666666666667e-05, 'epoch': 7.37}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.006, 'learning_rate': 1.9e-05, 'epoch': 7.41}
{'loss': 1.3861, 'learning_rate': 1.8833333333333335e-05, 'epoch': 7.45}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.0097, 'learning_rate': 1.866666666666667e-05, 'epoch': 7.49}
{'loss': 1.6564, 'learning_rate': 1.85e-05, 'epoch': 7.53}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.1862, 'learning_rate': 1.8333333333333333e-05, 'epoch': 7.57}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.4985, 'learning_rate': 1.8166666666666667e-05, 'epoch': 7.61}
{'loss': 2.4316, 'learning_rate': 1.8e-05, 'epoch': 7.65}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.0041, 'learning_rate': 1.7833333333333334e-05, 'epoch': 7.69}
{'loss': 2.1392, 'learning_rate': 1.7666666666666668e-05, 'epoch': 7.73}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.1656, 'learning_rate': 1.75e-05, 'epoch': 7.77}
{'loss': 1.9953, 'learning_rate': 1.7333333333333336e-05, 'epoch': 7.81}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.0186, 'learning_rate': 1.7166666666666666e-05, 'epoch': 7.85}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.785, 'learning_rate': 1.7000000000000003e-05, 'epoch': 7.89}
{'loss': 2.0944, 'learning_rate': 1.6833333333333334e-05, 'epoch': 7.93}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.9503, 'learning_rate': 1.6666666666666667e-05, 'epoch': 7.97}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'eval_rouge-1': 33.267159375, 'eval_rouge-2': 16.0434703125, 'eval_rouge-l': 28.649215625, 'eval_bleu-4': 0.11678074340816362, 'eval_runtime': 17.9529, 'eval_samples_per_second': 2.785, 'eval_steps_per_second': 0.111, 'epoch': 7.97}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
[2024-03-17 06:37:36,222] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2000 is about to be saved!
[2024-03-17 06:37:36,232] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ../darkword-ChatGLM3-6B/tmp-checkpoint-2000/global_step2000/mp_rank_00_model_states.pt
[2024-03-17 06:37:36,232] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/tmp-checkpoint-2000/global_step2000/mp_rank_00_model_states.pt...
[2024-03-17 06:37:36,252] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/tmp-checkpoint-2000/global_step2000/mp_rank_00_model_states.pt.
[2024-03-17 06:37:36,253] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/tmp-checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-17 06:37:36,273] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/tmp-checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-17 06:37:36,273] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved ../darkword-ChatGLM3-6B/tmp-checkpoint-2000/global_step2000/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-17 06:37:36,274] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.6354, 'learning_rate': 1.65e-05, 'epoch': 8.01}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.1573, 'learning_rate': 1.6333333333333335e-05, 'epoch': 8.05}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.007, 'learning_rate': 1.6166666666666665e-05, 'epoch': 8.09}
{'loss': 1.9373, 'learning_rate': 1.6000000000000003e-05, 'epoch': 8.13}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.1012, 'learning_rate': 1.5833333333333333e-05, 'epoch': 8.17}
{'loss': 1.6909, 'learning_rate': 1.5666666666666667e-05, 'epoch': 8.21}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.8594, 'learning_rate': 1.55e-05, 'epoch': 8.25}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.0571, 'learning_rate': 1.5333333333333334e-05, 'epoch': 8.29}
{'loss': 1.6583, 'learning_rate': 1.5166666666666668e-05, 'epoch': 8.33}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.0916, 'learning_rate': 1.5e-05, 'epoch': 8.37}
{'loss': 1.8123, 'learning_rate': 1.4833333333333336e-05, 'epoch': 8.41}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.2296, 'learning_rate': 1.4666666666666668e-05, 'epoch': 8.45}
{'loss': 2.1158, 'learning_rate': 1.45e-05, 'epoch': 8.49}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.9662, 'learning_rate': 1.4333333333333334e-05, 'epoch': 8.53}
{'loss': 2.106, 'learning_rate': 1.4166666666666668e-05, 'epoch': 8.57}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.6092, 'learning_rate': 1.4000000000000001e-05, 'epoch': 8.61}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.1021, 'learning_rate': 1.3833333333333334e-05, 'epoch': 8.65}
{'loss': 1.5899, 'learning_rate': 1.3666666666666666e-05, 'epoch': 8.69}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.2595, 'learning_rate': 1.3500000000000001e-05, 'epoch': 8.73}
{'loss': 1.7872, 'learning_rate': 1.3333333333333333e-05, 'epoch': 8.76}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.8506, 'learning_rate': 1.3166666666666665e-05, 'epoch': 8.8}
{'loss': 1.8438, 'learning_rate': 1.3000000000000001e-05, 'epoch': 8.84}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.6921, 'learning_rate': 1.2833333333333333e-05, 'epoch': 8.88}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.8303, 'learning_rate': 1.2666666666666668e-05, 'epoch': 8.92}
{'loss': 1.6382, 'learning_rate': 1.25e-05, 'epoch': 8.96}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.341, 'learning_rate': 1.2333333333333334e-05, 'epoch': 9.0}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.2887, 'learning_rate': 1.2166666666666668e-05, 'epoch': 9.04}
{'loss': 2.0219, 'learning_rate': 1.2e-05, 'epoch': 9.08}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.9221, 'learning_rate': 1.1833333333333334e-05, 'epoch': 9.12}
{'loss': 2.0011, 'learning_rate': 1.1666666666666668e-05, 'epoch': 9.16}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.9875, 'learning_rate': 1.1500000000000002e-05, 'epoch': 9.2}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.7206, 'learning_rate': 1.1333333333333334e-05, 'epoch': 9.24}
{'loss': 1.9255, 'learning_rate': 1.1166666666666668e-05, 'epoch': 9.28}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.0432, 'learning_rate': 1.1000000000000001e-05, 'epoch': 9.32}
{'loss': 1.8904, 'learning_rate': 1.0833333333333334e-05, 'epoch': 9.36}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.6444, 'learning_rate': 1.0666666666666667e-05, 'epoch': 9.4}
{'loss': 1.4805, 'learning_rate': 1.05e-05, 'epoch': 9.44}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.9101, 'learning_rate': 1.0333333333333333e-05, 'epoch': 9.48}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.3704, 'learning_rate': 1.0166666666666667e-05, 'epoch': 9.52}
{'loss': 2.0185, 'learning_rate': 1e-05, 'epoch': 9.56}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.1872, 'learning_rate': 9.833333333333333e-06, 'epoch': 9.6}
{'loss': 1.5246, 'learning_rate': 9.666666666666667e-06, 'epoch': 9.64}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.3997, 'learning_rate': 9.5e-06, 'epoch': 9.68}
{'loss': 1.7853, 'learning_rate': 9.333333333333334e-06, 'epoch': 9.72}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.8037, 'learning_rate': 9.166666666666666e-06, 'epoch': 9.76}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.731, 'learning_rate': 9e-06, 'epoch': 9.8}
{'loss': 1.9888, 'learning_rate': 8.833333333333334e-06, 'epoch': 9.84}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.7607, 'learning_rate': 8.666666666666668e-06, 'epoch': 9.88}
{'loss': 1.9375, 'learning_rate': 8.500000000000002e-06, 'epoch': 9.92}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.6881, 'learning_rate': 8.333333333333334e-06, 'epoch': 9.96}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'eval_rouge-1': 34.924125000000004, 'eval_rouge-2': 17.3413890625, 'eval_rouge-l': 30.5589765625, 'eval_bleu-4': 0.13785850458073326, 'eval_runtime': 18.5839, 'eval_samples_per_second': 2.691, 'eval_steps_per_second': 0.108, 'epoch': 9.96}
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
  "seq_length": 8192,13696,████████████████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
[2024-03-17 06:39:00,833] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2500 is about to be saved!
[2024-03-17 06:39:00,842] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ../darkword-ChatGLM3-6B/tmp-checkpoint-2500/global_step2500/mp_rank_00_model_states.pt
[2024-03-17 06:39:00,843] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/tmp-checkpoint-2500/global_step2500/mp_rank_00_model_states.pt...
[2024-03-17 06:39:00,866] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/tmp-checkpoint-2500/global_step2500/mp_rank_00_model_states.pt.
[2024-03-17 06:39:00,868] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/tmp-checkpoint-2500/global_step2500/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-17 06:39:00,895] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/tmp-checkpoint-2500/global_step2500/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-17 06:39:00,895] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved ../darkword-ChatGLM3-6B/tmp-checkpoint-2500/global_step2500/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-17 06:39:00,895] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2500 is ready now!
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.6169, 'learning_rate': 8.000000000000001e-06, 'epoch': 10.04}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.0171, 'learning_rate': 7.833333333333333e-06, 'epoch': 10.08}
{'loss': 2.4042, 'learning_rate': 7.666666666666667e-06, 'epoch': 10.12}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.5771, 'learning_rate': 7.5e-06, 'epoch': 10.16}
{'loss': 2.0272, 'learning_rate': 7.333333333333334e-06, 'epoch': 10.2}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.8006, 'learning_rate': 7.166666666666667e-06, 'epoch': 10.24}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.0263, 'learning_rate': 7.000000000000001e-06, 'epoch': 10.28}
{'loss': 1.4958, 'learning_rate': 6.833333333333333e-06, 'epoch': 10.32}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.6026, 'learning_rate': 6.666666666666667e-06, 'epoch': 10.36}
{'loss': 1.8548, 'learning_rate': 6.5000000000000004e-06, 'epoch': 10.4}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.8417, 'learning_rate': 6.333333333333334e-06, 'epoch': 10.44}
{'loss': 1.7097, 'learning_rate': 6.166666666666667e-06, 'epoch': 10.48}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.4586, 'learning_rate': 6e-06, 'epoch': 10.52}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.0157, 'learning_rate': 5.833333333333334e-06, 'epoch': 10.56}
{'loss': 2.0694, 'learning_rate': 5.666666666666667e-06, 'epoch': 10.6}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.4996, 'learning_rate': 5.500000000000001e-06, 'epoch': 10.64}
{'loss': 1.5495, 'learning_rate': 5.333333333333334e-06, 'epoch': 10.68}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.6774, 'learning_rate': 5.166666666666667e-06, 'epoch': 10.72}
{'loss': 1.8692, 'learning_rate': 5e-06, 'epoch': 10.76}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.198, 'learning_rate': 4.833333333333333e-06, 'epoch': 10.8}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.0853, 'learning_rate': 4.666666666666667e-06, 'epoch': 10.84}
{'loss': 1.9449, 'learning_rate': 4.5e-06, 'epoch': 10.88}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.2218, 'learning_rate': 4.333333333333334e-06, 'epoch': 10.92}
{'loss': 1.3532, 'learning_rate': 4.166666666666667e-06, 'epoch': 10.96}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.3347, 'learning_rate': 4.000000000000001e-06, 'epoch': 11.0}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.2545, 'learning_rate': 3.833333333333334e-06, 'epoch': 11.04}
{'loss': 1.7654, 'learning_rate': 3.666666666666667e-06, 'epoch': 11.08}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.8417, 'learning_rate': 3.5000000000000004e-06, 'epoch': 11.12}
{'loss': 1.5471, 'learning_rate': 3.3333333333333333e-06, 'epoch': 11.16}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.7099, 'learning_rate': 3.166666666666667e-06, 'epoch': 11.2}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.6738, 'learning_rate': 3e-06, 'epoch': 11.24}
{'loss': 1.8233, 'learning_rate': 2.8333333333333335e-06, 'epoch': 11.27}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.6344, 'learning_rate': 2.666666666666667e-06, 'epoch': 11.31}
{'loss': 1.9661, 'learning_rate': 2.5e-06, 'epoch': 11.35}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.4161, 'learning_rate': 2.3333333333333336e-06, 'epoch': 11.39}
{'loss': 1.7856, 'learning_rate': 2.166666666666667e-06, 'epoch': 11.43}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.8543, 'learning_rate': 2.0000000000000003e-06, 'epoch': 11.47}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.5541, 'learning_rate': 1.8333333333333335e-06, 'epoch': 11.51}
{'loss': 1.7068, 'learning_rate': 1.6666666666666667e-06, 'epoch': 11.55}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.6838, 'learning_rate': 1.5e-06, 'epoch': 11.59}
{'loss': 1.6727, 'learning_rate': 1.3333333333333334e-06, 'epoch': 11.63}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 2.2316, 'learning_rate': 1.1666666666666668e-06, 'epoch': 11.67}
{'loss': 1.6718, 'learning_rate': 1.0000000000000002e-06, 'epoch': 11.71}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.5381, 'learning_rate': 8.333333333333333e-07, 'epoch': 11.75}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.5227, 'learning_rate': 6.666666666666667e-07, 'epoch': 11.79}
{'loss': 1.6207, 'learning_rate': 5.000000000000001e-07, 'epoch': 11.83}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.7419, 'learning_rate': 3.3333333333333335e-07, 'epoch': 11.87}
{'loss': 1.6685, 'learning_rate': 1.6666666666666668e-07, 'epoch': 11.91}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'loss': 1.8867, 'learning_rate': 0.0, 'epoch': 11.95}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
{'eval_rouge-1': 34.129128125, 'eval_rouge-2': 17.334921875, 'eval_rouge-l': 29.55728125, 'eval_bleu-4': 0.1357285826458792, 'eval_runtime': 17.8978, 'eval_samples_per_second': 2.794, 'eval_steps_per_second': 0.112, 'epoch': 11.95}
  "tie_word_embeddings": false,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
[2024-03-17 06:40:24,318] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3000 is about to be saved!
[2024-03-17 06:40:24,330] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: ../darkword-ChatGLM3-6B/tmp-checkpoint-3000/global_step3000/mp_rank_00_model_states.pt
[2024-03-17 06:40:24,330] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/tmp-checkpoint-3000/global_step3000/mp_rank_00_model_states.pt...
[2024-03-17 06:40:24,347] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/tmp-checkpoint-3000/global_step3000/mp_rank_00_model_states.pt.
[2024-03-17 06:40:24,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ../darkword-ChatGLM3-6B/tmp-checkpoint-3000/global_step3000/zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-17 06:40:24,374] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ../darkword-ChatGLM3-6B/tmp-checkpoint-3000/global_step3000/zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-17 06:40:24,375] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved ../darkword-ChatGLM3-6B/tmp-checkpoint-3000/global_step3000/zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-17 06:40:24,375] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
  Num examples = 562,13696,lse,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
  Num examples = 562,13696,lse,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
  Num examples = 562,13696,lse,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
  Num examples = 562,13696,lse,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
  Num examples = 562,13696,lse,alse,███████▎                                                                | 1115/3000 [02:50<03:26,  9.12it/s]
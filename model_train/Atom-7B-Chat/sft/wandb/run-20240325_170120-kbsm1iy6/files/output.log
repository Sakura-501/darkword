  0%|                                                                                                                                                                        | 0/1050 [00:00<?, ?it/s]/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
03/25/2024 17:01:29 - WARNING - transformers_modules.FlagAlpha.Atom-7B-Chat.52b7761650cc65f24f8ae832184947906af0c337.model_atom - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1303: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])









  1%|█▍                                                                                                                                                           | 10/1050 [01:28<2:30:17,  8.67s/it]










  2%|██▊                                                                                                                                                          | 19/1050 [02:46<2:26:36,  8.53s/it]
  2%|██▉                                                                                                                                                          | 20/1050 [02:55<2:25:02,  8.45s/it][INFO|trainer.py:3242] 2024-03-25 17:04:24,566 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 17:04:24,566 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 17:04:24,566 >>   Batch size = 1
















 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏      | 90/94 [00:30<00:01,  3.62it/s]
  2%|██▉                                                                                                                                                          | 20/1050 [03:28<2:25:02,  8.45s/it][INFO|trainer.py:2936] 2024-03-25 17:05:01,552 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-20
[INFO|configuration_utils.py:729] 2024-03-25 17:05:01,924 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:05:01,925 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 17:05:01,998 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-20/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 17:05:01,999 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-20/special_tokens_map.json
[2024-03-25 17:05:02,318] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step20 is about to be saved!
[2024-03-25 17:05:02,344] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-20/global_step20/mp_rank_00_model_states.pt
[2024-03-25 17:05:02,345] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-20/global_step20/mp_rank_00_model_states.pt...
[2024-03-25 17:05:02,450] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-20/global_step20/mp_rank_00_model_states.pt.
[2024-03-25 17:05:02,453] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-20/global_step20/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 17:05:02,861] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-20/global_step20/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 17:05:02,862] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-20/global_step20/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 17:05:02,862] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20 is ready now!
+++++++++++++++++save call back++++++++++++++++
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 17:05:03,495 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:05:03,496 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  2%|███▎                                                                                                                                                         | 22/1050 [03:50<4:44:35, 16.61s/it]
  2%|███▍                                                                                                                                                         | 23/1050 [04:00<4:05:47, 14.36s/it]
  2%|███▌                                                                                                                                                         | 24/1050 [04:08<3:34:48, 12.56s/it]
  2%|███▋                                                                                                                                                         | 25/1050 [04:17<3:16:37, 11.51s/it]
  2%|███▉                                                                                                                                                         | 26/1050 [04:25<2:59:37, 10.53s/it]
  3%|████                                                                                                                                                         | 27/1050 [04:34<2:52:17, 10.10s/it]
  3%|████▏                                                                                                                                                        | 28/1050 [04:43<2:45:17,  9.70s/it]
  3%|████▎                                                                                                                                                        | 29/1050 [04:52<2:42:10,  9.53s/it]
  3%|████▎                                                                                                                                                        | 29/1050 [04:52<2:42:10,  9.53s/it]
  3%|████▍                                                                                                                                                        | 30/1050 [05:01<2:36:07,  9.18s/it]
  3%|████▋                                                                                                                                                        | 31/1050 [05:09<2:31:35,  8.93s/it]
  3%|████▊                                                                                                                                                        | 32/1050 [05:18<2:33:00,  9.02s/it]
  3%|████▉                                                                                                                                                        | 33/1050 [05:27<2:30:49,  8.90s/it]
  3%|█████                                                                                                                                                        | 34/1050 [05:36<2:31:02,  8.92s/it]
  3%|█████▏                                                                                                                                                       | 35/1050 [05:44<2:27:02,  8.69s/it]
  3%|█████▍                                                                                                                                                       | 36/1050 [05:53<2:29:08,  8.83s/it]
  4%|█████▌                                                                                                                                                       | 37/1050 [06:01<2:25:55,  8.64s/it]
  4%|█████▋                                                                                                                                                       | 38/1050 [06:10<2:28:02,  8.78s/it]
  4%|█████▊                                                                                                                                                       | 39/1050 [06:19<2:25:28,  8.63s/it]
  4%|█████▊                                                                                                                                                       | 39/1050 [06:19<2:25:28,  8.63s/it]
  0%|                                                                                                                                                                          | 0/94 [00:00<?, ?it/s][INFO|trainer.py:3242] 2024-03-25 17:07:58,030 >> ***** Running Evaluation *****
  5%|████████▌                                                                                                                                                         | 5/94 [00:01<00:46,  1.91it/s][INFO|trainer.py:3242] 2024-03-25 17:07:58,030 >> ***** Running Evaluation *****
 14%|██████████████████████▎                                                                                                                                          | 13/94 [00:04<00:23,  3.51it/s][INFO|trainer.py:3242] 2024-03-25 17:07:58,030 >> ***** Running Evaluation *****
 18%|█████████████████████████████                                                                                                                                    | 17/94 [00:06<00:28,  2.69it/s][INFO|trainer.py:3242] 2024-03-25 17:07:58,030 >> ***** Running Evaluation *****
 24%|███████████████████████████████████████▍                                                                                                                         | 23/94 [00:07<00:22,  3.18it/s][INFO|trainer.py:3242] 2024-03-25 17:07:58,030 >> ***** Running Evaluation *****
 30%|███████████████████████████████████████████████▉                                                                                                                 | 28/94 [00:09<00:21,  3.01it/s][INFO|trainer.py:3242] 2024-03-25 17:07:58,030 >> ***** Running Evaluation *****
 35%|████████████████████████████████████████████████████████▌                                                                                                        | 33/94 [00:12<00:32,  1.88it/s][INFO|trainer.py:3242] 2024-03-25 17:07:58,030 >> ***** Running Evaluation *****
 43%|████████████████████████████████████████████████████████████████████▌                                                                                            | 40/94 [00:13<00:15,  3.48it/s][INFO|trainer.py:3242] 2024-03-25 17:07:58,030 >> ***** Running Evaluation *****
 48%|█████████████████████████████████████████████████████████████████████████████                                                                                    | 45/94 [00:16<00:18,  2.59it/s][INFO|trainer.py:3242] 2024-03-25 17:07:58,030 >> ***** Running Evaluation *****
 54%|███████████████████████████████████████████████████████████████████████████████████████▎                                                                         | 51/94 [00:17<00:11,  3.58it/s][INFO|trainer.py:3242] 2024-03-25 17:07:58,030 >> ***** Running Evaluation *****
 61%|█████████████████████████████████████████████████████████████████████████████████████████████████▋                                                               | 57/94 [00:20<00:11,  3.12it/s][INFO|trainer.py:3242] 2024-03-25 17:07:58,030 >> ***** Running Evaluation *****
 65%|████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                        | 61/94 [00:21<00:09,  3.53it/s][INFO|trainer.py:3242] 2024-03-25 17:07:58,030 >> ***** Running Evaluation *****
 72%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                            | 68/94 [00:23<00:07,  3.31it/s][INFO|trainer.py:3242] 2024-03-25 17:07:58,030 >> ***** Running Evaluation *****
 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                    | 73/94 [00:26<00:08,  2.46it/s][INFO|trainer.py:3242] 2024-03-25 17:07:58,030 >> ***** Running Evaluation *****
 84%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                         | 79/94 [00:27<00:04,  3.46it/s][INFO|trainer.py:3242] 2024-03-25 17:07:58,030 >> ***** Running Evaluation *****
 90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌               | 85/94 [00:30<00:02,  3.18it/s][INFO|trainer.py:3242] 2024-03-25 17:07:58,030 >> ***** Running Evaluation *****
 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏      | 90/94 [00:32<00:01,  2.32it/s][INFO|trainer.py:3242] 2024-03-25 17:07:58,030 >> ***** Running Evaluation *****
 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏      | 90/94 [00:32<00:01,  2.32it/s][INFO|trainer.py:3242] 2024-03-25 17:07:58,030 >> ***** Running Evaluation *****
                                                                                                                                                                                                      [INFO|trainer.py:3242] 2024-03-25 17:07:58,030 >> ***** Running Evaluation *****
                                                                                                                                                                                                      [INFO|trainer.py:3242] 2024-03-25 17:07:58,030 >> ***** Running Evaluation *****
[2024-03-25 17:08:35,716] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step40 is about to be saved!
[2024-03-25 17:08:35,743] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-40/global_step40/mp_rank_00_model_states.pt
[2024-03-25 17:08:35,743] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-40/global_step40/mp_rank_00_model_states.pt...
[2024-03-25 17:08:35,851] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-40/global_step40/mp_rank_00_model_states.pt.
[2024-03-25 17:08:35,853] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-40/global_step40/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 17:08:36,192] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-40/global_step40/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 17:08:36,192] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-40/global_step40/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 17:08:36,193] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40 is ready now!
  "rope_theta": 500000,, 32, 4096,ation": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"e at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json69 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-40
  "rope_theta": 500000,, 32, 4096,ation": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"e at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json69 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-40
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 17:08:35,502 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-40/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 17:08:35,503 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-40/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 17:08:36,567 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:08:36,568 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








  5%|███████▎                                                                                                                                                     | 49/1050 [08:24<2:33:18,  9.19s/it]










  6%|████████▊                                                                                                                                                    | 59/1050 [09:51<2:23:15,  8.67s/it]
  6%|████████▉                                                                                                                                                    | 60/1050 [09:59<2:21:13,  8.56s/it][INFO|trainer.py:3242] 2024-03-25 17:11:28,820 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 17:11:28,820 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 17:11:28,820 >>   Batch size = 1















 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 91/94 [00:31<00:00,  3.33it/s]


  6%|████████▉                                                                                                                                                    | 60/1050 [10:31<2:21:13,  8.56s/it][INFO|trainer.py:2936] 2024-03-25 17:12:04,676 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-60
[2024-03-25 17:12:05,460] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step60 is about to be saved!
[2024-03-25 17:12:05,491] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-60/global_step60/mp_rank_00_model_states.pt
[2024-03-25 17:12:05,491] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-60/global_step60/mp_rank_00_model_states.pt...
[2024-03-25 17:12:05,589] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-60/global_step60/mp_rank_00_model_states.pt.
[2024-03-25 17:12:05,591] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-60/global_step60/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 17:12:06,002] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-60/global_step60/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 17:12:06,003] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-60/global_step60/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 17:12:06,003] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step60 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-25 17:12:05,168 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:12:05,169 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 17:12:05,245 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-60/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 17:12:05,246 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-60/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 17:12:06,376 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:12:06,377 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









  7%|██████████▍                                                                                                                                                  | 70/1050 [12:01<2:23:41,  8.80s/it]









  8%|███████████▊                                                                                                                                                 | 79/1050 [13:18<2:19:46,  8.64s/it]
  8%|███████████▉                                                                                                                                                 | 80/1050 [13:27<2:18:36,  8.57s/it][INFO|trainer.py:3242] 2024-03-25 17:14:56,671 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 17:14:56,671 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 17:14:56,671 >>   Batch size = 1















 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 91/94 [00:31<00:01,  2.44it/s]

{'eval_loss': 2.3475887775421143, 'eval_accuracy': 0.8324553601970308, 'eval_runtime': 33.0283, 'eval_samples_per_second': 2.846, 'eval_steps_per_second': 2.846, 'epoch': 0.76}
[2024-03-25 17:15:33,784] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step80 is about to be saved!
[2024-03-25 17:15:33,810] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-80/global_step80/mp_rank_00_model_states.pt
[2024-03-25 17:15:33,810] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-80/global_step80/mp_rank_00_model_states.pt...
[2024-03-25 17:15:33,899] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-80/global_step80/mp_rank_00_model_states.pt.
[2024-03-25 17:15:33,901] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-80/global_step80/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 17:15:34,283] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-80/global_step80/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 17:15:34,284] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-80/global_step80/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 17:15:34,284] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step80 is ready now!
  8%|███████████▉                                                                                                                                                 | 80/1050 [14:00<2:18:36,  8.57s/it][INFO|trainer.py:2936] 2024-03-25 17:15:33,072 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-80
[INFO|configuration_utils.py:729] 2024-03-25 17:15:33,498 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:15:33,499 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 17:15:33,568 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-80/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 17:15:33,568 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-80/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 17:15:34,657 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:15:34,658 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








  8%|█████████████▎                                                                                                                                               | 89/1050 [15:21<2:25:59,  9.11s/it]










  9%|██████████████▊                                                                                                                                              | 99/1050 [16:48<2:17:07,  8.65s/it]
 10%|██████████████▊                                                                                                                                             | 100/1050 [16:57<2:15:16,  8.54s/it][INFO|trainer.py:3242] 2024-03-25 17:18:26,563 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 17:18:26,563 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 17:18:26,563 >>   Batch size = 1















 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 91/94 [00:31<00:01,  2.24it/s]

{'eval_loss': 2.291584014892578, 'eval_accuracy': 0.834302524457823, 'eval_runtime': 33.5629, 'eval_samples_per_second': 2.801, 'eval_steps_per_second': 2.801, 'epoch': 0.95}
[2024-03-25 17:19:04,145] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step100 is about to be saved!
[2024-03-25 17:19:04,171] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-100/global_step100/mp_rank_00_model_states.pt
[2024-03-25 17:19:04,171] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-100/global_step100/mp_rank_00_model_states.pt...
[2024-03-25 17:19:04,470] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-100/global_step100/mp_rank_00_model_states.pt.
[2024-03-25 17:19:04,472] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 17:19:04,874] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 17:19:04,875] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 17:19:04,875] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
 10%|██████████████▊                                                                                                                                             | 100/1050 [17:30<2:15:16,  8.54s/it][INFO|trainer.py:2936] 2024-03-25 17:19:03,468 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-100
[INFO|configuration_utils.py:729] 2024-03-25 17:19:03,853 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:19:03,854 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 17:19:03,925 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 17:19:03,925 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-100/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 17:19:05,255 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:19:05,256 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 10%|████████████████▏                                                                                                                                           | 109/1050 [18:53<2:26:47,  9.36s/it]










 11%|█████████████████▋                                                                                                                                          | 119/1050 [20:18<2:14:44,  8.68s/it]
 11%|█████████████████▊                                                                                                                                          | 120/1050 [20:26<2:12:15,  8.53s/it][INFO|trainer.py:3242] 2024-03-25 17:21:56,206 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 17:21:56,206 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 17:21:56,206 >>   Batch size = 1
















 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 91/94 [00:32<00:01,  2.66it/s]

{'eval_loss': 2.2560458183288574, 'eval_accuracy': 0.8351234863515086, 'eval_runtime': 34.2327, 'eval_samples_per_second': 2.746, 'eval_steps_per_second': 2.746, 'epoch': 1.14}
[2024-03-25 17:22:34,055] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step120 is about to be saved!
[2024-03-25 17:22:34,085] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-120/global_step120/mp_rank_00_model_states.pt
[2024-03-25 17:22:34,085] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-120/global_step120/mp_rank_00_model_states.pt...
[2024-03-25 17:22:34,191] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-120/global_step120/mp_rank_00_model_states.pt.
[2024-03-25 17:22:34,193] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-120/global_step120/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 17:22:34,578] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-120/global_step120/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 17:22:34,579] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-120/global_step120/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 17:22:34,579] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step120 is ready now!
 11%|█████████████████▊                                                                                                                                          | 120/1050 [21:00<2:12:15,  8.53s/it][INFO|trainer.py:2936] 2024-03-25 17:22:33,446 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-120
[INFO|configuration_utils.py:729] 2024-03-25 17:22:33,819 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:22:33,820 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 17:22:33,892 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-120/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 17:22:33,893 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-120/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 17:22:34,954 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:22:34,955 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 12%|███████████████████▏                                                                                                                                        | 129/1050 [22:24<2:25:00,  9.45s/it]










 13%|████████████████████▋                                                                                                                                       | 139/1050 [23:51<2:13:11,  8.77s/it]
 13%|████████████████████▊                                                                                                                                       | 140/1050 [23:59<2:10:42,  8.62s/it][INFO|trainer.py:3242] 2024-03-25 17:25:28,793 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 17:25:28,794 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 17:25:28,794 >>   Batch size = 1
















 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍        | 89/94 [00:32<00:02,  2.18it/s]

{'eval_loss': 2.2429139614105225, 'eval_accuracy': 0.8366285831565985, 'eval_runtime': 33.8592, 'eval_samples_per_second': 2.776, 'eval_steps_per_second': 2.776, 'epoch': 1.33}
[2024-03-25 17:26:06,580] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step140 is about to be saved!
[2024-03-25 17:26:06,607] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-140/global_step140/mp_rank_00_model_states.pt
[2024-03-25 17:26:06,608] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-140/global_step140/mp_rank_00_model_states.pt...
[2024-03-25 17:26:06,696] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-140/global_step140/mp_rank_00_model_states.pt.
[2024-03-25 17:26:06,698] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-140/global_step140/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 17:26:07,076] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-140/global_step140/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 17:26:07,076] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-140/global_step140/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 17:26:07,076] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step140 is ready now!
 13%|████████████████████▊                                                                                                                                       | 140/1050 [24:33<2:10:42,  8.62s/it][INFO|trainer.py:2936] 2024-03-25 17:26:06,012 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-140
[INFO|configuration_utils.py:729] 2024-03-25 17:26:06,386 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:26:06,387 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 17:26:06,464 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-140/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 17:26:06,465 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-140/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 17:26:07,441 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:26:07,442 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









 14%|██████████████████████▎                                                                                                                                     | 150/1050 [26:03<2:15:29,  9.03s/it]









 15%|███████████████████████▌                                                                                                                                    | 159/1050 [27:20<2:06:46,  8.54s/it]
 15%|███████████████████████▊                                                                                                                                    | 160/1050 [27:28<2:05:12,  8.44s/it][INFO|trainer.py:3242] 2024-03-25 17:28:58,470 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 17:28:58,471 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 17:28:58,471 >>   Batch size = 1















 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 88/94 [00:30<00:02,  2.84it/s]

{'eval_loss': 2.1844077110290527, 'eval_accuracy': 0.8375521652869946, 'eval_runtime': 32.6559, 'eval_samples_per_second': 2.879, 'eval_steps_per_second': 2.879, 'epoch': 1.52}
[2024-03-25 17:29:34,298] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step160 is about to be saved!
[2024-03-25 17:29:34,324] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-160/global_step160/mp_rank_00_model_states.pt
[2024-03-25 17:29:34,324] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-160/global_step160/mp_rank_00_model_states.pt...
[2024-03-25 17:29:34,438] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-160/global_step160/mp_rank_00_model_states.pt.
[2024-03-25 17:29:34,440] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-160/global_step160/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 17:29:34,736] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-160/global_step160/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 17:29:34,736] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-160/global_step160/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 17:29:34,737] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step160 is ready now!
 15%|███████████████████████▊                                                                                                                                    | 160/1050 [28:01<2:05:12,  8.44s/it][INFO|trainer.py:2936] 2024-03-25 17:29:33,771 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-160
[INFO|configuration_utils.py:729] 2024-03-25 17:29:34,141 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:29:34,142 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 17:29:34,214 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-160/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 17:29:34,214 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-160/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 17:29:35,115 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:29:35,116 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 16%|█████████████████████████                                                                                                                                   | 169/1050 [29:13<1:51:08,  7.57s/it]










 17%|██████████████████████████▌                                                                                                                                 | 179/1050 [30:56<2:45:05, 11.37s/it]
 17%|██████████████████████████▋                                                                                                                                 | 180/1050 [31:02<2:23:57,  9.93s/it][INFO|trainer.py:3242] 2024-03-25 17:32:32,229 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 17:32:32,229 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 17:32:32,229 >>   Batch size = 1












 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 87/94 [00:23<00:01,  3.79it/s]
{'eval_loss': 2.1684341430664062, 'eval_accuracy': 0.8400834644591914, 'eval_runtime': 25.5465, 'eval_samples_per_second': 3.68, 'eval_steps_per_second': 3.68, 'epoch': 1.71}
[2024-03-25 17:33:00,448] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step180 is about to be saved!
[2024-03-25 17:33:00,476] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-180/global_step180/mp_rank_00_model_states.pt
[2024-03-25 17:33:00,476] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-180/global_step180/mp_rank_00_model_states.pt...
[2024-03-25 17:33:00,568] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-180/global_step180/mp_rank_00_model_states.pt.
[2024-03-25 17:33:00,570] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-180/global_step180/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 17:33:00,936] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-180/global_step180/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 17:33:00,937] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-180/global_step180/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 17:33:01,178] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step180 is ready now!
 17%|██████████████████████████▋                                                                                                                                 | 180/1050 [31:28<2:23:57,  9.93s/it][INFO|trainer.py:2936] 2024-03-25 17:32:59,938 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-180
[INFO|configuration_utils.py:729] 2024-03-25 17:33:00,302 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:33:00,304 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 17:33:00,380 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-180/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 17:33:00,380 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-180/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 17:33:01,547 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:33:01,548 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 18%|████████████████████████████                                                                                                                                | 189/1050 [33:08<2:47:02, 11.64s/it]










 19%|█████████████████████████████▌                                                                                                                              | 199/1050 [34:31<1:59:49,  8.45s/it]
 19%|█████████████████████████████▋                                                                                                                              | 200/1050 [34:38<1:51:26,  7.87s/it][INFO|trainer.py:3242] 2024-03-25 17:36:07,549 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 17:36:07,549 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 17:36:07,549 >>   Batch size = 1





















 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 91/94 [00:42<00:00,  3.80it/s]

 19%|█████████████████████████████▋                                                                                                                              | 200/1050 [35:21<1:51:26,  7.87s/it][INFO|trainer.py:2936] 2024-03-25 17:36:53,783 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-200
[INFO|configuration_utils.py:729] 2024-03-25 17:36:54,149 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:36:54,150 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[2024-03-25 17:36:54,287] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[2024-03-25 17:36:54,314] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-200/global_step200/mp_rank_00_model_states.pt
[2024-03-25 17:36:54,314] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-200/global_step200/mp_rank_00_model_states.pt...
[2024-03-25 17:36:54,409] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-200/global_step200/mp_rank_00_model_states.pt.
[2024-03-25 17:36:54,412] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 17:36:54,818] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 17:36:54,819] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 17:36:54,819] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|tokenization_utils_base.py:2433] 2024-03-25 17:36:54,222 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 17:36:54,223 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-200/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 17:36:55,237 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:36:55,238 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 20%|███████████████████████████████                                                                                                                             | 209/1050 [36:42<2:07:32,  9.10s/it]










 21%|████████████████████████████████▌                                                                                                                           | 219/1050 [38:06<1:40:52,  7.28s/it]
 21%|████████████████████████████████▋                                                                                                                           | 220/1050 [38:31<2:54:17, 12.60s/it][INFO|trainer.py:3242] 2024-03-25 17:40:00,885 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 17:40:00,885 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 17:40:00,885 >>   Batch size = 1












 91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎             | 86/94 [00:23<00:02,  3.84it/s]
{'eval_loss': 2.1617801189422607, 'eval_accuracy': 0.8408360128617364, 'eval_runtime': 25.4532, 'eval_samples_per_second': 3.693, 'eval_steps_per_second': 3.693, 'epoch': 2.09}
[2024-03-25 17:40:29,252] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step220 is about to be saved!
[2024-03-25 17:40:29,279] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-220/global_step220/mp_rank_00_model_states.pt
[2024-03-25 17:40:29,280] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-220/global_step220/mp_rank_00_model_states.pt...
[2024-03-25 17:40:29,381] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-220/global_step220/mp_rank_00_model_states.pt.
[2024-03-25 17:40:29,382] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-220/global_step220/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
 21%|████████████████████████████████▋                                                                                                                           | 220/1050 [38:56<2:54:17, 12.60s/it][INFO|trainer.py:2936] 2024-03-25 17:40:28,483 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-220
[INFO|configuration_utils.py:729] 2024-03-25 17:40:28,901 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:40:28,902 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 17:40:29,183 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-220/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 17:40:29,184 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-220/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 17:40:30,050 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:40:30,051 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-03-25 17:40:29,681] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-220/global_step220/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 17:40:29,682] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step220 is ready now!
+++++++++++++++++save call back++++++++++++++++









 22%|██████████████████████████████████▏                                                                                                                         | 230/1050 [40:39<2:42:42, 11.91s/it]









 23%|███████████████████████████████████▌                                                                                                                        | 239/1050 [41:59<2:19:53, 10.35s/it]
 23%|███████████████████████████████████▋                                                                                                                        | 240/1050 [42:05<2:03:50,  9.17s/it][INFO|trainer.py:3242] 2024-03-25 17:43:35,414 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 17:43:35,414 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 17:43:35,414 >>   Batch size = 1












 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 92/94 [00:24<00:00,  3.69it/s]

 23%|███████████████████████████████████▋                                                                                                                        | 240/1050 [42:31<2:03:50,  9.17s/it][INFO|trainer.py:2936] 2024-03-25 17:44:05,778 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-240
[INFO|configuration_utils.py:729] 2024-03-25 17:44:06,355 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:44:06,356 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 17:44:06,431 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-240/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 17:44:06,432 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-240/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-25 17:44:06,500] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step240 is about to be saved!
[2024-03-25 17:44:06,533] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-240/global_step240/mp_rank_00_model_states.pt
[2024-03-25 17:44:06,533] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-240/global_step240/mp_rank_00_model_states.pt...
[2024-03-25 17:44:06,624] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-240/global_step240/mp_rank_00_model_states.pt.
[2024-03-25 17:44:06,626] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-240/global_step240/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 17:44:07,005] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-240/global_step240/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 17:44:07,006] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-240/global_step240/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 17:44:07,006] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step240 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-25 17:44:07,427 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:44:07,428 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 24%|████████████████████████████████████▉                                                                                                                       | 249/1050 [44:07<2:30:36, 11.28s/it]










 25%|██████████████████████████████████████▍                                                                                                                     | 259/1050 [45:31<1:45:18,  7.99s/it]
 25%|██████████████████████████████████████▋                                                                                                                     | 260/1050 [45:37<1:39:30,  7.56s/it][INFO|trainer.py:3242] 2024-03-25 17:47:07,300 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 17:47:07,300 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 17:47:07,300 >>   Batch size = 1





















 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 88/94 [00:41<00:01,  3.84it/s]
 25%|██████████████████████████████████████▋                                                                                                                     | 260/1050 [46:21<1:39:30,  7.56s/it][INFO|trainer.py:2936] 2024-03-25 17:47:52,660 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-260
[INFO|configuration_utils.py:729] 2024-03-25 17:47:53,026 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:47:53,027 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[2024-03-25 17:47:53,172] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step260 is about to be saved!
[2024-03-25 17:47:53,199] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-260/global_step260/mp_rank_00_model_states.pt
[2024-03-25 17:47:53,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-260/global_step260/mp_rank_00_model_states.pt...
[2024-03-25 17:47:53,300] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-260/global_step260/mp_rank_00_model_states.pt.
[2024-03-25 17:47:53,302] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-260/global_step260/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 17:47:53,682] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-260/global_step260/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 17:47:53,683] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-260/global_step260/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 17:47:53,683] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step260 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|tokenization_utils_base.py:2433] 2024-03-25 17:47:53,105 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-260/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 17:47:53,106 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-260/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 17:47:54,071 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:47:54,072 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 26%|███████████████████████████████████████▉                                                                                                                    | 269/1050 [47:41<1:55:46,  8.89s/it]










 27%|█████████████████████████████████████████▍                                                                                                                  | 279/1050 [49:05<1:32:54,  7.23s/it]
 27%|█████████████████████████████████████████▌                                                                                                                  | 280/1050 [49:30<2:40:40, 12.52s/it][INFO|trainer.py:3242] 2024-03-25 17:50:59,489 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 17:50:59,489 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 17:50:59,489 >>   Batch size = 1












 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 87/94 [00:23<00:01,  3.80it/s]
 27%|█████████████████████████████████████████▌                                                                                                                  | 280/1050 [49:55<2:40:40, 12.52s/it][INFO|trainer.py:2936] 2024-03-25 17:51:28,115 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-280
[INFO|configuration_utils.py:729] 2024-03-25 17:51:28,476 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:51:28,477 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 17:51:28,545 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-280/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 17:51:28,546 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-280/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-25 17:51:28,609] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step280 is about to be saved!
[2024-03-25 17:51:28,636] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-280/global_step280/mp_rank_00_model_states.pt
[2024-03-25 17:51:28,636] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-280/global_step280/mp_rank_00_model_states.pt...
[2024-03-25 17:51:28,729] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-280/global_step280/mp_rank_00_model_states.pt.
[2024-03-25 17:51:28,731] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-280/global_step280/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 17:51:29,029] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-280/global_step280/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 17:51:29,030] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-280/global_step280/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 17:51:29,030] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step280 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-25 17:51:29,407 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:51:29,408 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 28%|██████████████████████████████████████████▉                                                                                                                 | 289/1050 [51:17<1:40:19,  7.91s/it]










 29%|████████████████████████████████████████████▌                                                                                                               | 300/1050 [53:05<1:55:51,  9.27s/it][INFO|trainer.py:3242] 2024-03-25 17:54:34,712 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 17:54:34,712 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 17:54:34,712 >>   Batch size = 1
  2%|███▍                                                                                                                                                              | 2/94 [00:00<00:12,  7.33it/s]












 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 93/94 [00:24<00:00,  3.81it/s]

{'eval_loss': 2.1056833267211914, 'eval_accuracy': 0.843811999726346, 'eval_runtime': 25.1086, 'eval_samples_per_second': 3.744, 'eval_steps_per_second': 3.744, 'epoch': 2.84}
[2024-03-25 17:55:02,409] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step300 is about to be saved!
[2024-03-25 17:55:02,435] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-300/global_step300/mp_rank_00_model_states.pt
[2024-03-25 17:55:02,435] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-300/global_step300/mp_rank_00_model_states.pt...
[2024-03-25 17:55:02,528] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-300/global_step300/mp_rank_00_model_states.pt.
 29%|████████████████████████████████████████████▌                                                                                                               | 300/1050 [53:30<1:55:51,  9.27s/it][INFO|trainer.py:2936] 2024-03-25 17:55:01,914 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-300
[INFO|configuration_utils.py:729] 2024-03-25 17:55:02,271 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:55:02,271 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 17:55:02,346 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 17:55:02,346 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-300/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 17:55:03,248 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:55:03,249 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-03-25 17:55:02,835] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 17:55:02,836] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 17:55:02,836] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step300 is ready now!
+++++++++++++++++save call back++++++++++++++++








 29%|█████████████████████████████████████████████▉                                                                                                              | 309/1050 [55:08<2:19:16, 11.28s/it]










 30%|███████████████████████████████████████████████▌                                                                                                            | 320/1050 [56:38<1:33:07,  7.65s/it][INFO|trainer.py:3242] 2024-03-25 17:58:07,588 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 17:58:07,588 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 17:58:07,588 >>   Batch size = 1
{'loss': 1.6614, 'learning_rate': 9.627564263195184e-05, 'epoch': 3.03}





















 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 88/94 [00:41<00:01,  3.73it/s]
{'eval_loss': 2.1384034156799316, 'eval_accuracy': 0.8444619278921803, 'eval_runtime': 43.7068, 'eval_samples_per_second': 2.151, 'eval_steps_per_second': 2.151, 'epoch': 3.03}
[2024-03-25 17:58:54,817] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step320 is about to be saved!
[2024-03-25 17:58:54,843] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-320/global_step320/mp_rank_00_model_states.pt
 30%|███████████████████████████████████████████████▌                                                                                                            | 320/1050 [57:21<1:33:07,  7.65s/it][INFO|trainer.py:2936] 2024-03-25 17:58:54,306 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-320
[INFO|configuration_utils.py:729] 2024-03-25 17:58:54,678 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:58:54,679 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 17:58:54,753 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-320/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 17:58:54,754 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-320/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 17:58:55,701 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 17:58:55,702 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-03-25 17:58:54,936] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-320/global_step320/mp_rank_00_model_states.pt.
[2024-03-25 17:58:54,939] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-320/global_step320/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 17:58:55,322] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-320/global_step320/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 17:58:55,323] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-320/global_step320/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 17:58:55,323] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step320 is ready now!
+++++++++++++++++save call back++++++++++++++++









 31%|█████████████████████████████████████████████████                                                                                                           | 330/1050 [58:49<1:37:22,  8.11s/it]









 32%|█████████████████████████████████████████████████▋                                                                                                        | 339/1050 [1:00:06<1:23:57,  7.09s/it]
 32%|█████████████████████████████████████████████████▊                                                                                                        | 340/1050 [1:00:31<2:27:36, 12.47s/it][INFO|trainer.py:3242] 2024-03-25 18:02:01,026 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 18:02:01,026 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 18:02:01,026 >>   Batch size = 1












 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 93/94 [00:24<00:00,  3.80it/s]

{'eval_loss': 2.2021842002868652, 'eval_accuracy': 0.8447013751111719, 'eval_runtime': 25.306, 'eval_samples_per_second': 3.715, 'eval_steps_per_second': 3.715, 'epoch': 3.22}
[2024-03-25 18:02:28,960] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step340 is about to be saved!
[2024-03-25 18:02:28,987] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-340/global_step340/mp_rank_00_model_states.pt
[2024-03-25 18:02:28,987] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-340/global_step340/mp_rank_00_model_states.pt...
[2024-03-25 18:02:29,088] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-340/global_step340/mp_rank_00_model_states.pt.
 32%|█████████████████████████████████████████████████▊                                                                                                        | 340/1050 [1:00:56<2:27:36, 12.47s/it][INFO|trainer.py:2936] 2024-03-25 18:02:28,462 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-340
[INFO|configuration_utils.py:729] 2024-03-25 18:02:28,824 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:02:28,825 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 18:02:28,896 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-340/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 18:02:28,896 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-340/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-25 18:02:29,486] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-340/global_step340/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 18:02:29,487] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-340/global_step340/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 18:02:29,487] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step340 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-25 18:02:30,191 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:02:30,192 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 33%|███████████████████████████████████████████████████▏                                                                                                      | 349/1050 [1:02:17<1:32:09,  7.89s/it]










 34%|████████████████████████████████████████████████████▋                                                                                                     | 359/1050 [1:04:00<1:50:37,  9.61s/it]
 34%|████████████████████████████████████████████████████▊                                                                                                     | 360/1050 [1:04:07<1:39:47,  8.68s/it][INFO|trainer.py:3242] 2024-03-25 18:05:36,664 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 18:05:36,664 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 18:05:36,664 >>   Batch size = 1





















 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 91/94 [00:40<00:05,  1.90s/it]
{'eval_loss': 2.1832993030548096, 'eval_accuracy': 0.8440172401997674, 'eval_runtime': 43.6541, 'eval_samples_per_second': 2.153, 'eval_steps_per_second': 2.153, 'epoch': 3.41}
[2024-03-25 18:06:22,952] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step360 is about to be saved!
[2024-03-25 18:06:22,978] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-360/global_step360/mp_rank_00_model_states.pt
[2024-03-25 18:06:22,979] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-360/global_step360/mp_rank_00_model_states.pt...
[2024-03-25 18:06:23,070] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-360/global_step360/mp_rank_00_model_states.pt.
 34%|████████████████████████████████████████████████████▊                                                                                                     | 360/1050 [1:04:50<1:39:47,  8.68s/it][INFO|trainer.py:2936] 2024-03-25 18:06:22,459 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-360
[INFO|configuration_utils.py:729] 2024-03-25 18:06:22,819 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:06:22,820 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 18:06:22,890 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-360/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 18:06:22,891 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-360/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 18:06:23,826 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:06:23,827 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-03-25 18:06:23,449] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-360/global_step360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 18:06:23,450] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-360/global_step360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 18:06:23,450] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step360 is ready now!
+++++++++++++++++save call back++++++++++++++++








 35%|██████████████████████████████████████████████████████                                                                                                    | 369/1050 [1:06:11<1:54:08, 10.06s/it]










 36%|███████████████████████████████████████████████████████▌                                                                                                  | 379/1050 [1:07:34<1:24:10,  7.53s/it]
 36%|███████████████████████████████████████████████████████▋                                                                                                  | 380/1050 [1:07:41<1:20:27,  7.21s/it][INFO|trainer.py:3242] 2024-03-25 18:09:10,635 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 18:09:10,636 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 18:09:10,636 >>   Batch size = 1





















 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 87/94 [00:41<00:01,  3.67it/s]
 36%|███████████████████████████████████████████████████████▋                                                                                                  | 380/1050 [1:08:25<1:20:27,  7.21s/it][INFO|trainer.py:2936] 2024-03-25 18:09:57,476 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-380
[INFO|configuration_utils.py:729] 2024-03-25 18:09:57,879 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:09:57,880 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 18:09:57,950 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-380/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 18:09:57,951 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-380/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-25 18:09:58,012] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step380 is about to be saved!
[2024-03-25 18:09:58,038] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-380/global_step380/mp_rank_00_model_states.pt
[2024-03-25 18:09:58,038] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-380/global_step380/mp_rank_00_model_states.pt...
[2024-03-25 18:09:58,130] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-380/global_step380/mp_rank_00_model_states.pt.
[2024-03-25 18:09:58,133] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-380/global_step380/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 18:09:58,515] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-380/global_step380/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 18:09:58,516] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-380/global_step380/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 18:09:58,516] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step380 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-25 18:09:58,898 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:09:58,899 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 37%|█████████████████████████████████████████████████████████                                                                                                 | 389/1050 [1:09:46<1:31:33,  8.31s/it]










 38%|██████████████████████████████████████████████████████████▌                                                                                               | 399/1050 [1:11:28<1:56:43, 10.76s/it]
 38%|██████████████████████████████████████████████████████████▋                                                                                               | 400/1050 [1:11:35<1:42:33,  9.47s/it][INFO|trainer.py:3242] 2024-03-25 18:13:04,881 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 18:13:04,881 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 18:13:04,881 >>   Batch size = 1












 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 87/94 [00:23<00:01,  3.65it/s]
 38%|██████████████████████████████████████████████████████████▋                                                                                               | 400/1050 [1:12:01<1:42:33,  9.47s/it][INFO|trainer.py:2936] 2024-03-25 18:13:32,992 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-400
[INFO|configuration_utils.py:729] 2024-03-25 18:13:33,556 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:13:33,557 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 18:13:33,633 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 18:13:33,633 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-400/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 18:13:34,613 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:13:34,615 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-03-25 18:13:33,700] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!
[2024-03-25 18:13:33,729] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-400/global_step400/mp_rank_00_model_states.pt
[2024-03-25 18:13:33,729] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-400/global_step400/mp_rank_00_model_states.pt...
[2024-03-25 18:13:33,819] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-400/global_step400/mp_rank_00_model_states.pt.
[2024-03-25 18:13:33,821] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 18:13:34,208] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 18:13:34,209] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 18:13:34,209] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
+++++++++++++++++save call back++++++++++++++++









 39%|████████████████████████████████████████████████████████████▏                                                                                             | 410/1050 [1:13:46<1:45:38,  9.90s/it]









 40%|█████████████████████████████████████████████████████████████▌                                                                                            | 420/1050 [1:15:10<1:20:16,  7.64s/it][INFO|trainer.py:3242] 2024-03-25 18:16:40,292 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 18:16:40,292 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 18:16:40,292 >>   Batch size = 1
  2%|███▍                                                                                                                                                              | 2/94 [00:00<00:12,  7.35it/s]






















 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 91/94 [00:42<00:00,  3.67it/s]

 40%|█████████████████████████████████████████████████████████████▌                                                                                            | 420/1050 [1:15:54<1:20:16,  7.64s/it][INFO|trainer.py:2936] 2024-03-25 18:17:27,063 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-420
[2024-03-25 18:17:27,559] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step420 is about to be saved!
[2024-03-25 18:17:27,586] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-420/global_step420/mp_rank_00_model_states.pt
[2024-03-25 18:17:27,586] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-420/global_step420/mp_rank_00_model_states.pt...
[2024-03-25 18:17:27,680] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-420/global_step420/mp_rank_00_model_states.pt.
[2024-03-25 18:17:27,682] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-420/global_step420/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[INFO|configuration_utils.py:729] 2024-03-25 18:17:27,428 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:17:27,429 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 18:17:27,495 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-420/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 18:17:27,496 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-420/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 18:17:28,441 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:17:28,442 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-03-25 18:17:28,070] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-420/global_step420/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 18:17:28,071] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-420/global_step420/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 18:17:28,071] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step420 is ready now!
+++++++++++++++++save call back++++++++++++++++








 41%|██████████████████████████████████████████████████████████████▉                                                                                           | 429/1050 [1:17:15<1:29:42,  8.67s/it]










 42%|████████████████████████████████████████████████████████████████▌                                                                                         | 440/1050 [1:19:04<1:56:25, 11.45s/it][INFO|trainer.py:3242] 2024-03-25 18:20:34,139 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 18:20:34,139 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 18:20:34,139 >>   Batch size = 1
  4%|██████▉                                                                                                                                                           | 4/94 [00:00<00:20,  4.37it/s]












 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 87/94 [00:22<00:01,  3.86it/s]

 42%|████████████████████████████████████████████████████████████████▌                                                                                         | 440/1050 [1:19:29<1:56:25, 11.45s/it][INFO|trainer.py:2936] 2024-03-25 18:21:01,384 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-440
[2024-03-25 18:21:01,915] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step440 is about to be saved!
[2024-03-25 18:21:01,943] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-440/global_step440/mp_rank_00_model_states.pt
[2024-03-25 18:21:01,943] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-440/global_step440/mp_rank_00_model_states.pt...
[2024-03-25 18:21:02,029] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-440/global_step440/mp_rank_00_model_states.pt.
[2024-03-25 18:21:02,031] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-440/global_step440/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[INFO|configuration_utils.py:729] 2024-03-25 18:21:01,779 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:21:01,780 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 18:21:01,848 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-440/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 18:21:01,849 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-440/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 18:21:02,700 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:21:02,701 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-03-25 18:21:02,307] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-440/global_step440/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 18:21:02,308] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-440/global_step440/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 18:21:02,308] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step440 is ready now!
+++++++++++++++++save call back++++++++++++++++









 43%|██████████████████████████████████████████████████████████████████                                                                                        | 450/1050 [1:21:15<2:06:59, 12.70s/it]









 44%|███████████████████████████████████████████████████████████████████▍                                                                                      | 460/1050 [1:22:39<1:24:29,  8.59s/it][INFO|trainer.py:3242] 2024-03-25 18:24:08,538 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 18:24:08,538 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 18:24:08,538 >>   Batch size = 1
  4%|██████▉                                                                                                                                                           | 4/94 [00:00<00:19,  4.56it/s]

















 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 93/94 [00:32<00:01,  1.61s/it]
 44%|███████████████████████████████████████████████████████████████████▍                                                                                      | 460/1050 [1:23:13<1:24:29,  8.59s/it][INFO|trainer.py:2936] 2024-03-25 18:24:48,047 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-460
[INFO|configuration_utils.py:729] 2024-03-25 18:24:48,429 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:24:48,430 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 18:24:48,504 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-460/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 18:24:48,505 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-460/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 18:24:49,432 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:24:49,433 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-03-25 18:24:48,568] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step460 is about to be saved!
[2024-03-25 18:24:48,600] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-460/global_step460/mp_rank_00_model_states.pt
[2024-03-25 18:24:48,600] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-460/global_step460/mp_rank_00_model_states.pt...
[2024-03-25 18:24:48,694] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-460/global_step460/mp_rank_00_model_states.pt.
[2024-03-25 18:24:48,696] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-460/global_step460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 18:24:49,064] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-460/global_step460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 18:24:49,064] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-460/global_step460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 18:24:49,065] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step460 is ready now!
+++++++++++++++++save call back++++++++++++++++









 45%|████████████████████████████████████████████████████████████████████▉                                                                                     | 470/1050 [1:24:46<1:30:59,  9.41s/it]









 46%|██████████████████████████████████████████████████████████████████████▎                                                                                   | 479/1050 [1:26:02<1:15:10,  7.90s/it]
 46%|██████████████████████████████████████████████████████████████████████▍                                                                                   | 480/1050 [1:26:09<1:10:59,  7.47s/it][INFO|trainer.py:3242] 2024-03-25 18:27:38,690 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 18:27:38,690 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 18:27:38,690 >>   Batch size = 1





















 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 87/94 [00:41<00:01,  3.80it/s]

 46%|██████████████████████████████████████████████████████████████████████▍                                                                                   | 480/1050 [1:26:52<1:10:59,  7.47s/it][INFO|trainer.py:2936] 2024-03-25 18:28:24,003 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-480
[2024-03-25 18:28:24,491] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step480 is about to be saved!
[2024-03-25 18:28:24,517] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-480/global_step480/mp_rank_00_model_states.pt
[2024-03-25 18:28:24,517] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-480/global_step480/mp_rank_00_model_states.pt...
[2024-03-25 18:28:24,607] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-480/global_step480/mp_rank_00_model_states.pt.
[2024-03-25 18:28:24,609] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-480/global_step480/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[INFO|configuration_utils.py:729] 2024-03-25 18:28:24,379 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:28:24,379 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 18:28:24,440 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-480/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 18:28:24,441 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-480/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 18:28:25,333 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:28:25,334 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-03-25 18:28:24,959] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-480/global_step480/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 18:28:24,960] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-480/global_step480/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 18:28:24,960] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step480 is ready now!
+++++++++++++++++save call back++++++++++++++++








 47%|███████████████████████████████████████████████████████████████████████▋                                                                                  | 489/1050 [1:28:13<1:21:24,  8.71s/it]










 48%|█████████████████████████████████████████████████████████████████████████▏                                                                                | 499/1050 [1:29:47<1:36:00, 10.45s/it]
 48%|█████████████████████████████████████████████████████████████████████████▎                                                                                | 500/1050 [1:30:01<1:44:00, 11.35s/it][INFO|trainer.py:3242] 2024-03-25 18:31:30,763 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 18:31:30,763 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 18:31:30,763 >>   Batch size = 1












 91%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎             | 86/94 [00:23<00:02,  3.46it/s]
 48%|█████████████████████████████████████████████████████████████████████████▎                                                                                | 500/1050 [1:30:26<1:44:00, 11.35s/it][INFO|trainer.py:2936] 2024-03-25 18:31:59,276 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-500
[INFO|configuration_utils.py:729] 2024-03-25 18:31:59,646 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:31:59,647 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 18:31:59,720 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 18:31:59,721 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-500/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-25 18:31:59,769] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step500 is about to be saved!
[2024-03-25 18:31:59,795] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-500/global_step500/mp_rank_00_model_states.pt
[2024-03-25 18:31:59,795] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-500/global_step500/mp_rank_00_model_states.pt...
[2024-03-25 18:31:59,890] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-500/global_step500/mp_rank_00_model_states.pt.
[2024-03-25 18:31:59,892] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-500/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 18:32:00,271] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-500/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 18:32:00,272] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-500/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 18:32:00,272] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-25 18:32:00,685 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:32:00,686 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









 49%|██████████████████████████████████████████████████████████████████████████▊                                                                               | 510/1050 [1:32:13<1:41:07, 11.24s/it]









 49%|████████████████████████████████████████████████████████████████████████████                                                                              | 519/1050 [1:33:30<1:17:54,  8.80s/it]
 50%|████████████████████████████████████████████████████████████████████████████▎                                                                             | 520/1050 [1:33:37<1:11:49,  8.13s/it][INFO|trainer.py:3242] 2024-03-25 18:35:06,563 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 18:35:06,564 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 18:35:06,564 >>   Batch size = 1





















 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 88/94 [00:41<00:02,  2.45it/s]
 50%|████████████████████████████████████████████████████████████████████████████▎                                                                             | 520/1050 [1:34:20<1:11:49,  8.13s/it][INFO|trainer.py:2936] 2024-03-25 18:35:52,109 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-520
[INFO|configuration_utils.py:729] 2024-03-25 18:35:52,478 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:35:52,479 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 18:35:52,546 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-520/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 18:35:52,547 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-520/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-25 18:35:52,600] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step520 is about to be saved!
[2024-03-25 18:35:52,627] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-520/global_step520/mp_rank_00_model_states.pt
[2024-03-25 18:35:52,627] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-520/global_step520/mp_rank_00_model_states.pt...
[2024-03-25 18:35:52,730] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-520/global_step520/mp_rank_00_model_states.pt.
[2024-03-25 18:35:52,732] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-520/global_step520/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 18:35:53,114] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-520/global_step520/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 18:35:53,115] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-520/global_step520/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 18:35:53,115] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step520 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-25 18:35:53,490 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:35:53,491 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 50%|█████████████████████████████████████████████████████████████████████████████▌                                                                            | 529/1050 [1:35:40<1:25:15,  9.82s/it]










 51%|███████████████████████████████████████████████████████████████████████████████                                                                           | 539/1050 [1:37:05<1:04:33,  7.58s/it]
 51%|███████████████████████████████████████████████████████████████████████████████▏                                                                          | 540/1050 [1:37:11<1:01:45,  7.26s/it][INFO|trainer.py:3242] 2024-03-25 18:38:41,291 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 18:38:41,291 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 18:38:41,291 >>   Batch size = 1





















 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 87/94 [00:41<00:01,  3.81it/s]

 51%|███████████████████████████████████████████████████████████████████████████████▏                                                                          | 540/1050 [1:37:55<1:01:45,  7.26s/it][INFO|trainer.py:2936] 2024-03-25 18:39:26,747 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-540
[2024-03-25 18:39:27,237] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step540 is about to be saved!
[2024-03-25 18:39:27,264] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-540/global_step540/mp_rank_00_model_states.pt
[2024-03-25 18:39:27,264] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-540/global_step540/mp_rank_00_model_states.pt...
[2024-03-25 18:39:27,356] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-540/global_step540/mp_rank_00_model_states.pt.
[2024-03-25 18:39:27,358] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-540/global_step540/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[INFO|configuration_utils.py:729] 2024-03-25 18:39:27,115 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:39:27,116 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 18:39:27,188 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-540/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 18:39:27,188 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-540/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 18:39:28,093 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:39:28,094 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-03-25 18:39:27,717] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-540/global_step540/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 18:39:27,718] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-540/global_step540/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 18:39:27,719] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step540 is ready now!
+++++++++++++++++save call back++++++++++++++++









 52%|████████████████████████████████████████████████████████████████████████████████▋                                                                         | 550/1050 [1:39:23<1:05:04,  7.81s/it]









 53%|██████████████████████████████████████████████████████████████████████████████████▏                                                                       | 560/1050 [1:41:04<1:27:13, 10.68s/it][INFO|trainer.py:3242] 2024-03-25 18:42:34,327 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 18:42:34,327 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 18:42:34,327 >>   Batch size = 1
  2%|███▍                                                                                                                                                              | 2/94 [00:00<00:12,  7.43it/s]













 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 91/94 [00:24<00:00,  3.67it/s]

 53%|██████████████████████████████████████████████████████████████████████████████████▏                                                                       | 560/1050 [1:41:30<1:27:13, 10.68s/it][INFO|trainer.py:2936] 2024-03-25 18:43:02,854 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-560
[2024-03-25 18:43:03,342] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step560 is about to be saved!
[2024-03-25 18:43:03,368] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-560/global_step560/mp_rank_00_model_states.pt
[2024-03-25 18:43:03,369] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-560/global_step560/mp_rank_00_model_states.pt...
[2024-03-25 18:43:03,471] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-560/global_step560/mp_rank_00_model_states.pt.
[2024-03-25 18:43:03,474] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-560/global_step560/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[INFO|configuration_utils.py:729] 2024-03-25 18:43:03,225 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:43:03,225 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 18:43:03,293 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-560/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 18:43:03,293 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-560/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 18:43:04,121 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:43:04,122 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-03-25 18:43:03,743] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-560/global_step560/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 18:43:03,744] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-560/global_step560/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 18:43:03,744] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step560 is ready now!
+++++++++++++++++save call back++++++++++++++++








 54%|███████████████████████████████████████████████████████████████████████████████████▍                                                                      | 569/1050 [1:43:02<1:28:11, 11.00s/it]










 55%|█████████████████████████████████████████████████████████████████████████████████████                                                                     | 580/1050 [1:44:39<1:07:02,  8.56s/it][INFO|trainer.py:3242] 2024-03-25 18:46:09,176 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 18:46:09,176 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 18:46:09,176 >>   Batch size = 1
{'loss': 0.7791, 'learning_rate': 7.246153846153847e-05, 'epoch': 5.5}














 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 93/94 [00:27<00:00,  1.04it/s]
 55%|█████████████████████████████████████████████████████████████████████████████████████                                                                     | 580/1050 [1:45:09<1:07:02,  8.56s/it][INFO|trainer.py:2936] 2024-03-25 18:46:42,207 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-580
[INFO|configuration_utils.py:729] 2024-03-25 18:46:42,577 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:46:42,578 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 18:46:42,651 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-580/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 18:46:42,651 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-580/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-25 18:46:42,715] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step580 is about to be saved!
[2024-03-25 18:46:42,750] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-580/global_step580/mp_rank_00_model_states.pt
[2024-03-25 18:46:42,750] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-580/global_step580/mp_rank_00_model_states.pt...
[2024-03-25 18:46:42,841] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-580/global_step580/mp_rank_00_model_states.pt.
[2024-03-25 18:46:42,843] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-580/global_step580/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 18:46:43,138] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-580/global_step580/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 18:46:43,139] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-580/global_step580/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 18:46:43,139] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step580 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-25 18:46:43,725 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:46:43,726 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 56%|██████████████████████████████████████████████████████████████████████████████████████▍                                                                   | 589/1050 [1:46:40<1:26:09, 11.21s/it]










 57%|█████████████████████████████████████████████████████████████████████████████████████████▏                                                                  | 600/1050 [1:48:09<58:55,  7.86s/it][INFO|trainer.py:3242] 2024-03-25 18:49:39,271 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 18:49:39,271 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 18:49:39,271 >>   Batch size = 1
  0%|                                                                                                                                                                          | 0/94 [00:00<?, ?it/s]





















 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍        | 89/94 [00:41<00:01,  3.74it/s]
 57%|█████████████████████████████████████████████████████████████████████████████████████████▏                                                                  | 600/1050 [1:48:53<58:55,  7.86s/it][INFO|trainer.py:2936] 2024-03-25 18:50:25,869 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-600
[INFO|configuration_utils.py:729] 2024-03-25 18:50:26,231 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:50:26,232 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 18:50:26,305 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 18:50:26,306 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-600/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 18:50:27,208 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:50:27,209 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-03-25 18:50:26,357] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step600 is about to be saved!
[2024-03-25 18:50:26,383] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-600/global_step600/mp_rank_00_model_states.pt
[2024-03-25 18:50:26,384] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-600/global_step600/mp_rank_00_model_states.pt...
[2024-03-25 18:50:26,476] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-600/global_step600/mp_rank_00_model_states.pt.
[2024-03-25 18:50:26,478] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 18:50:26,814] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 18:50:26,814] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-600/global_step600/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 18:50:26,814] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step600 is ready now!
+++++++++++++++++save call back++++++++++++++++









 58%|█████████████████████████████████████████████████████████████████████████████████████████▍                                                                | 610/1050 [1:50:21<1:02:09,  8.48s/it]









 59%|██████████████████████████████████████████████████████████████████████████████████████████▉                                                               | 620/1050 [1:51:53<1:10:03,  9.78s/it][INFO|trainer.py:3242] 2024-03-25 18:53:23,409 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 18:53:23,409 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 18:53:23,409 >>   Batch size = 1
{'loss': 0.7018, 'learning_rate': 6.630769230769232e-05, 'epoch': 5.88}
















 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏      | 90/94 [00:30<00:01,  3.81it/s]
 59%|██████████████████████████████████████████████████████████████████████████████████████████▉                                                               | 620/1050 [1:52:27<1:10:03,  9.78s/it][INFO|trainer.py:2936] 2024-03-25 18:53:59,026 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-620
[INFO|configuration_utils.py:729] 2024-03-25 18:53:59,395 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:53:59,396 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 18:53:59,483 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-620/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 18:53:59,484 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-620/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-25 18:53:59,532] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step620 is about to be saved!
[2024-03-25 18:53:59,558] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-620/global_step620/mp_rank_00_model_states.pt
[2024-03-25 18:53:59,558] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-620/global_step620/mp_rank_00_model_states.pt...
[2024-03-25 18:53:59,651] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-620/global_step620/mp_rank_00_model_states.pt.
[2024-03-25 18:53:59,654] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-620/global_step620/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 18:54:00,035] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-620/global_step620/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[INFO|configuration_utils.py:729] 2024-03-25 18:54:00,423 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:54:00,424 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-03-25 18:54:00,036] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-620/global_step620/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 18:54:00,036] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step620 is ready now!
+++++++++++++++++save call back++++++++++++++++









 60%|█████████████████████████████████████████████████████████████████████████████████████████████▌                                                              | 630/1050 [1:53:54<52:47,  7.54s/it]









 61%|█████████████████████████████████████████████████████████████████████████████████████████████▊                                                            | 640/1050 [1:55:34<1:04:04,  9.38s/it][INFO|trainer.py:3242] 2024-03-25 18:57:04,029 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 18:57:04,029 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 18:57:04,029 >>   Batch size = 1
{'loss': 0.5877, 'learning_rate': 6.323076923076924e-05, 'epoch': 6.07}












 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏      | 90/94 [00:23<00:01,  3.77it/s]

 61%|█████████████████████████████████████████████████████████████████████████████████████████████▊                                                            | 640/1050 [1:55:59<1:04:04,  9.38s/it][INFO|trainer.py:2936] 2024-03-25 18:57:32,239 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-640
[INFO|configuration_utils.py:729] 2024-03-25 18:57:32,640 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:57:32,641 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 18:57:32,717 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-640/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 18:57:32,718 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-640/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 18:57:33,675 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 18:57:33,676 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-03-25 18:57:32,770] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step640 is about to be saved!
[2024-03-25 18:57:32,804] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-640/global_step640/mp_rank_00_model_states.pt
[2024-03-25 18:57:32,804] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-640/global_step640/mp_rank_00_model_states.pt...
[2024-03-25 18:57:32,911] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-640/global_step640/mp_rank_00_model_states.pt.
[2024-03-25 18:57:32,913] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-640/global_step640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 18:57:33,283] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-640/global_step640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 18:57:33,284] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-640/global_step640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 18:57:33,284] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step640 is ready now!
+++++++++++++++++save call back++++++++++++++++









 62%|███████████████████████████████████████████████████████████████████████████████████████████████▎                                                          | 650/1050 [1:57:43<1:05:03,  9.76s/it]









 63%|██████████████████████████████████████████████████████████████████████████████████████████████████                                                          | 660/1050 [1:59:05<48:59,  7.54s/it][INFO|trainer.py:3242] 2024-03-25 19:00:35,455 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 19:00:35,456 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 19:00:35,456 >>   Batch size = 1
  3%|█████▏                                                                                                                                                            | 3/94 [00:00<00:17,  5.17it/s]





















 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍        | 89/94 [00:40<00:01,  3.74it/s]
 63%|██████████████████████████████████████████████████████████████████████████████████████████████████                                                          | 660/1050 [1:59:48<48:59,  7.54s/it][INFO|trainer.py:2936] 2024-03-25 19:01:19,833 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-660
[INFO|configuration_utils.py:729] 2024-03-25 19:01:20,205 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:01:20,206 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 19:01:20,282 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-660/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 19:01:20,283 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-660/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-25 19:01:20,335] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step660 is about to be saved!
[2024-03-25 19:01:20,362] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-660/global_step660/mp_rank_00_model_states.pt
[2024-03-25 19:01:20,362] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-660/global_step660/mp_rank_00_model_states.pt...
[2024-03-25 19:01:20,463] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-660/global_step660/mp_rank_00_model_states.pt.
[2024-03-25 19:01:20,465] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-660/global_step660/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[INFO|configuration_utils.py:729] 2024-03-25 19:01:21,241 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:01:21,242 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-03-25 19:01:20,849] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-660/global_step660/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 19:01:20,850] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-660/global_step660/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 19:01:20,850] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step660 is ready now!
+++++++++++++++++save call back++++++++++++++++









 64%|███████████████████████████████████████████████████████████████████████████████████████████████████▌                                                        | 670/1050 [2:01:13<50:49,  8.03s/it]







 65%|████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                       | 679/1050 [2:02:29<45:29,  7.36s/it]
 65%|███████████████████████████████████████████████████████████████████████████████████████████████████▋                                                      | 680/1050 [2:02:52<1:13:10, 11.87s/it][INFO|trainer.py:3242] 2024-03-25 19:04:21,788 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 19:04:21,788 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 19:04:21,788 >>   Batch size = 1











 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 87/94 [00:22<00:01,  3.91it/s]


100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:24<00:00,  3.58it/s]
[2024-03-25 19:04:50,448] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step680 is about to be saved!
[2024-03-25 19:04:50,476] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-680/global_step680/mp_rank_00_model_states.pt
[2024-03-25 19:04:50,476] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-680/global_step680/mp_rank_00_model_states.pt...
[2024-03-25 19:04:50,575] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-680/global_step680/mp_rank_00_model_states.pt.
 65%|███████████████████████████████████████████████████████████████████████████████████████████████████▋                                                      | 680/1050 [2:03:17<1:13:10, 11.87s/it][INFO|trainer.py:2936] 2024-03-25 19:04:49,960 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-680
[INFO|configuration_utils.py:729] 2024-03-25 19:04:50,329 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:04:50,330 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 19:04:50,398 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-680/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 19:04:50,399 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-680/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-25 19:04:50,960] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-680/global_step680/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 19:04:50,961] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-680/global_step680/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 19:04:50,961] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step680 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-25 19:04:51,341 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:04:51,342 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 66%|█████████████████████████████████████████████████████████████████████████████████████████████████████                                                     | 689/1050 [2:04:54<1:16:32, 12.72s/it]










 67%|███████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                    | 699/1050 [2:06:16<49:05,  8.39s/it]
 67%|████████████████████████████████████████████████████████████████████████████████████████████████████████                                                    | 700/1050 [2:06:22<45:33,  7.81s/it][INFO|trainer.py:3242] 2024-03-25 19:07:52,231 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 19:07:52,231 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 19:07:52,231 >>   Batch size = 1




















 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 91/94 [00:40<00:00,  3.76it/s]

{'eval_loss': 2.6276004314422607, 'eval_accuracy': 0.8444277211466101, 'eval_runtime': 41.5136, 'eval_samples_per_second': 2.264, 'eval_steps_per_second': 2.264, 'epoch': 6.64}
[2024-03-25 19:08:36,240] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step700 is about to be saved!
[2024-03-25 19:08:36,267] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-700/global_step700/mp_rank_00_model_states.pt
[2024-03-25 19:08:36,267] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-700/global_step700/mp_rank_00_model_states.pt...
[2024-03-25 19:08:36,374] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-700/global_step700/mp_rank_00_model_states.pt.
[2024-03-25 19:08:36,376] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-700/global_step700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 19:08:36,738] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-700/global_step700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 19:08:36,739] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-700/global_step700/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 19:08:36,739] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step700 is ready now!
 67%|████████████████████████████████████████████████████████████████████████████████████████████████████████                                                    | 700/1050 [2:07:04<45:33,  7.81s/it][INFO|trainer.py:2936] 2024-03-25 19:08:35,748 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-700
[INFO|configuration_utils.py:729] 2024-03-25 19:08:36,119 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:08:36,120 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 19:08:36,189 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 19:08:36,190 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-700/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 19:08:37,127 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:08:37,128 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                  | 709/1050 [2:08:22<51:35,  9.08s/it]










 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                 | 719/1050 [2:09:46<42:46,  7.75s/it]
 69%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                | 720/1050 [2:10:07<1:04:08, 11.66s/it][INFO|trainer.py:3242] 2024-03-25 19:11:36,840 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 19:11:36,840 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 19:11:36,840 >>   Batch size = 1












 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍        | 89/94 [00:24<00:01,  3.70it/s]

 69%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                | 720/1050 [2:10:33<1:04:08, 11.66s/it][INFO|trainer.py:2936] 2024-03-25 19:12:04,680 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-720
[INFO|configuration_utils.py:729] 2024-03-25 19:12:05,257 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:12:05,258 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 19:12:05,332 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-720/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 19:12:05,332 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-720/special_tokens_map.json
[2024-03-25 19:12:05,383] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step720 is about to be saved!
[2024-03-25 19:12:05,410] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-720/global_step720/mp_rank_00_model_states.pt
[2024-03-25 19:12:05,410] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-720/global_step720/mp_rank_00_model_states.pt...
[2024-03-25 19:12:05,505] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-720/global_step720/mp_rank_00_model_states.pt.
[2024-03-25 19:12:05,508] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-720/global_step720/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 19:12:05,813] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-720/global_step720/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 19:12:05,813] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-720/global_step720/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 19:12:05,814] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step720 is ready now!
+++++++++++++++++save call back++++++++++++++++
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 19:12:06,267 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:12:06,268 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 69%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                               | 729/1050 [2:12:07<1:06:18, 12.40s/it]










 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                              | 739/1050 [2:13:28<41:40,  8.04s/it]
 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                              | 740/1050 [2:13:35<39:04,  7.56s/it][INFO|trainer.py:3242] 2024-03-25 19:15:05,152 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 19:15:05,152 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 19:15:05,152 >>   Batch size = 1




















 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 93/94 [00:40<00:00,  3.82it/s]


 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                              | 740/1050 [2:14:16<39:04,  7.56s/it][INFO|trainer.py:2936] 2024-03-25 19:15:48,940 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-740
[2024-03-25 19:15:49,810] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step740 is about to be saved!
[2024-03-25 19:15:49,837] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-740/global_step740/mp_rank_00_model_states.pt
[2024-03-25 19:15:49,837] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-740/global_step740/mp_rank_00_model_states.pt...
[2024-03-25 19:15:49,935] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-740/global_step740/mp_rank_00_model_states.pt.
[2024-03-25 19:15:49,937] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-740/global_step740/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 19:15:50,265] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-740/global_step740/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 19:15:50,266] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-740/global_step740/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 19:15:50,266] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step740 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-25 19:15:49,679 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:15:49,680 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 19:15:49,755 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-740/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 19:15:49,755 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-740/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 19:15:50,629 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:15:50,630 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                            | 749/1050 [2:15:35<40:40,  8.11s/it]










 72%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                           | 759/1050 [2:17:14<48:17,  9.96s/it]
 72%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                           | 760/1050 [2:17:22<45:46,  9.47s/it][INFO|trainer.py:3242] 2024-03-25 19:18:52,409 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 19:18:52,409 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 19:18:52,409 >>   Batch size = 1
















 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 87/94 [00:31<00:02,  2.47it/s]
{'eval_loss': 2.759053945541382, 'eval_accuracy': 0.844256687418759, 'eval_runtime': 33.3636, 'eval_samples_per_second': 2.817, 'eval_steps_per_second': 2.817, 'epoch': 7.2}
[2024-03-25 19:19:29,244] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step760 is about to be saved!
[2024-03-25 19:19:29,271] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-760/global_step760/mp_rank_00_model_states.pt
[2024-03-25 19:19:29,271] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-760/global_step760/mp_rank_00_model_states.pt...
[2024-03-25 19:19:29,360] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-760/global_step760/mp_rank_00_model_states.pt.
 72%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                           | 760/1050 [2:17:56<45:46,  9.47s/it][INFO|trainer.py:2936] 2024-03-25 19:19:28,283 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-760
[INFO|configuration_utils.py:729] 2024-03-25 19:19:29,119 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:19:29,120 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 19:19:29,194 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-760/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 19:19:29,194 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-760/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-25 19:19:29,753] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-760/global_step760/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 19:19:29,754] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-760/global_step760/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 19:19:29,754] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step760 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-25 19:19:30,126 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:19:30,126 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 73%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                         | 769/1050 [2:19:16<43:11,  9.22s/it]










 74%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                        | 779/1050 [2:20:41<37:52,  8.38s/it]
 74%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                        | 780/1050 [2:20:49<37:24,  8.31s/it][INFO|trainer.py:3242] 2024-03-25 19:22:18,830 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 19:22:18,831 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 19:22:18,831 >>   Batch size = 1















 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 88/94 [00:30<00:02,  2.11it/s]

{'eval_loss': 2.818028211593628, 'eval_accuracy': 0.8431620715605117, 'eval_runtime': 32.882, 'eval_samples_per_second': 2.859, 'eval_steps_per_second': 2.859, 'epoch': 7.39}
[2024-03-25 19:22:55,501] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step780 is about to be saved!
[2024-03-25 19:22:55,527] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-780/global_step780/mp_rank_00_model_states.pt
[2024-03-25 19:22:55,528] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-780/global_step780/mp_rank_00_model_states.pt...
[2024-03-25 19:22:55,625] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-780/global_step780/mp_rank_00_model_states.pt.
 74%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                        | 780/1050 [2:21:22<37:24,  8.31s/it][INFO|trainer.py:2936] 2024-03-25 19:22:55,009 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-780
[INFO|configuration_utils.py:729] 2024-03-25 19:22:55,378 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:22:55,379 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 19:22:55,453 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-780/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 19:22:55,454 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-780/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-25 19:22:56,017] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-780/global_step780/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 19:22:56,018] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-780/global_step780/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 19:22:56,018] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step780 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-25 19:22:56,419 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:22:56,419 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                      | 789/1050 [2:22:42<39:19,  9.04s/it]










 76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                     | 799/1050 [2:24:06<34:37,  8.28s/it]
 76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                     | 800/1050 [2:24:15<35:23,  8.49s/it][INFO|trainer.py:3242] 2024-03-25 19:25:44,823 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 19:25:44,823 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 19:25:44,823 >>   Batch size = 1















 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 92/94 [00:31<00:00,  3.69it/s]

{'eval_loss': 2.8020827770233154, 'eval_accuracy': 0.8439488267086269, 'eval_runtime': 32.3591, 'eval_samples_per_second': 2.905, 'eval_steps_per_second': 2.905, 'epoch': 7.58}
[2024-03-25 19:26:19,865] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step800 is about to be saved!
[2024-03-25 19:26:19,892] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-800/global_step800/mp_rank_00_model_states.pt
[2024-03-25 19:26:19,892] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-800/global_step800/mp_rank_00_model_states.pt...
[2024-03-25 19:26:19,993] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-800/global_step800/mp_rank_00_model_states.pt.
 76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                     | 800/1050 [2:24:47<35:23,  8.49s/it][INFO|trainer.py:2936] 2024-03-25 19:26:19,366 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-800
[INFO|configuration_utils.py:729] 2024-03-25 19:26:19,739 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:26:19,740 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 19:26:19,813 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 19:26:19,813 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-800/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-25 19:26:20,370] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 19:26:20,371] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-800/global_step800/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 19:26:20,371] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step800 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-25 19:26:20,749 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:26:20,750 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 77%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                   | 809/1050 [2:26:07<36:14,  9.02s/it]










 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                  | 819/1050 [2:27:31<32:16,  8.38s/it]
 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                  | 820/1050 [2:27:40<32:53,  8.58s/it][INFO|trainer.py:3242] 2024-03-25 19:29:09,987 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 19:29:09,987 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 19:29:09,987 >>   Batch size = 1
















 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 93/94 [00:31<00:00,  3.63it/s]

 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                  | 820/1050 [2:28:13<32:53,  8.58s/it][INFO|trainer.py:2936] 2024-03-25 19:29:46,076 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-820
[INFO|configuration_utils.py:729] 2024-03-25 19:29:46,448 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:29:46,449 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 19:29:46,521 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-820/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 19:29:46,522 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-820/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-25 19:29:46,573] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step820 is about to be saved!
[2024-03-25 19:29:46,600] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-820/global_step820/mp_rank_00_model_states.pt
[2024-03-25 19:29:46,600] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-820/global_step820/mp_rank_00_model_states.pt...
[2024-03-25 19:29:46,694] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-820/global_step820/mp_rank_00_model_states.pt.
[2024-03-25 19:29:46,697] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-820/global_step820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 19:29:47,072] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-820/global_step820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 19:29:47,072] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-820/global_step820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 19:29:47,073] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step820 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-25 19:29:47,450 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:29:47,451 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 79%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                | 829/1050 [2:29:33<33:15,  9.03s/it]










 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                               | 839/1050 [2:30:57<29:18,  8.33s/it]
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                               | 840/1050 [2:31:06<29:51,  8.53s/it][INFO|trainer.py:3242] 2024-03-25 19:32:36,324 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 19:32:36,325 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 19:32:36,325 >>   Batch size = 1















 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍        | 89/94 [00:30<00:01,  3.22it/s]


 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                               | 840/1050 [2:31:38<29:51,  8.53s/it][INFO|trainer.py:2936] 2024-03-25 19:33:10,672 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-840
[2024-03-25 19:33:11,170] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step840 is about to be saved!
[2024-03-25 19:33:11,196] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-840/global_step840/mp_rank_00_model_states.pt
[2024-03-25 19:33:11,196] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-840/global_step840/mp_rank_00_model_states.pt...
[2024-03-25 19:33:11,296] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-840/global_step840/mp_rank_00_model_states.pt.
[2024-03-25 19:33:11,299] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-840/global_step840/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 19:33:11,680] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-840/global_step840/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 19:33:11,680] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-840/global_step840/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 19:33:11,681] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step840 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-25 19:33:11,042 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:33:11,043 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 19:33:11,120 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-840/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 19:33:11,121 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-840/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 19:33:12,061 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:33:12,062 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                             | 849/1050 [2:32:59<30:57,  9.24s/it]










 82%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                            | 860/1050 [2:34:31<26:49,  8.47s/it][INFO|trainer.py:3242] 2024-03-25 19:36:00,827 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 19:36:00,827 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 19:36:00,827 >>   Batch size = 1
  0%|                                                                                                                                                                          | 0/94 [00:00<?, ?it/s]
















 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 88/94 [00:29<00:02,  2.77it/s]
{'eval_loss': 2.9838502407073975, 'eval_accuracy': 0.8414859410275707, 'eval_runtime': 31.9437, 'eval_samples_per_second': 2.943, 'eval_steps_per_second': 2.943, 'epoch': 8.15}
[2024-03-25 19:36:35,989] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step860 is about to be saved!
[2024-03-25 19:36:36,016] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-860/global_step860/mp_rank_00_model_states.pt
[2024-03-25 19:36:36,016] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-860/global_step860/mp_rank_00_model_states.pt...
[2024-03-25 19:36:36,114] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-860/global_step860/mp_rank_00_model_states.pt.
[2024-03-25 19:36:36,116] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-860/global_step860/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 19:36:36,494] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-860/global_step860/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 19:36:36,494] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-860/global_step860/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 19:36:36,494] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step860 is ready now!
 82%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                            | 860/1050 [2:35:03<26:49,  8.47s/it][INFO|trainer.py:2936] 2024-03-25 19:36:35,263 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-860
[INFO|configuration_utils.py:729] 2024-03-25 19:36:35,857 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:36:35,858 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 19:36:35,936 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-860/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 19:36:35,936 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-860/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 19:36:36,873 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:36:36,874 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                          | 870/1050 [2:36:31<26:19,  8.78s/it]









 84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                         | 880/1050 [2:37:55<23:40,  8.35s/it][INFO|trainer.py:3242] 2024-03-25 19:39:25,022 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 19:39:25,022 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 19:39:25,022 >>   Batch size = 1
  0%|                                                                                                                                                                          | 0/94 [00:00<?, ?it/s]
















 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 91/94 [00:32<00:01,  2.90it/s]

{'eval_loss': 2.9122679233551025, 'eval_accuracy': 0.8433673120339331, 'eval_runtime': 33.3628, 'eval_samples_per_second': 2.818, 'eval_steps_per_second': 2.818, 'epoch': 8.34}
[2024-03-25 19:40:02,217] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step880 is about to be saved!
[2024-03-25 19:40:02,246] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-880/global_step880/mp_rank_00_model_states.pt
[2024-03-25 19:40:02,246] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-880/global_step880/mp_rank_00_model_states.pt...
[2024-03-25 19:40:02,334] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-880/global_step880/mp_rank_00_model_states.pt.
[2024-03-25 19:40:02,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-880/global_step880/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 19:40:02,729] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-880/global_step880/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 19:40:02,730] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-880/global_step880/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 19:40:02,730] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step880 is ready now!
 84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                         | 880/1050 [2:38:28<23:40,  8.35s/it][INFO|trainer.py:2936] 2024-03-25 19:40:01,704 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-880
[INFO|configuration_utils.py:729] 2024-03-25 19:40:02,097 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:40:02,098 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 19:40:02,166 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-880/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 19:40:02,166 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-880/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 19:40:03,139 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:40:03,140 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                        | 889/1050 [2:39:49<24:08,  9.00s/it]










 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                      | 899/1050 [2:41:14<21:34,  8.57s/it]
 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                      | 900/1050 [2:41:22<21:13,  8.49s/it][INFO|trainer.py:3242] 2024-03-25 19:42:52,022 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 19:42:52,022 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 19:42:52,022 >>   Batch size = 1
















 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 91/94 [00:31<00:00,  3.55it/s]
 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                      | 900/1050 [2:41:55<21:13,  8.49s/it][INFO|trainer.py:2936] 2024-03-25 19:43:27,086 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-900
[INFO|configuration_utils.py:729] 2024-03-25 19:43:27,453 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:43:27,455 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 19:43:27,530 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-900/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 19:43:27,530 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-900/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-25 19:43:27,575] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step900 is about to be saved!
[2024-03-25 19:43:27,606] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-900/global_step900/mp_rank_00_model_states.pt
[2024-03-25 19:43:27,606] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-900/global_step900/mp_rank_00_model_states.pt...
[2024-03-25 19:43:27,703] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-900/global_step900/mp_rank_00_model_states.pt.
[2024-03-25 19:43:27,705] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-900/global_step900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 19:43:27,978] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-900/global_step900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 19:43:27,979] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-900/global_step900/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 19:43:27,979] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step900 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-25 19:43:28,612 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:43:28,613 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









 87%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                    | 910/1050 [2:43:24<20:59,  8.99s/it]









 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                   | 920/1050 [2:44:47<18:01,  8.32s/it][INFO|trainer.py:3242] 2024-03-25 19:46:17,083 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 19:46:17,083 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 19:46:17,083 >>   Batch size = 1
  0%|                                                                                                                                                                          | 0/94 [00:00<?, ?it/s]
















 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 92/94 [00:31<00:00,  2.65it/s]

{'eval_loss': 2.8880221843719482, 'eval_accuracy': 0.8441882739276185, 'eval_runtime': 33.859, 'eval_samples_per_second': 2.776, 'eval_steps_per_second': 2.776, 'epoch': 8.72}
[2024-03-25 19:46:54,755] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step920 is about to be saved!
[2024-03-25 19:46:54,782] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-920/global_step920/mp_rank_00_model_states.pt
[2024-03-25 19:46:54,782] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-920/global_step920/mp_rank_00_model_states.pt...
[2024-03-25 19:46:54,874] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-920/global_step920/mp_rank_00_model_states.pt.
 88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                   | 920/1050 [2:45:21<18:01,  8.32s/it][INFO|trainer.py:2936] 2024-03-25 19:46:54,265 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-920
[INFO|configuration_utils.py:729] 2024-03-25 19:46:54,640 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:46:54,641 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 19:46:54,713 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-920/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 19:46:54,713 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-920/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 19:46:55,604 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:46:55,605 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-03-25 19:46:55,226] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-920/global_step920/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 19:46:55,226] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-920/global_step920/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 19:46:55,227] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step920 is ready now!
+++++++++++++++++save call back++++++++++++++++








 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                  | 929/1050 [2:46:42<18:38,  9.24s/it]










 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                | 940/1050 [2:48:15<15:50,  8.64s/it][INFO|trainer.py:3242] 2024-03-25 19:49:45,315 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 19:49:45,315 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 19:49:45,315 >>   Batch size = 1
  3%|█████▏                                                                                                                                                            | 3/94 [00:00<00:17,  5.14it/s]
















 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏      | 90/94 [00:30<00:01,  3.73it/s]

 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                | 940/1050 [2:48:48<15:50,  8.64s/it][INFO|trainer.py:2936] 2024-03-25 19:50:20,205 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-940
[2024-03-25 19:50:20,681] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step940 is about to be saved!
[2024-03-25 19:50:20,708] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-940/global_step940/mp_rank_00_model_states.pt
[2024-03-25 19:50:20,708] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-940/global_step940/mp_rank_00_model_states.pt...
[2024-03-25 19:50:20,800] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-940/global_step940/mp_rank_00_model_states.pt.
[2024-03-25 19:50:20,803] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-940/global_step940/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 19:50:21,086] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-940/global_step940/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 19:50:21,087] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-940/global_step940/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 19:50:21,087] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step940 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-25 19:50:20,576 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:50:20,577 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 19:50:20,642 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-940/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 19:50:20,642 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-940/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 19:50:21,469 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:50:21,470 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









 90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏              | 950/1050 [2:50:18<15:12,  9.13s/it]









 91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋             | 960/1050 [2:51:44<12:43,  8.49s/it][INFO|trainer.py:3242] 2024-03-25 19:53:14,289 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 19:53:14,290 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 19:53:14,290 >>   Batch size = 1
{'loss': 0.1873, 'learning_rate': 1.4000000000000001e-05, 'epoch': 9.1}
















 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 92/94 [00:31<00:00,  3.62it/s]
 91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋             | 960/1050 [2:52:18<12:43,  8.49s/it][INFO|trainer.py:2936] 2024-03-25 19:53:51,205 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-960
[INFO|configuration_utils.py:729] 2024-03-25 19:53:51,578 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:53:51,579 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 19:53:51,650 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-960/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 19:53:51,650 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-960/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 19:53:52,525 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:53:52,526 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-03-25 19:53:51,693] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step960 is about to be saved!
[2024-03-25 19:53:51,719] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-960/global_step960/mp_rank_00_model_states.pt
[2024-03-25 19:53:51,720] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-960/global_step960/mp_rank_00_model_states.pt...
[2024-03-25 19:53:51,808] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-960/global_step960/mp_rank_00_model_states.pt.
[2024-03-25 19:53:51,810] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-960/global_step960/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 19:53:52,143] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-960/global_step960/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 19:53:52,143] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-960/global_step960/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 19:53:52,143] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step960 is ready now!
+++++++++++++++++save call back++++++++++++++++









 92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 970/1050 [2:53:49<11:56,  8.95s/it]









 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍          | 979/1050 [2:55:05<10:02,  8.48s/it]
 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 980/1050 [2:55:13<09:47,  8.40s/it][INFO|trainer.py:3242] 2024-03-25 19:56:43,366 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 19:56:43,366 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 19:56:43,366 >>   Batch size = 1
















 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍        | 89/94 [00:31<00:01,  2.55it/s]

 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌          | 980/1050 [2:55:46<09:47,  8.40s/it][INFO|trainer.py:2936] 2024-03-25 19:57:18,703 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-980
[2024-03-25 19:57:19,182] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step980 is about to be saved!
[2024-03-25 19:57:19,208] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-980/global_step980/mp_rank_00_model_states.pt
[2024-03-25 19:57:19,208] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-980/global_step980/mp_rank_00_model_states.pt...
[2024-03-25 19:57:19,303] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-980/global_step980/mp_rank_00_model_states.pt.
[2024-03-25 19:57:19,305] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-980/global_step980/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[INFO|configuration_utils.py:729] 2024-03-25 19:57:19,075 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:57:19,076 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 19:57:19,142 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-980/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 19:57:19,142 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-980/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 19:57:20,064 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 19:57:20,065 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-03-25 19:57:19,662] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-980/global_step980/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 19:57:19,663] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-980/global_step980/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 19:57:19,663] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step980 is ready now!
+++++++++++++++++save call back++++++++++++++++








 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉         | 989/1050 [2:57:07<09:17,  9.14s/it]










 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍       | 999/1050 [2:58:31<07:05,  8.34s/it]
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌       | 1000/1050 [2:58:40<07:08,  8.57s/it][INFO|trainer.py:3242] 2024-03-25 20:00:09,669 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 20:00:09,669 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 20:00:09,670 >>   Batch size = 1















 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 92/94 [00:30<00:00,  3.49it/s]

 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌       | 1000/1050 [2:59:12<07:08,  8.57s/it][INFO|trainer.py:2936] 2024-03-25 20:00:43,962 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1000
[INFO|configuration_utils.py:729] 2024-03-25 20:00:44,330 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 20:00:44,331 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 20:00:44,401 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 20:00:44,402 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1000/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-25 20:00:44,441] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
[2024-03-25 20:00:44,471] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1000/global_step1000/mp_rank_00_model_states.pt
[2024-03-25 20:00:44,471] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1000/global_step1000/mp_rank_00_model_states.pt...
[2024-03-25 20:00:44,569] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1000/global_step1000/mp_rank_00_model_states.pt.
[2024-03-25 20:00:44,571] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 20:00:44,850] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 20:00:44,851] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 20:00:44,851] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-25 20:00:45,222 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 20:00:45,223 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉      | 1009/1050 [3:00:32<06:12,  9.08s/it]










 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍    | 1019/1050 [3:01:57<04:20,  8.40s/it]
 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌    | 1020/1050 [3:02:06<04:17,  8.58s/it][INFO|trainer.py:3242] 2024-03-25 20:03:35,582 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 20:03:35,582 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 20:03:35,582 >>   Batch size = 1
















 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 93/94 [00:31<00:00,  3.24it/s]
 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌    | 1020/1050 [3:02:38<04:17,  8.58s/it][INFO|trainer.py:2936] 2024-03-25 20:04:11,573 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1020
[INFO|configuration_utils.py:729] 2024-03-25 20:04:11,955 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 20:04:11,956 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 20:04:12,026 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1020/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 20:04:12,026 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1020/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 20:04:13,418 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 20:04:13,419 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[2024-03-25 20:04:12,064] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1020 is about to be saved!
[2024-03-25 20:04:12,094] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1020/global_step1020/mp_rank_00_model_states.pt
[2024-03-25 20:04:12,094] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1020/global_step1020/mp_rank_00_model_states.pt...
[2024-03-25 20:04:12,180] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1020/global_step1020/mp_rank_00_model_states.pt.
[2024-03-25 20:04:12,182] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1020/global_step1020/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 20:04:12,440] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1020/global_step1020/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 20:04:12,441] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1020/global_step1020/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 20:04:12,441] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1020 is ready now!
+++++++++++++++++save call back++++++++++++++++
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉   | 1029/1050 [3:03:59<03:06,  8.87s/it]










 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍ | 1039/1050 [3:05:24<01:33,  8.48s/it]
 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 1040/1050 [3:05:32<01:24,  8.45s/it][INFO|trainer.py:3242] 2024-03-25 20:07:01,968 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 20:07:01,968 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 20:07:01,968 >>   Batch size = 1
















 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 88/94 [00:30<00:01,  3.52it/s]

 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 1040/1050 [3:06:05<01:24,  8.45s/it][INFO|trainer.py:2936] 2024-03-25 20:07:37,681 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1040
[2024-03-25 20:07:38,169] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1040 is about to be saved!
[INFO|configuration_utils.py:729] 2024-03-25 20:07:38,054 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 20:07:38,055 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 20:07:38,127 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1040/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 20:07:38,130 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1040/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-25 20:07:39,024 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 20:07:39,025 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-03-25 20:07:38,198] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1040/global_step1040/mp_rank_00_model_states.pt
[2024-03-25 20:07:38,198] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1040/global_step1040/mp_rank_00_model_states.pt...
[2024-03-25 20:07:38,289] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1040/global_step1040/mp_rank_00_model_states.pt.
[2024-03-25 20:07:38,291] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1040/global_step1040/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-25 20:07:38,643] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1040/global_step1040/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-25 20:07:38,644] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-1040/global_step1040/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-25 20:07:38,644] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1040 is ready now!
+++++++++++++++++save call back++++++++++++++++








100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1050/1050 [3:07:34<00:00,  8.92s/it][INFO|trainer.py:1962] 2024-03-25 20:09:03,640 >>
Training completed. Do not forget to share your model on huggingface.co/models =)
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1050/1050 [3:07:34<00:00, 10.72s/it]
{'loss': 0.1932, 'learning_rate': 1.5384615384615385e-07, 'epoch': 9.95}
{'train_runtime': 11264.9199, 'train_samples_per_second': 0.749, 'train_steps_per_second': 0.093, 'train_loss': 1.0644639101482574, 'epoch': 9.95}
[INFO|trainer.py:2936] 2024-03-25 20:09:06,065 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat
[INFO|configuration_utils.py:729] 2024-03-25 20:09:06,431 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-25 20:09:06,432 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-25 20:09:06,499 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-25 20:09:06,500 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/special_tokens_map.json
[INFO|trainer.py:3242] 2024-03-25 20:09:06,556 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-25 20:09:06,556 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-25 20:09:06,556 >>   Batch size = 1
  4%|██████▉                                                                                                                                                           | 4/94 [00:00<00:20,  4.47it/s]
***** train metrics *****
  epoch                    =       9.95
  train_loss               =     1.0645
  train_runtime            = 3:07:44.91
  train_samples            =        844
  train_samples_per_second =      0.749
  train_steps_per_second   =      0.093
















 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 93/94 [00:32<00:00,  3.26it/s]
***** eval metrics *****
  epoch                   =       9.95
  eval_accuracy           =     0.8429
  eval_loss               =     3.0227
  eval_runtime            = 0:00:32.93
  eval_samples            =         94
  eval_samples_per_second =      2.854
  eval_steps_per_second   =      2.854

100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [00:32<00:00,  2.88it/s]
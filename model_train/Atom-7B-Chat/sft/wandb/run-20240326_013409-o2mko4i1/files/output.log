  0%|                                                                                                                                                                        | 0/8440 [00:00<?, ?it/s]/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
03/26/2024 01:34:19 - WARNING - transformers_modules.FlagAlpha.Atom-7B-Chat.52b7761650cc65f24f8ae832184947906af0c337.model_atom - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/deepspeed/runtime/zero/stage_1_and_2.py:1303: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)
  total_norm_cuda = get_accelerator().FloatTensor([float(total_norm)])



  0%|▏                                                                                                                                                             | 8/8440 [00:09<2:19:10,  1.01it/s]





  0%|▎                                                                                                                                                            | 18/8440 [00:19<2:11:38,  1.07it/s]
  0%|▎                                                                                                                                                            | 20/8440 [00:21<2:13:39,  1.05it/s][INFO|trainer.py:3242] 2024-03-26 01:34:40,626 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 01:34:40,626 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 01:34:40,626 >>   Batch size = 1





















 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 87/94 [00:42<00:01,  3.73it/s]

{'eval_loss': 3.778869390487671, 'eval_accuracy': 0.7840528152151605, 'eval_runtime': 44.4739, 'eval_samples_per_second': 2.114, 'eval_steps_per_second': 2.114, 'epoch': 0.02}
[2024-03-26 01:35:28,926] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step20 is about to be saved!
[2024-03-26 01:35:28,954] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-20/global_step20/mp_rank_00_model_states.pt
[2024-03-26 01:35:28,954] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-20/global_step20/mp_rank_00_model_states.pt...
[2024-03-26 01:35:29,057] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-20/global_step20/mp_rank_00_model_states.pt.
[2024-03-26 01:35:29,059] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-20/global_step20/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 01:35:29,410] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-20/global_step20/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 01:35:29,411] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-20/global_step20/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 01:35:29,411] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20 is ready now!
  0%|▎                                                                                                                                                            | 20/8440 [01:05<2:13:39,  1.05it/s][INFO|trainer.py:2936] 2024-03-26 01:35:28,245 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-20
[INFO|configuration_utils.py:729] 2024-03-26 01:35:28,617 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:35:28,618 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 01:35:28,691 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-20/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 01:35:28,691 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-20/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-26 01:35:29,813 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:35:29,814 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  0%|▍                                                                                                                                                           | 23/8440 [01:13<19:09:26,  8.19s/it]
  0%|▍                                                                                                                                                           | 25/8440 [01:15<10:30:20,  4.49s/it]
  0%|▌                                                                                                                                                            | 27/8440 [01:17<6:16:10,  2.68s/it]
  0%|▌                                                                                                                                                            | 27/8440 [01:17<6:16:10,  2.68s/it]
  0%|▌                                                                                                                                                            | 30/8440 [01:20<3:35:34,  1.54s/it]
  0%|▌                                                                                                                                                            | 32/8440 [01:22<2:56:03,  1.26s/it]
  0%|▋                                                                                                                                                            | 34/8440 [01:24<2:35:10,  1.11s/it]
  0%|▋                                                                                                                                                            | 36/8440 [01:26<2:24:38,  1.03s/it]
  0%|▋                                                                                                                                                            | 37/8440 [01:26<2:20:30,  1.00s/it]
  0%|▋                                                                                                                                                            | 38/8440 [01:32<5:39:02,  2.42s/it]
  0%|▋                                                                                                                                                            | 39/8440 [01:38<8:09:48,  3.50s/it]
  0%|▋                                                                                                                                                            | 39/8440 [01:38<8:09:48,  3.50s/it]
  0%|                                                                                                                                                                          | 0/94 [00:00<?, ?it/s][INFO|trainer.py:3242] 2024-03-26 01:36:03,609 >> ***** Running Evaluation *****
  3%|█████▏                                                                                                                                                            | 3/94 [00:02<01:35,  1.05s/it][INFO|trainer.py:3242] 2024-03-26 01:36:03,609 >> ***** Running Evaluation *****
  6%|██████████▎                                                                                                                                                       | 6/94 [00:04<00:56,  1.56it/s][INFO|trainer.py:3242] 2024-03-26 01:36:03,609 >> ***** Running Evaluation *****
 15%|███████████████████████▉                                                                                                                                         | 14/94 [00:06<00:23,  3.35it/s][INFO|trainer.py:3242] 2024-03-26 01:36:03,609 >> ***** Running Evaluation *****
 22%|███████████████████████████████████▉                                                                                                                             | 21/94 [00:08<00:20,  3.55it/s][INFO|trainer.py:3242] 2024-03-26 01:36:03,609 >> ***** Running Evaluation *****
 30%|███████████████████████████████████████████████▉                                                                                                                 | 28/94 [00:10<00:18,  3.58it/s][INFO|trainer.py:3242] 2024-03-26 01:36:03,609 >> ***** Running Evaluation *****
 37%|███████████████████████████████████████████████████████████▉                                                                                                     | 35/94 [00:12<00:16,  3.50it/s][INFO|trainer.py:3242] 2024-03-26 01:36:03,609 >> ***** Running Evaluation *****
 43%|████████████████████████████████████████████████████████████████████▌                                                                                            | 40/94 [00:14<00:15,  3.56it/s][INFO|trainer.py:3242] 2024-03-26 01:36:03,609 >> ***** Running Evaluation *****
 51%|██████████████████████████████████████████████████████████████████████████████████▏                                                                              | 48/94 [00:16<00:13,  3.50it/s][INFO|trainer.py:3242] 2024-03-26 01:36:03,609 >> ***** Running Evaluation *****
 59%|██████████████████████████████████████████████████████████████████████████████████████████████▏                                                                  | 55/94 [00:18<00:10,  3.65it/s][INFO|trainer.py:3242] 2024-03-26 01:36:03,609 >> ***** Running Evaluation *****
 66%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                      | 62/94 [00:20<00:08,  3.57it/s][INFO|trainer.py:3242] 2024-03-26 01:36:03,609 >> ***** Running Evaluation *****
 73%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                          | 69/94 [00:22<00:07,  3.55it/s][INFO|trainer.py:3242] 2024-03-26 01:36:03,609 >> ***** Running Evaluation *****
 82%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                             | 77/94 [00:24<00:04,  3.61it/s][INFO|trainer.py:3242] 2024-03-26 01:36:03,609 >> ***** Running Evaluation *****
 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                  | 83/94 [00:26<00:03,  3.12it/s][INFO|trainer.py:3242] 2024-03-26 01:36:03,609 >> ***** Running Evaluation *****
 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏      | 90/94 [00:28<00:01,  3.53it/s][INFO|trainer.py:3242] 2024-03-26 01:36:03,609 >> ***** Running Evaluation *****
 96%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏      | 90/94 [00:28<00:01,  3.53it/s][INFO|trainer.py:3242] 2024-03-26 01:36:03,609 >> ***** Running Evaluation *****
                                                                                                                                                                                                      [INFO|trainer.py:3242] 2024-03-26 01:36:03,609 >> ***** Running Evaluation *****
                                                                                                                                                                                                      [INFO|trainer.py:3242] 2024-03-26 01:36:03,609 >> ***** Running Evaluation *****
[2024-03-26 01:36:38,342] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step40 is about to be saved!
[2024-03-26 01:36:38,369] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-40/global_step40/mp_rank_00_model_states.pt
[2024-03-26 01:36:38,369] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-40/global_step40/mp_rank_00_model_states.pt...
[2024-03-26 01:36:38,464] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-40/global_step40/mp_rank_00_model_states.pt.
[2024-03-26 01:36:38,466] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-40/global_step40/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 01:36:38,806] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-40/global_step40/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 01:36:38,806] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-40/global_step40/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 01:36:38,807] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40 is ready now!
  "num_attention_heads": 32, 4096,ation": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"e at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json21 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-40
  "num_attention_heads": 32, 4096,ation": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"e at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json21 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-40
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 01:36:38,162 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-40/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 01:36:38,162 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-40/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-26 01:36:39,228 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:36:39,229 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")



  1%|▉                                                                                                                                                            | 48/8440 [02:28<4:47:42,  2.06s/it]







  1%|█                                                                                                                                                            | 58/8440 [02:56<7:07:28,  3.06s/it]
  1%|█                                                                                                                                                            | 60/8440 [02:58<4:38:17,  1.99s/it][INFO|trainer.py:3242] 2024-03-26 01:37:17,208 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 01:37:17,208 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 01:37:17,208 >>   Batch size = 1












 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍        | 89/94 [00:24<00:01,  3.69it/s]

{'eval_loss': 3.3294661045074463, 'eval_accuracy': 0.793664910720394, 'eval_runtime': 25.8434, 'eval_samples_per_second': 3.637, 'eval_steps_per_second': 3.637, 'epoch': 0.07}
[2024-03-26 01:37:46,627] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step60 is about to be saved!
[2024-03-26 01:37:46,653] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-60/global_step60/mp_rank_00_model_states.pt
[2024-03-26 01:37:46,654] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-60/global_step60/mp_rank_00_model_states.pt...
[2024-03-26 01:37:46,742] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-60/global_step60/mp_rank_00_model_states.pt.
[2024-03-26 01:37:46,744] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-60/global_step60/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 01:37:47,107] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-60/global_step60/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 01:37:47,107] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-60/global_step60/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 01:37:47,108] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step60 is ready now!
  1%|█                                                                                                                                                            | 60/8440 [03:23<4:38:17,  1.99s/it][INFO|trainer.py:2936] 2024-03-26 01:37:46,066 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-60
[INFO|configuration_utils.py:729] 2024-03-26 01:37:46,438 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:37:46,439 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 01:37:46,513 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-60/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 01:37:46,513 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-60/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-26 01:37:47,482 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:37:47,483 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")



  1%|█▎                                                                                                                                                           | 68/8440 [03:36<4:06:27,  1.77s/it]







  1%|█▍                                                                                                                                                           | 78/8440 [04:04<4:33:55,  1.97s/it]
  1%|█▍                                                                                                                                                           | 80/8440 [04:06<3:20:39,  1.44s/it][INFO|trainer.py:3242] 2024-03-26 01:38:25,250 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 01:38:25,250 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 01:38:25,250 >>   Batch size = 1












 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋          | 88/94 [00:24<00:01,  3.71it/s]

{'eval_loss': 3.1017582416534424, 'eval_accuracy': 0.8000615721420264, 'eval_runtime': 26.1301, 'eval_samples_per_second': 3.597, 'eval_steps_per_second': 3.597, 'epoch': 0.09}
[2024-03-26 01:38:54,994] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step80 is about to be saved!
[2024-03-26 01:38:55,020] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-80/global_step80/mp_rank_00_model_states.pt
[2024-03-26 01:38:55,020] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-80/global_step80/mp_rank_00_model_states.pt...
[2024-03-26 01:38:55,117] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-80/global_step80/mp_rank_00_model_states.pt.
[2024-03-26 01:38:55,119] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-80/global_step80/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 01:38:55,443] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-80/global_step80/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 01:38:55,443] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-80/global_step80/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 01:38:55,443] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step80 is ready now!
  1%|█▍                                                                                                                                                           | 80/8440 [04:32<3:20:39,  1.44s/it][INFO|trainer.py:2936] 2024-03-26 01:38:54,401 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-80
[INFO|configuration_utils.py:729] 2024-03-26 01:38:54,762 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:38:54,763 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 01:38:54,832 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-80/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 01:38:54,832 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-80/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-26 01:38:55,814 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:38:55,815 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")





  1%|█▋                                                                                                                                                           | 89/8440 [04:52<7:55:21,  3.42s/it]






  1%|█▊                                                                                                                                                           | 99/8440 [05:14<2:57:05,  1.27s/it]
  1%|█▊                                                                                                                                                          | 100/8440 [05:15<2:51:55,  1.24s/it][INFO|trainer.py:3242] 2024-03-26 01:39:34,299 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 01:39:34,299 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 01:39:34,299 >>   Batch size = 1










 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 91/94 [00:25<00:00,  3.71it/s]


  1%|█▊                                                                                                                                                          | 100/8440 [05:41<2:51:55,  1.24s/it][INFO|trainer.py:2936] 2024-03-26 01:40:03,677 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-100
[2024-03-26 01:40:04,289] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step100 is about to be saved!
[2024-03-26 01:40:04,315] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-100/global_step100/mp_rank_00_model_states.pt
[2024-03-26 01:40:04,315] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-100/global_step100/mp_rank_00_model_states.pt...
[2024-03-26 01:40:04,405] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-100/global_step100/mp_rank_00_model_states.pt.
[2024-03-26 01:40:04,407] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 01:40:04,655] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 01:40:04,655] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 01:40:04,656] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-26 01:40:04,047 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:40:04,048 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 01:40:04,122 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 01:40:04,122 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-100/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-26 01:40:05,031 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:40:05,032 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")





  1%|█▉                                                                                                                                                         | 108/8440 [06:09<10:48:54,  4.67s/it]





  1%|██▏                                                                                                                                                         | 119/8440 [06:22<2:24:44,  1.04s/it]
  1%|██▏                                                                                                                                                         | 120/8440 [06:23<2:21:03,  1.02s/it][INFO|trainer.py:3242] 2024-03-26 01:40:42,595 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 01:40:42,595 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 01:40:42,595 >>   Batch size = 1












 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 92/94 [00:25<00:00,  3.68it/s]

  1%|██▏                                                                                                                                                         | 120/8440 [06:49<2:21:03,  1.02s/it][INFO|trainer.py:2936] 2024-03-26 01:41:11,749 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-120
[INFO|configuration_utils.py:729] 2024-03-26 01:41:12,119 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:41:12,120 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[2024-03-26 01:41:12,365] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step120 is about to be saved!
[2024-03-26 01:41:12,391] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-120/global_step120/mp_rank_00_model_states.pt
[2024-03-26 01:41:12,392] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-120/global_step120/mp_rank_00_model_states.pt...
[2024-03-26 01:41:12,488] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-120/global_step120/mp_rank_00_model_states.pt.
[2024-03-26 01:41:12,490] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-120/global_step120/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 01:41:12,753] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-120/global_step120/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 01:41:12,754] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-120/global_step120/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 01:41:12,754] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step120 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|tokenization_utils_base.py:2433] 2024-03-26 01:41:12,198 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-120/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 01:41:12,198 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-120/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-26 01:41:13,124 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:41:13,125 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")





  2%|██▎                                                                                                                                                         | 128/8440 [07:20<7:40:23,  3.32s/it]





  2%|██▌                                                                                                                                                         | 139/8440 [07:30<2:17:15,  1.01it/s]
  2%|██▌                                                                                                                                                         | 140/8440 [07:31<2:15:28,  1.02it/s][INFO|trainer.py:3242] 2024-03-26 01:41:50,535 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 01:41:50,536 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 01:41:50,536 >>   Batch size = 1












 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 92/94 [00:25<00:00,  3.63it/s]

  2%|██▌                                                                                                                                                         | 140/8440 [07:57<2:15:28,  1.02it/s][INFO|trainer.py:2936] 2024-03-26 01:42:19,453 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-140
[INFO|configuration_utils.py:729] 2024-03-26 01:42:20,113 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:42:20,114 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 01:42:20,192 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-140/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 01:42:20,192 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-140/special_tokens_map.json
[2024-03-26 01:42:20,306] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step140 is about to be saved!
[2024-03-26 01:42:20,333] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-140/global_step140/mp_rank_00_model_states.pt
[2024-03-26 01:42:20,333] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-140/global_step140/mp_rank_00_model_states.pt...
[2024-03-26 01:42:20,429] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-140/global_step140/mp_rank_00_model_states.pt.
[2024-03-26 01:42:20,431] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-140/global_step140/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 01:42:20,799] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-140/global_step140/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 01:42:20,800] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-140/global_step140/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 01:42:20,800] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step140 is ready now!
+++++++++++++++++save call back++++++++++++++++
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-26 01:42:21,182 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:42:21,183 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")





  2%|██▊                                                                                                                                                         | 149/8440 [08:28<4:38:14,  2.01s/it]




  2%|██▉                                                                                                                                                         | 158/8440 [08:37<2:19:38,  1.01s/it]
  2%|██▉                                                                                                                                                         | 160/8440 [08:39<2:15:30,  1.02it/s][INFO|trainer.py:3242] 2024-03-26 01:42:58,151 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 01:42:58,151 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 01:42:58,152 >>   Batch size = 1












 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 87/94 [00:23<00:01,  3.69it/s]
  2%|██▉                                                                                                                                                         | 160/8440 [09:05<2:15:30,  1.02it/s][INFO|trainer.py:2936] 2024-03-26 01:43:27,831 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-160
[INFO|configuration_utils.py:729] 2024-03-26 01:43:28,201 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:43:28,201 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 01:43:28,267 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-160/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 01:43:28,268 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-160/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-26 01:43:28,383] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step160 is about to be saved!
[2024-03-26 01:43:28,415] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-160/global_step160/mp_rank_00_model_states.pt
[2024-03-26 01:43:28,415] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-160/global_step160/mp_rank_00_model_states.pt...
[2024-03-26 01:43:28,505] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-160/global_step160/mp_rank_00_model_states.pt.
[2024-03-26 01:43:28,507] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-160/global_step160/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 01:43:28,847] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-160/global_step160/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 01:43:28,848] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-160/global_step160/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 01:43:28,848] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step160 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-26 01:43:29,226 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:43:29,227 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")





  2%|███                                                                                                                                                         | 168/8440 [09:33<5:22:11,  2.34s/it]





  2%|███▎                                                                                                                                                        | 178/8440 [09:42<2:15:02,  1.02it/s]
  2%|███▎                                                                                                                                                        | 180/8440 [09:44<2:15:01,  1.02it/s][INFO|trainer.py:3242] 2024-03-26 01:44:03,769 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 01:44:03,769 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 01:44:03,769 >>   Batch size = 1












 97%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊     | 91/94 [00:24<00:00,  3.40it/s]

{'eval_loss': 2.6169140338897705, 'eval_accuracy': 0.8244167749880277, 'eval_runtime': 25.7209, 'eval_samples_per_second': 3.655, 'eval_steps_per_second': 3.655, 'epoch': 0.21}
[2024-03-26 01:44:33,695] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step180 is about to be saved!
[2024-03-26 01:44:33,727] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-180/global_step180/mp_rank_00_model_states.pt
[2024-03-26 01:44:33,728] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-180/global_step180/mp_rank_00_model_states.pt...
[2024-03-26 01:44:33,819] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-180/global_step180/mp_rank_00_model_states.pt.
  2%|███▎                                                                                                                                                        | 180/8440 [10:10<2:15:01,  1.02it/s][INFO|trainer.py:2936] 2024-03-26 01:44:33,136 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-180
[INFO|configuration_utils.py:729] 2024-03-26 01:44:33,511 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:44:33,512 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 01:44:33,579 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-180/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 01:44:33,579 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-180/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-26 01:44:34,547 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:44:34,548 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[2024-03-26 01:44:34,149] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-180/global_step180/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 01:44:34,150] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-180/global_step180/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 01:44:34,150] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step180 is ready now!
+++++++++++++++++save call back++++++++++++++++
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")





  2%|███▌                                                                                                                                                        | 190/8440 [10:39<3:44:04,  1.63s/it]






  2%|███▋                                                                                                                                                        | 200/8440 [10:53<3:28:33,  1.52s/it][INFO|trainer.py:3242] 2024-03-26 01:45:12,450 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 01:45:12,451 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 01:45:12,451 >>   Batch size = 1
{'loss': 2.6035, 'learning_rate': 1.585228157566145e-05, 'epoch': 0.24}
























 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 92/94 [00:49<00:00,  3.46it/s]

  2%|███▋                                                                                                                                                        | 200/8440 [11:44<3:28:33,  1.52s/it][INFO|trainer.py:2936] 2024-03-26 01:46:05,970 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-200
[INFO|configuration_utils.py:729] 2024-03-26 01:46:06,342 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:46:06,343 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 01:46:06,419 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 01:46:06,419 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-200/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-26 01:46:06,569] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!
[2024-03-26 01:46:06,600] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-200/global_step200/mp_rank_00_model_states.pt
[2024-03-26 01:46:06,600] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-200/global_step200/mp_rank_00_model_states.pt...
[2024-03-26 01:46:06,713] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-200/global_step200/mp_rank_00_model_states.pt.
[2024-03-26 01:46:06,716] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 01:46:07,086] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 01:46:07,087] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 01:46:07,087] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-26 01:46:07,645 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:46:07,646 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")



  2%|███▊                                                                                                                                                        | 209/8440 [11:57<4:27:22,  1.95s/it]





  3%|████                                                                                                                                                        | 219/8440 [12:07<2:13:34,  1.03it/s]
  3%|████                                                                                                                                                        | 220/8440 [12:08<2:10:58,  1.05it/s][INFO|trainer.py:3242] 2024-03-26 01:46:27,201 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 01:46:27,202 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 01:46:27,202 >>   Batch size = 1























 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████            | 87/94 [00:45<00:01,  3.67it/s]
  3%|████                                                                                                                                                        | 220/8440 [12:55<2:10:58,  1.05it/s][INFO|trainer.py:2936] 2024-03-26 01:47:21,701 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-220
[INFO|configuration_utils.py:729] 2024-03-26 01:47:22,088 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:47:22,090 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 01:47:22,226 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-220/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 01:47:22,227 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-220/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-26 01:47:22,464] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step220 is about to be saved!
[2024-03-26 01:47:22,525] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-220/global_step220/mp_rank_00_model_states.pt
[2024-03-26 01:47:22,526] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-220/global_step220/mp_rank_00_model_states.pt...
[2024-03-26 01:47:22,703] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-220/global_step220/mp_rank_00_model_states.pt.
[2024-03-26 01:47:22,709] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-220/global_step220/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 01:47:23,365] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-220/global_step220/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 01:47:23,366] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-220/global_step220/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 01:47:23,366] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step220 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-26 01:47:23,750 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:47:23,751 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")



  3%|████▏                                                                                                                                                       | 229/8440 [13:13<4:23:37,  1.93s/it]





  3%|████▍                                                                                                                                                       | 240/8440 [13:27<4:37:46,  2.03s/it][INFO|trainer.py:3242] 2024-03-26 01:47:46,557 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 01:47:46,557 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 01:47:46,557 >>   Batch size = 1
{'loss': 2.6505, 'learning_rate': 1.639777790131734e-05, 'epoch': 0.28}











































 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 92/94 [01:26<00:01,  1.56it/s]

{'eval_loss': 2.5525717735290527, 'eval_accuracy': 0.8261613190121092, 'eval_runtime': 89.2747, 'eval_samples_per_second': 1.053, 'eval_steps_per_second': 1.053, 'epoch': 0.28}
[2024-03-26 01:49:20,111] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step240 is about to be saved!
[2024-03-26 01:49:20,164] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-240/global_step240/mp_rank_00_model_states.pt
  3%|████▍                                                                                                                                                       | 240/8440 [14:56<4:37:46,  2.03s/it][INFO|trainer.py:2936] 2024-03-26 01:49:19,463 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-240
[INFO|configuration_utils.py:729] 2024-03-26 01:49:19,839 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:49:19,840 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 01:49:19,923 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-240/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 01:49:19,924 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-240/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-26 01:49:20,315] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-240/global_step240/mp_rank_00_model_states.pt.
[2024-03-26 01:49:20,319] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-240/global_step240/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 01:49:20,748] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-240/global_step240/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 01:49:20,749] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-240/global_step240/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 01:49:20,749] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step240 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-26 01:49:21,148 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:49:21,150 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








  3%|████▌                                                                                                                                                       | 249/8440 [15:24<8:25:35,  3.70s/it]








  3%|████▊                                                                                                                                                       | 260/8440 [16:01<6:49:29,  3.00s/it][INFO|trainer.py:3242] 2024-03-26 01:50:20,566 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 01:50:20,566 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 01:50:20,566 >>   Batch size = 1
{'loss': 2.7706, 'learning_rate': 1.6637261392459344e-05, 'epoch': 0.31}



































 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 93/94 [01:12<00:00,  1.28it/s]

{'eval_loss': 2.541405439376831, 'eval_accuracy': 0.8267086269412328, 'eval_runtime': 75.1245, 'eval_samples_per_second': 1.251, 'eval_steps_per_second': 1.251, 'epoch': 0.31}
[2024-03-26 01:51:46,743] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step260 is about to be saved!
[2024-03-26 01:51:46,801] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-260/global_step260/mp_rank_00_model_states.pt
  3%|████▊                                                                                                                                                       | 260/8440 [17:16<6:49:29,  3.00s/it][INFO|trainer.py:2936] 2024-03-26 01:51:45,982 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-260
[INFO|configuration_utils.py:729] 2024-03-26 01:51:46,364 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:51:46,366 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 01:51:46,497 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-260/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 01:51:46,498 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-260/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-26 01:51:46,991] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-260/global_step260/mp_rank_00_model_states.pt.
[2024-03-26 01:51:46,995] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-260/global_step260/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 01:51:47,682] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-260/global_step260/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 01:51:47,684] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-260/global_step260/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 01:51:47,684] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step260 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-26 01:51:48,070 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:51:48,071 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")







  3%|████▉                                                                                                                                                      | 269/8440 [17:56<13:26:42,  5.92s/it]










  3%|█████▏                                                                                                                                                      | 279/8440 [18:23<4:58:04,  2.19s/it]
  3%|█████▏                                                                                                                                                      | 280/8440 [18:24<4:07:30,  1.82s/it][INFO|trainer.py:3242] 2024-03-26 01:52:43,211 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 01:52:43,211 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 01:52:43,211 >>   Batch size = 1




































 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 93/94 [01:15<00:00,  1.59it/s]

  3%|█████▏                                                                                                                                                      | 280/8440 [19:40<4:07:30,  1.82s/it][INFO|trainer.py:2936] 2024-03-26 01:54:06,312 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-280
[INFO|configuration_utils.py:729] 2024-03-26 01:54:06,702 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:54:06,703 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 01:54:06,817 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-280/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 01:54:06,818 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-280/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-26 01:54:07,041] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step280 is about to be saved!
[2024-03-26 01:54:07,096] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-280/global_step280/mp_rank_00_model_states.pt
[2024-03-26 01:54:07,097] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-280/global_step280/mp_rank_00_model_states.pt...
[2024-03-26 01:54:07,245] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-280/global_step280/mp_rank_00_model_states.pt.
[2024-03-26 01:54:07,249] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-280/global_step280/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 01:54:07,809] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-280/global_step280/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 01:54:07,810] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-280/global_step280/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 01:54:07,810] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step280 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-26 01:54:08,185 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:54:08,186 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")








  3%|█████▎                                                                                                                                                      | 289/8440 [20:23<9:07:01,  4.03s/it]









  4%|█████▌                                                                                                                                                      | 300/8440 [20:43<4:12:40,  1.86s/it][INFO|trainer.py:3242] 2024-03-26 01:55:03,095 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 01:55:03,096 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 01:55:03,096 >>   Batch size = 1
{'loss': 2.5455, 'learning_rate': 1.7065411446555598e-05, 'epoch': 0.36}






































 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 93/94 [01:18<00:01,  1.45s/it]

  4%|█████▌                                                                                                                                                      | 300/8440 [22:05<4:12:40,  1.86s/it][INFO|trainer.py:2936] 2024-03-26 01:56:32,818 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-300
[INFO|configuration_utils.py:729] 2024-03-26 01:56:33,199 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:56:33,200 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 01:56:33,306 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 01:56:33,307 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-300/special_tokens_map.json
[2024-03-26 01:56:33,507] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step300 is about to be saved!
[2024-03-26 01:56:33,555] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-300/global_step300/mp_rank_00_model_states.pt
[2024-03-26 01:56:33,556] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-300/global_step300/mp_rank_00_model_states.pt...
[2024-03-26 01:56:33,703] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-300/global_step300/mp_rank_00_model_states.pt.
[2024-03-26 01:56:33,707] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 01:56:34,829] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-26 01:56:35,215 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:56:35,218 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[2024-03-26 01:56:34,830] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 01:56:34,831] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step300 is ready now!
+++++++++++++++++save call back++++++++++++++++
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")







  4%|█████▋                                                                                                                                                      | 309/8440 [22:35<7:30:03,  3.32s/it]









  4%|█████▉                                                                                                                                                      | 318/8440 [22:52<4:37:28,  2.05s/it]
  4%|█████▉                                                                                                                                                      | 320/8440 [22:55<3:43:21,  1.65s/it][INFO|trainer.py:3242] 2024-03-26 01:57:14,407 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 01:57:14,407 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 01:57:14,408 >>   Batch size = 1













































 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 93/94 [01:32<00:01,  1.27s/it]

  4%|█████▉                                                                                                                                                      | 320/8440 [24:29<3:43:21,  1.65s/it][INFO|trainer.py:2936] 2024-03-26 01:58:56,832 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-320
[INFO|configuration_utils.py:729] 2024-03-26 01:58:57,211 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:58:57,213 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 01:58:57,330 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-320/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 01:58:57,331 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-320/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-26 01:58:57,539] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step320 is about to be saved!
[2024-03-26 01:58:57,583] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-320/global_step320/mp_rank_00_model_states.pt
[2024-03-26 01:58:57,584] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-320/global_step320/mp_rank_00_model_states.pt...
[2024-03-26 01:58:57,736] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-320/global_step320/mp_rank_00_model_states.pt.
[2024-03-26 01:58:57,739] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-320/global_step320/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 01:58:58,230] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-320/global_step320/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 01:58:58,231] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-320/global_step320/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 01:58:58,231] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step320 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-26 01:58:58,750 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 01:58:58,751 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")







  4%|██████                                                                                                                                                      | 329/8440 [24:54<7:51:14,  3.49s/it]









  4%|██████▎                                                                                                                                                     | 339/8440 [25:29<8:59:54,  4.00s/it]
  4%|██████▎                                                                                                                                                     | 340/8440 [25:31<7:33:03,  3.36s/it][INFO|trainer.py:3242] 2024-03-26 01:59:50,243 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 01:59:50,243 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 01:59:50,243 >>   Batch size = 1






































 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 93/94 [01:16<00:00,  1.55it/s]

  4%|██████▎                                                                                                                                                     | 340/8440 [26:49<7:33:03,  3.36s/it][INFO|trainer.py:2936] 2024-03-26 02:01:14,661 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-340
[INFO|configuration_utils.py:729] 2024-03-26 02:01:15,049 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 02:01:15,051 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 02:01:15,161 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-340/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 02:01:15,162 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-340/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-26 02:01:15,375] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step340 is about to be saved!
[2024-03-26 02:01:15,438] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-340/global_step340/mp_rank_00_model_states.pt
[2024-03-26 02:01:15,439] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-340/global_step340/mp_rank_00_model_states.pt...
[2024-03-26 02:01:15,601] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-340/global_step340/mp_rank_00_model_states.pt.
[2024-03-26 02:01:15,604] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-340/global_step340/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 02:01:16,148] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-340/global_step340/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 02:01:16,149] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-340/global_step340/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 02:01:16,149] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step340 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-26 02:01:16,758 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 02:01:16,759 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")







  4%|██████▍                                                                                                                                                    | 349/8440 [27:30<12:41:11,  5.64s/it]










  4%|██████▋                                                                                                                                                     | 359/8440 [27:49<4:21:45,  1.94s/it]
  4%|██████▋                                                                                                                                                     | 360/8440 [27:51<4:23:05,  1.95s/it][INFO|trainer.py:3242] 2024-03-26 02:02:10,656 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 02:02:10,657 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 02:02:10,657 >>   Batch size = 1



































 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 93/94 [01:10<00:00,  1.77it/s]


  4%|██████▋                                                                                                                                                     | 360/8440 [29:03<4:23:05,  1.95s/it][INFO|trainer.py:2936] 2024-03-26 02:03:29,844 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-360
[2024-03-26 02:03:30,560] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step360 is about to be saved!
[2024-03-26 02:03:30,608] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-360/global_step360/mp_rank_00_model_states.pt
[2024-03-26 02:03:30,608] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-360/global_step360/mp_rank_00_model_states.pt...
[2024-03-26 02:03:30,745] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-360/global_step360/mp_rank_00_model_states.pt.
[2024-03-26 02:03:30,747] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-360/global_step360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 02:03:31,285] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-360/global_step360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 02:03:31,286] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-360/global_step360/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 02:03:31,286] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step360 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-26 02:03:30,228 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 02:03:30,229 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 02:03:30,341 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-360/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 02:03:30,342 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-360/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-26 02:03:31,734 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 02:03:31,735 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")







  4%|██████▊                                                                                                                                                     | 369/8440 [29:42<7:50:54,  3.50s/it]









  4%|██████▉                                                                                                                                                     | 378/8440 [29:59<4:14:20,  1.89s/it]
  5%|███████                                                                                                                                                     | 380/8440 [30:02<3:35:12,  1.60s/it][INFO|trainer.py:3242] 2024-03-26 02:04:21,341 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 02:04:21,341 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 02:04:21,342 >>   Batch size = 1











































 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 93/94 [01:24<00:01,  1.57s/it]

  5%|███████                                                                                                                                                     | 380/8440 [31:28<3:35:12,  1.60s/it][INFO|trainer.py:2936] 2024-03-26 02:06:00,020 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-380
[2024-03-26 02:06:00,966] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step380 is about to be saved!
[2024-03-26 02:06:01,025] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-380/global_step380/mp_rank_00_model_states.pt
[2024-03-26 02:06:01,025] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-380/global_step380/mp_rank_00_model_states.pt...
[2024-03-26 02:06:01,215] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-380/global_step380/mp_rank_00_model_states.pt.
[2024-03-26 02:06:01,220] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-380/global_step380/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[INFO|configuration_utils.py:729] 2024-03-26 02:06:00,558 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 02:06:00,560 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 02:06:00,708 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-380/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 02:06:00,709 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-380/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-26 02:06:01,920] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-380/global_step380/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 02:06:01,922] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-380/global_step380/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 02:06:01,922] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step380 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-26 02:06:02,316 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 02:06:02,318 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")







  5%|███████▏                                                                                                                                                    | 390/8440 [32:00<6:44:59,  3.02s/it]









  5%|███████▎                                                                                                                                                   | 399/8440 [32:29<10:30:03,  4.70s/it]
  5%|███████▎                                                                                                                                                   | 400/8440 [32:35<10:59:58,  4.93s/it][INFO|trainer.py:3242] 2024-03-26 02:06:54,436 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 02:06:54,436 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 02:06:54,436 >>   Batch size = 1





































 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 93/94 [01:15<00:00,  1.36it/s]

  5%|███████▎                                                                                                                                                   | 400/8440 [33:51<10:59:58,  4.93s/it][INFO|trainer.py:2936] 2024-03-26 02:08:17,618 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-400
[INFO|configuration_utils.py:729] 2024-03-26 02:08:17,999 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 02:08:18,000 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 02:08:18,073 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 02:08:18,073 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-400/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-26 02:08:18,213] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step400 is about to be saved!
[2024-03-26 02:08:18,240] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-400/global_step400/mp_rank_00_model_states.pt
[2024-03-26 02:08:18,241] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-400/global_step400/mp_rank_00_model_states.pt...
[2024-03-26 02:08:18,337] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-400/global_step400/mp_rank_00_model_states.pt.
[2024-03-26 02:08:18,339] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 02:08:18,654] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 02:08:18,655] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-400/global_step400/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 02:08:18,655] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step400 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-26 02:08:19,094 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 02:08:19,097 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")







  5%|███████▌                                                                                                                                                    | 409/8440 [34:16<7:27:48,  3.35s/it]










  5%|███████▋                                                                                                                                                    | 419/8440 [34:50<5:20:02,  2.39s/it]
  5%|███████▊                                                                                                                                                    | 420/8440 [34:52<5:04:16,  2.28s/it][INFO|trainer.py:3242] 2024-03-26 02:09:11,243 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 02:09:11,243 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 02:09:11,243 >>   Batch size = 1






































 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 92/94 [01:14<00:01,  1.67it/s]
  5%|███████▊                                                                                                                                                    | 420/8440 [36:08<5:04:16,  2.28s/it][INFO|trainer.py:2936] 2024-03-26 02:10:34,210 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-420
[INFO|configuration_utils.py:729] 2024-03-26 02:10:34,587 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 02:10:34,589 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 02:10:34,700 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-420/tokenizer_config.json
[2024-03-26 02:10:34,897] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step420 is about to be saved!
[2024-03-26 02:10:34,936] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-420/global_step420/mp_rank_00_model_states.pt
[2024-03-26 02:10:34,936] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-420/global_step420/mp_rank_00_model_states.pt...
[2024-03-26 02:10:35,075] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-420/global_step420/mp_rank_00_model_states.pt.
[2024-03-26 02:10:35,078] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-420/global_step420/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 02:10:35,617] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-420/global_step420/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 02:10:35,618] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-420/global_step420/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 02:10:35,618] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step420 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|tokenization_utils_base.py:2442] 2024-03-26 02:10:34,701 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-420/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-26 02:10:36,008 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 02:10:36,009 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









  5%|███████▉                                                                                                                                                    | 430/8440 [36:51<7:57:46,  3.58s/it]









  5%|████████                                                                                                                                                    | 439/8440 [37:07<3:55:57,  1.77s/it]
  5%|████████▏                                                                                                                                                   | 440/8440 [37:09<4:00:47,  1.81s/it][INFO|trainer.py:3242] 2024-03-26 02:11:28,981 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 02:11:28,981 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 02:11:28,981 >>   Batch size = 1








































 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎ | 93/94 [01:19<00:01,  1.53s/it]

  5%|████████▏                                                                                                                                                   | 440/8440 [38:32<4:00:47,  1.81s/it][INFO|trainer.py:2936] 2024-03-26 02:12:59,479 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-440
[INFO|configuration_utils.py:729] 2024-03-26 02:12:59,850 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 02:12:59,851 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 02:12:59,925 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-440/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 02:12:59,926 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-440/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-26 02:13:00,079] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step440 is about to be saved!
[2024-03-26 02:13:00,105] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-440/global_step440/mp_rank_00_model_states.pt
[2024-03-26 02:13:00,105] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-440/global_step440/mp_rank_00_model_states.pt...
[2024-03-26 02:13:00,201] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-440/global_step440/mp_rank_00_model_states.pt.
[2024-03-26 02:13:00,203] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-440/global_step440/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 02:13:00,859] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-440/global_step440/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 02:13:00,860] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-440/global_step440/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 02:13:00,860] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step440 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-26 02:13:01,300 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 02:13:01,304 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")









  5%|████████▎                                                                                                                                                   | 450/8440 [39:05<7:00:05,  3.15s/it]









  5%|████████▍                                                                                                                                                  | 460/8440 [39:39<11:01:49,  4.98s/it][INFO|trainer.py:3242] 2024-03-26 02:13:58,248 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 02:13:58,248 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 02:13:58,248 >>   Batch size = 1
  0%|                                                                                                                                                                          | 0/94 [00:00<?, ?it/s]




































100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 94/94 [01:12<00:00,  1.44it/s]


  5%|████████▍                                                                                                                                                  | 460/8440 [40:52<11:01:49,  4.98s/it][INFO|trainer.py:2936] 2024-03-26 02:15:22,810 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-460
[2024-03-26 02:15:23,576] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step460 is about to be saved!
[2024-03-26 02:15:23,637] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-460/global_step460/mp_rank_00_model_states.pt
[2024-03-26 02:15:23,637] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-460/global_step460/mp_rank_00_model_states.pt...
[2024-03-26 02:15:23,823] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-460/global_step460/mp_rank_00_model_states.pt.
[2024-03-26 02:15:23,827] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-460/global_step460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[INFO|configuration_utils.py:729] 2024-03-26 02:15:23,179 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 02:15:23,181 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 02:15:23,313 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-460/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 02:15:23,314 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-460/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[INFO|configuration_utils.py:729] 2024-03-26 02:15:24,880 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 02:15:24,883 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
[2024-03-26 02:15:24,487] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-460/global_step460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 02:15:24,489] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-460/global_step460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 02:15:24,489] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step460 is ready now!
+++++++++++++++++save call back++++++++++++++++








  6%|████████▋                                                                                                                                                  | 470/8440 [41:33<11:24:30,  5.15s/it]








  6%|████████▊                                                                                                                                                   | 480/8440 [41:59<4:36:25,  2.08s/it][INFO|trainer.py:3242] 2024-03-26 02:16:18,229 >> ***** Running Evaluation *****
[INFO|trainer.py:3244] 2024-03-26 02:16:18,230 >>   Num examples = 94
[INFO|trainer.py:3247] 2024-03-26 02:16:18,230 >>   Batch size = 1
  0%|                                                                                                                                                                          | 0/94 [00:00<?, ?it/s]



































 98%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌   | 92/94 [01:09<00:02,  1.03s/it]
  6%|████████▊                                                                                                                                                   | 480/8440 [43:11<4:36:25,  2.08s/it][INFO|trainer.py:2936] 2024-03-26 02:17:38,494 >> Saving model checkpoint to /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-480
[INFO|configuration_utils.py:729] 2024-03-26 02:17:38,862 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 02:17:38,863 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
[INFO|tokenization_utils_base.py:2433] 2024-03-26 02:17:38,949 >> tokenizer config file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-480/tokenizer_config.json
[INFO|tokenization_utils_base.py:2442] 2024-03-26 02:17:38,950 >> Special tokens file saved in /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-480/special_tokens_map.json
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1877: UserWarning: Positional args are being deprecated, use kwargs instead. Refer to https://pytorch.org/docs/master/generated/torch.nn.Module.html#torch.nn.Module.state_dict for details.
  warnings.warn(
[2024-03-26 02:17:39,061] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step480 is about to be saved!
[2024-03-26 02:17:39,095] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-480/global_step480/mp_rank_00_model_states.pt
[2024-03-26 02:17:39,095] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-480/global_step480/mp_rank_00_model_states.pt...
[2024-03-26 02:17:39,276] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-480/global_step480/mp_rank_00_model_states.pt.
[2024-03-26 02:17:39,280] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-480/global_step480/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-03-26 02:17:39,776] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-480/global_step480/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-03-26 02:17:39,777] [INFO] [engine.py:3487:_save_zero_checkpoint] zero checkpoint saved /home/w1nd/darkword/1darkword/model_train/Atom-7B-Chat/darkword-Atom-7B-Chat/tmp-checkpoint-480/global_step480/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-03-26 02:17:39,778] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step480 is ready now!
+++++++++++++++++save call back++++++++++++++++
[INFO|configuration_utils.py:729] 2024-03-26 02:17:40,160 >> loading configuration file config.json from cache at /home/w1nd/.cache/huggingface/hub/models--FlagAlpha--Atom-7B-Chat/snapshots/52b7761650cc65f24f8ae832184947906af0c337/config.json
[INFO|configuration_utils.py:792] 2024-03-26 02:17:40,162 >> Model config LlamaConfig {
  "_name_or_path": "/mnt/nvme3n1/model_public/Atom7B/checkpoint-101k-32kl-9k-10k-chat",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "auto_map": {
    "AutoConfig": "FlagAlpha/Atom-7B-Chat--configuration_atom.LlamaConfig",
    "AutoModel": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForCausalLM": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForCausalLM",
    "AutoModelForSequenceClassification": "FlagAlpha/Atom-7B-Chat--model_atom.LlamaForSequenceClassification"
  },
  "bos_token_id": 1,
  "eos_token_id": 2,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 2,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": {
    "factor": 8.0,
    "type": "dynamic"
  },
  "rope_theta": 500000,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.2",
  "use_cache": true,
  "vocab_size": 65000
}
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/w1nd/.conda/envs/llm/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")



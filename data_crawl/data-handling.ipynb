{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telegram相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将用于telegram查询群组的关键词从csv提取到json\n",
    "import csv\n",
    "\n",
    "results={}\n",
    "results[\"all\"]=[]\n",
    "regex_value=\"\"\n",
    "with open(\"dark_keywords_telegram_search/dark_keywords.csv\",) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        results.setdefault(row[\"category\"],[])\n",
    "        results[row[\"category\"]].append(row[\"keyword\"])\n",
    "        results[\"all\"].append(row[\"keyword\"])\n",
    "        regex_value+=(row[\"keyword\"]+\"|\")\n",
    "        # print(regex_value)\n",
    "csvfile.close()\n",
    "print(regex_value[:-1])\n",
    "results[\"regex\"]=regex_value[:-1]\n",
    "\n",
    "results\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"dark_keywords_telegram_search/dark_keywords.json\",\"w\") as jsonfile:\n",
    "    json.dump(results,jsonfile,ensure_ascii=False)\n",
    "jsonfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将查询出来的group_channle_url汇总成regex表达式\n",
    "import csv\n",
    "results={}\n",
    "regex_value=\"\"\n",
    "with open(\"dark_keywords_telegram_search/keyword_group_channel_url.csv\",) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        regex_value+=(row[\"group_channel_url\"]+\"|\")\n",
    "        # print(regex_value)\n",
    "csvfile.close()\n",
    "print(regex_value[:-1])\n",
    "results[\"regex\"]=regex_value[:-1]\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"dark_keywords_telegram_search/keyword_group_channel_url.json\",\"w\") as jsonfile:\n",
    "    json.dump(results,jsonfile,ensure_ascii=False)\n",
    "jsonfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将group_channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atom相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将整理的用于大模型微调的黑话origin数据集整理成一列：csv->csv\n",
    "import csv\n",
    "\n",
    "results=[[\"text\"]]\n",
    "with open(\"darkword_data_collected/darkword_origin_data.csv\",\"r\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        text=\"<s>Human: \"+row[\"background\"]+row[\"category\"]+\"，\"+row[\"input\"]+\"\\n</s><s>Assistant: \"+row[\"content\"]+\"\\n</s>\"\n",
    "        print(text)\n",
    "        results.append([text])\n",
    "csvfile.close()\n",
    "\n",
    "with open(\"darkword_data_collected/darkword_train_data.csv\",\"w\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将整理的用于大模型微调的黑话origin数据集整理成一列：csv->csv（version3）\n",
    "# <s>{category}的{input}</s><s>{content}</s>\n",
    "import csv\n",
    "\n",
    "results=[[\"text\"]]\n",
    "with open(\"darkword_origin_data.csv\",\"r\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        text=\"<s>Human: \"+row[\"category\"]+\"的\"+row[\"input\"]+\"\\n</s><s>Assistant: \"+row[\"content\"]+\"\\n</s>\"\n",
    "        print(text)\n",
    "        results.append([text])\n",
    "csvfile.close()\n",
    "\n",
    "with open(\"darkword_data_collected/darkword_format_data.csv\",\"w\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atom-7b\n",
    "# 将格式化后好的数据按比例划分为训练数据集合验证数据集合。\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"darkword_data_atom/darkword_format_data.csv\")\n",
    "train_df, valid_df = train_test_split(df,test_size=0.1, random_state=51)\n",
    "train_df.to_csv(\"darkword_data_atom/darkword_train_data.csv\",index=False)\n",
    "valid_df.to_csv(\"darkword_data_atom/darkword_validate_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatglm3相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一版测试，将所有conversation都存入一个数组[]，再写入json文件\n",
    "# chatglm3-6b数据集处理\n",
    "# 格式：{\"conversations\":[{\"role\":\"user\",\"content\":row[\"content\"]},{\"role\":\"assistant\",\"content\":row[\"summary\"]},{},{}]}\n",
    "import csv,json\n",
    "\n",
    "# origin_data_file=\"darkword_origin_data.csv\"\n",
    "origin_data_file=\"test_origin_data.csv\"\n",
    "output_data_dir=\"darkword_data_chatglm3/\"\n",
    "output_data_file=output_data_dir+\"format.json\"\n",
    "train_data_file=output_data_dir+\"train.json\"\n",
    "validation_data_file=output_data_dir+\"dev.json\"\n",
    "results=[]\n",
    "with open(origin_data_file,\"r\",encoding=\"utf-8\") as csvfile:\n",
    "    # with open(output_data_file,\"wt\",encoding=\"utf-8\") as outputfile:\n",
    "    #     reader=csv.DictReader(csvfile)\n",
    "    #     for row in reader:\n",
    "    #         sample={\"conversations\":[{\"role\":\"user\",\"content\":row[\"category\"]+\"的\"+row[\"input\"]},{\"role\":\"assistant\",\"content\":row[\"content\"]}]}\n",
    "    #         outputfile.write(json.dumps(sample,ensure_ascii=False)+\"\\n\")\n",
    "    # outputfile.close()\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        sample={\"conversations\":[{\"role\":\"user\",\"content\":row[\"category\"]+\"的\"+row[\"input\"]},{\"role\":\"assistant\",\"content\":row[\"content\"]}]}\n",
    "        results.append(sample)\n",
    "csvfile.close()\n",
    "with open(output_data_file,\"wt\",encoding=\"utf-8\") as output:\n",
    "    json.dump(results,output,ensure_ascii=False,indent=4)\n",
    "output.close()\n",
    "\n",
    "# atom-7b\n",
    "# 将格式化后好的数据按比例随机划分为训练数据集合验证数据集合。\n",
    "import random\n",
    "\n",
    "data=[]\n",
    "with open(output_data_file,\"r\",encoding=\"utf-8\") as readfile:\n",
    "    data=json.load(readfile)\n",
    "readfile.close()\n",
    "# data\n",
    "# 打乱\n",
    "random.shuffle(data)\n",
    "# 定义比例\n",
    "train_ratio = 0.9\n",
    "train_size = int(len(data)*train_ratio)\n",
    "# 分割\n",
    "train_data=data[:train_size]\n",
    "validation_data=data[train_size:]\n",
    "\n",
    "with open(train_data_file,\"wt\",encoding=\"utf-8\") as outputfile:\n",
    "    # for sample in train_data:\n",
    "    #     outputfile.write(json.dumps(sample,ensure_ascii=False)+\"\\n\")\n",
    "    json.dump(train_data,outputfile,ensure_ascii=False,indent=4)\n",
    "outputfile.close()\n",
    "with open(validation_data_file,\"wt\",encoding=\"utf-8\") as outputfile:\n",
    "    # for sample in validation_data:\n",
    "    #     outputfile.write(json.dumps(sample,ensure_ascii=False)+\"\\n\")\n",
    "    json.dump(validation_data,outputfile,ensure_ascii=False,indent=4)\n",
    "outputfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二版测试，将每个conversation一行一行的存入json文件中。\n",
    "# chatglm3-6b数据集处理\n",
    "# 格式：{\"conversations\":[{\"role\":\"user\",\"content\":row[\"content\"]},{\"role\":\"assistant\",\"content\":row[\"summary\"]},{},{}]}\n",
    "import csv,json\n",
    "\n",
    "origin_data_file=\"darkword_origin_data.csv\"\n",
    "# origin_data_file=\"test_origin_data.csv\"\n",
    "output_data_dir=\"darkword_data_chatglm3/\"\n",
    "output_data_file=output_data_dir+\"format.json\"\n",
    "train_data_file=output_data_dir+\"train.json\"\n",
    "validation_data_file=output_data_dir+\"dev.json\"\n",
    "results=[]\n",
    "with open(origin_data_file,\"r\",encoding=\"utf-8\") as csvfile:\n",
    "    with open(output_data_file,\"wt\",encoding=\"utf-8\") as outputfile:\n",
    "        reader=csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            sample={\"conversations\":[{\"role\":\"user\",\"content\":row[\"category\"]+\"的\"+row[\"input\"]},{\"role\":\"assistant\",\"content\":row[\"content\"]}]}\n",
    "            outputfile.write(json.dumps(sample,ensure_ascii=False)+\"\\n\")\n",
    "    outputfile.close()\n",
    "csvfile.close()\n",
    "\n",
    "# atom-7b\n",
    "# 将格式化后好的数据按比例随机划分为训练数据集合验证数据集合。\n",
    "import random\n",
    "\n",
    "data=[]\n",
    "with open(output_data_file,\"r\",encoding=\"utf-8\") as readfile:\n",
    "    data=[json.loads(line) for line in readfile]\n",
    "readfile.close()\n",
    "# data\n",
    "# 打乱\n",
    "random.shuffle(data)\n",
    "# 定义比例\n",
    "train_ratio = 0.9\n",
    "train_size = int(len(data)*train_ratio)\n",
    "# 分割\n",
    "train_data=data[:train_size]\n",
    "validation_data=data[train_size:]\n",
    "\n",
    "with open(train_data_file,\"wt\",encoding=\"utf-8\") as outputfile:\n",
    "    for sample in train_data:\n",
    "        outputfile.write(json.dumps(sample,ensure_ascii=False)+\"\\n\")\n",
    "outputfile.close()\n",
    "with open(validation_data_file,\"wt\",encoding=\"utf-8\") as outputfile:\n",
    "    for sample in validation_data:\n",
    "        outputfile.write(json.dumps(sample,ensure_ascii=False)+\"\\n\")\n",
    "outputfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baichuan2相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将原始数据集转换为baichun2微调的格式\n",
    "# 格式：[{\"id\":*,\"conversations\":[{\"from\":\"*\",\"value\":\"*\"},{\"from\":\"gpt\",\"value\":\"*\"},{},{}]},{},{}]\n",
    "import csv,json\n",
    "\n",
    "results=[]\n",
    "with open(\"darkword_origin_data.csv\",\"r\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for index,row in enumerate(reader,start=1):\n",
    "        conversation=[{\"from\":\"human\",\"value\":row[\"category\"]+\"的\"+row[\"input\"]+\"\\n\"},{\"from\":\"gpt\",\"value\":row[\"content\"]+\"\\n\"}]\n",
    "        one_result={\"id\":index,\"conversations\":conversation}\n",
    "        results.append(one_result)\n",
    "csvfile.close()\n",
    "\n",
    "with open(\"darkword_data_baichuan2/train_data.json\",\"w\",encoding=\"utf-8\") as output:\n",
    "    json.dump(results,output,ensure_ascii=False,indent=4)\n",
    "output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

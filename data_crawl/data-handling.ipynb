{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Telegram相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将用于telegram查询群组的关键词从csv提取到json\n",
    "import csv\n",
    "\n",
    "results={}\n",
    "results[\"all\"]=[]\n",
    "regex_value=\"\"\n",
    "with open(\"dark_keywords_telegram_search/dark_keywords.csv\",) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        results.setdefault(row[\"category\"],[])\n",
    "        results[row[\"category\"]].append(row[\"keyword\"])\n",
    "        results[\"all\"].append(row[\"keyword\"])\n",
    "        regex_value+=(row[\"keyword\"]+\"|\")\n",
    "        # print(regex_value)\n",
    "csvfile.close()\n",
    "print(regex_value[:-1])\n",
    "results[\"regex\"]=regex_value[:-1]\n",
    "\n",
    "results\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"dark_keywords_telegram_search/dark_keywords.json\",\"w\") as jsonfile:\n",
    "    json.dump(results,jsonfile,ensure_ascii=False)\n",
    "jsonfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将查询出来的group_channle_url汇总成regex表达式\n",
    "import csv\n",
    "results={}\n",
    "regex_value=\"\"\n",
    "with open(\"dark_keywords_telegram_search/keyword_group_channel_url.csv\",) as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        regex_value+=(row[\"group_channel_url\"]+\"|\")\n",
    "        # print(regex_value)\n",
    "csvfile.close()\n",
    "print(regex_value[:-1])\n",
    "results[\"regex\"]=regex_value[:-1]\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"dark_keywords_telegram_search/keyword_group_channel_url.json\",\"w\") as jsonfile:\n",
    "    json.dump(results,jsonfile,ensure_ascii=False)\n",
    "jsonfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将筛选出的282条telegram聊天记录进行整合成origin_data的格式：\n",
    "# background,kind,input,content\n",
    "# input_template: 下面是一段telegram的聊天记录，请判断其是否与黑灰产相关，若相关请将其进一步分类：\\nrow[\"message_text\"]\n",
    "# content_template: 正常：这是一段正常的聊天记录，与黑灰产无关。其他：这是一段与黑灰产相关的聊天记录。 黑灰产：这是一段与row[\"kind\"]相关的黑灰产聊天记录。\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def clean_text(text):  \n",
    "    # 定义一个正则表达式模式，匹配中文、数字、英文字符、常见标点符号和换行符  \n",
    "    pattern = re.compile(r'[\\u4e00-\\u9fa5a-zA-Z0-9，。！？、；：“”（）、。《》【】、\\#\\$\\%\\^\\&\\*\\(\\)\\_\\+\\-\\=\\{\\}\\|\\[\\]\\\\\\:\\'\\\"\\<\\>\\?\\~\\.\\,\\!\\@\\n\\r\\t/]+')  \n",
    "      \n",
    "    # 使用正则表达式查找所有匹配的字符，并将它们连接起来  \n",
    "    cleaned_text = pattern.findall(text)  \n",
    "      \n",
    "    # 由于findall返回的是列表，每个元素是匹配的一个片段  \n",
    "    # 我们需要将这些片段连接起来，形成一个完整的字符串  \n",
    "    cleaned_text = ''.join(cleaned_text)  \n",
    "      \n",
    "    return cleaned_text \n",
    "\n",
    "results=[[\"from\",\"kind\",\"input\",\"content\"]]\n",
    "with open(\"/home/w1nd/darkword/1darkword/data_crawl/darkword_data_telegram/telegram_chat_data_ready_to_merge.csv\",\"r\",encoding=\"utf-8\") as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        input = \"下面是一段telegram的聊天记录，请判断其是否与黑灰产相关，若相关请将其进一步分类：\\n\"+clean_text(row[\"message_text\"])\n",
    "        # print(input)\n",
    "        kind = row[\"kind\"]\n",
    "        if kind == \"正常\":\n",
    "            content = \"这是一段正常的聊天记录，与黑灰产无关。\"\n",
    "        elif kind ==\"其他\":\n",
    "            content = \"这是一段与黑灰产相关的聊天记录。\"\n",
    "        else:\n",
    "            content = \"这是一段与\"+row[\"kind\"]+\"相关的黑灰产聊天记录。\"\n",
    "        \n",
    "        one_result=[\"Telegram_Chat_Message\",row[\"kind\"],input,content]\n",
    "        results.append(one_result)\n",
    "file.close()\n",
    "\n",
    "with open(\"/home/w1nd/darkword/1darkword/data_crawl/darkword_data_telegram/telegram_chat_data_final.csv\",\"wt\",encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(results)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将原始数据集随机切割成train和eval数据集（9:1）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"/home/w1nd/darkword/1darkword/data_crawl/darkdata_all_origin.csv\",encoding=\"utf-8\")\n",
    "train_df, eval_df = train_test_split(df,test_size=0.1,random_state=51)\n",
    "train_file_path=\"/home/w1nd/darkword/1darkword/data_crawl/data_train_or_eval/darkword_train_data.csv\"\n",
    "eval_file_path=\"/home/w1nd/darkword/1darkword/data_crawl/data_train_or_eval/darkword_eval_data.csv\"\n",
    "train_df.to_csv(train_file_path,index=False)\n",
    "eval_df.to_csv(eval_file_path,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atom相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将整理的用于大模型微调的黑话origin数据集整理成一列：csv->csv\n",
    "# 有train和eval两个\n",
    "import csv\n",
    "\n",
    "train_file_path=\"/home/w1nd/darkword/1darkword/data_crawl/data_train_or_eval/darkword_train_data.csv\"\n",
    "eval_file_path=\"/home/w1nd/darkword/1darkword/data_crawl/data_train_or_eval/darkword_eval_data.csv\"\n",
    "atom_train_file_path=\"/home/w1nd/darkword/1darkword/data_crawl/darkword_data_atom/darkword_train_data.csv\"\n",
    "atom_eval_file_path=\"/home/w1nd/darkword/1darkword/data_crawl/darkword_data_atom/darkword_eval_data.csv\"\n",
    "\n",
    "train_results=[[\"text\"]]\n",
    "eval_results=[[\"text\"]]\n",
    "with open(train_file_path,\"r\",encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        text=\"<s>Human: \"+row[\"input\"]+\"\\n</s><s>Assistant: \"+row[\"content\"]+\"\\n</s>\"\n",
    "        print(text)\n",
    "        train_results.append([text])\n",
    "csvfile.close()\n",
    "with open(eval_file_path,\"r\",encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        text=\"<s>Human: \"+row[\"input\"]+\"\\n</s><s>Assistant: \"+row[\"content\"]+\"\\n</s>\"\n",
    "        print(text)\n",
    "        eval_results.append([text])\n",
    "csvfile.close()\n",
    "\n",
    "\n",
    "with open(atom_train_file_path,\"wt\",encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(train_results)\n",
    "file.close()\n",
    "with open(atom_eval_file_path,\"wt\",encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(eval_results)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatglm3相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一版测试，将所有conversation都存入一个数组[]，再写入json文件\n",
    "# chatglm3-6b数据集处理\n",
    "# 格式：{\"conversations\":[{\"role\":\"user\",\"content\":row[\"input\"]},{\"role\":\"assistant\",\"content\":row[\"content\"]},{},{}]}\n",
    "import csv,json\n",
    "\n",
    "origin_train_data_file=\"/home/w1nd/darkword/1darkword/data_crawl/data_train_or_eval/darkword_train_data.csv\"\n",
    "origin_eval_data_file=\"/home/w1nd/darkword/1darkword/data_crawl/data_train_or_eval/darkword_eval_data.csv\"\n",
    "\n",
    "output_data_dir=\"darkword_data_chatglm3/\"\n",
    "train_data_file=output_data_dir+\"train.json\"\n",
    "eval_data_file=output_data_dir+\"dev.json\"\n",
    "train_results=[]\n",
    "eval_results=[]\n",
    "with open(origin_train_data_file,\"r\",encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        sample={\"conversations\":[{\"role\":\"user\",\"content\":row[\"input\"]},{\"role\":\"assistant\",\"content\":row[\"content\"]}]}\n",
    "        train_results.append(sample)\n",
    "csvfile.close()\n",
    "with open(origin_eval_data_file,\"r\",encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        sample={\"conversations\":[{\"role\":\"user\",\"content\":row[\"input\"]},{\"role\":\"assistant\",\"content\":row[\"content\"]}]}\n",
    "        eval_results.append(sample)\n",
    "csvfile.close()\n",
    "\n",
    "with open(train_data_file,\"wt\",encoding=\"utf-8\") as output:\n",
    "    json.dump(train_results,output,ensure_ascii=False,indent=4)\n",
    "output.close()\n",
    "with open(eval_data_file,\"wt\",encoding=\"utf-8\") as output:\n",
    "    json.dump(eval_results,output,ensure_ascii=False,indent=4)\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二版测试，将每个conversation一行一行的存入json文件中。（不知道哪个格式是对的。。）\n",
    "# chatglm3-6b数据集处理\n",
    "# 格式：{\"conversations\":[{\"role\":\"user\",\"content\":row[\"content\"]},{\"role\":\"assistant\",\"content\":row[\"summary\"]},{},{}]}\n",
    "import csv,json\n",
    "\n",
    "origin_train_data_file=\"/home/w1nd/darkword/1darkword/data_crawl/data_train_or_eval/darkword_train_data.csv\"\n",
    "origin_eval_data_file=\"/home/w1nd/darkword/1darkword/data_crawl/data_train_or_eval/darkword_eval_data.csv\"\n",
    "\n",
    "output_data_dir=\"darkword_data_chatglm3/\"\n",
    "train_data_file=output_data_dir+\"train.json\"\n",
    "eval_data_file=output_data_dir+\"dev.json\"\n",
    "results=[]\n",
    "\n",
    "with open(origin_train_data_file,\"r\",encoding=\"utf-8\") as csvfile:\n",
    "    with open(train_data_file,\"wt\",encoding=\"utf-8\") as outputfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            sample={\"conversations\":[{\"role\":\"user\",\"content\":row[\"input\"]},{\"role\":\"assistant\",\"content\":row[\"content\"]}]}\n",
    "            outputfile.write(json.dumps(sample,ensure_ascii=False)+\"\\n\")\n",
    "    outputfile.close()\n",
    "csvfile.close()\n",
    "\n",
    "with open(origin_eval_data_file,\"r\",encoding=\"utf-8\") as csvfile:\n",
    "    with open(eval_data_file,\"wt\",encoding=\"utf-8\") as outputfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            sample={\"conversations\":[{\"role\":\"user\",\"content\":row[\"input\"]},{\"role\":\"assistant\",\"content\":row[\"content\"]}]}\n",
    "            outputfile.write(json.dumps(sample,ensure_ascii=False)+\"\\n\")\n",
    "    outputfile.close()\n",
    "csvfile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baichuan2相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将原始数据集转换为baichun2微调的格式\n",
    "# 格式：[{\"id\":*,\"conversations\":[{\"from\":\"*\",\"value\":\"*\"},{\"from\":\"gpt\",\"value\":\"*\"},{},{}]},{},{}]\n",
    "import csv,json\n",
    "\n",
    "train_file_path=\"/home/w1nd/darkword/1darkword/data_crawl/data_train_or_eval/darkword_train_data.csv\"\n",
    "output_file_path=\"/home/w1nd/darkword/1darkword/data_crawl/darkword_data_baichuan2/train_data.json\"\n",
    "train_results=[]\n",
    "\n",
    "with open(train_file_path,\"r\",encoding=\"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for index,row in enumerate(reader,start=1):\n",
    "        conversation=[{\"from\":\"human\",\"value\":row[\"input\"]+\"\\n\"},{\"from\":\"gpt\",\"value\":row[\"content\"]+\"\\n\"}]\n",
    "        one_result={\"id\":index,\"conversations\":conversation}\n",
    "        train_results.append(one_result)\n",
    "csvfile.close()\n",
    "\n",
    "with open(output_file_path,\"wt\",encoding=\"utf-8\") as output:\n",
    "    json.dump(train_results,output,ensure_ascii=False,indent=4)\n",
    "output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
